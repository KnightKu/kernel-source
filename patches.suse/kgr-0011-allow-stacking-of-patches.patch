From: Jiri Slaby <jslaby@suse.cz>
Date: Tue, 8 Jul 2014 11:09:24 +0200
Subject: kgr: allow stacking of patches
Patch-mainline: not yet, kgraft
References: fate#313296 bnc#901169

We may want to patch some function twice. kGraft does not support this
yet. So take care of this path correctly by remembering all patches in
a list and walking the list when adding another patch. If it patches
the same function, set the 'loc_old' to already patched function, not
the original one.

kgr_patch_code() also has to unregister the already existing
ftrace_ops on a given location. Otherwise it would make ftrace to
always redirect to the old function. Hence we need to set unreg_ops in
KGR_PATCH_INIT to remove the ftrace_ops redirection installed by some
previous patching run.

Symmetrically, we need to handle reverts properly as well, i.e
KGR_PATCH_REVERT_SLOW needs to set new_ops to the version of the ops that
has been there before second patch came in.

On the top of that ensure that patches are reverted in the correct
order by incrementing the reference count of previous patches.

js: do not chain ftrace locations, always jump from 'name'
jk: fix patch stacking
js: de-duplicate code and extract to kgr_get_last_pf
js: prepend kgr_ to the list of patches
pm: remove patch from global list when being removed
js: do not allow double revert
pm: allow to search various types of struct kgr_patch_fun

Signed-off-by: Petr Mladek <pmladek@suse.cz>
Signed-off-by: Jiri Kosina <jkosina@suse.cz>
Signed-off-by: Jiri Slaby <jslaby@suse.cz>
---
 include/linux/kgraft.h |    9 ++
 kernel/kgraft.c        |  176 +++++++++++++++++++++++++++++++++++++++++++++++--
 kernel/kgraft_files.c  |   10 ++
 3 files changed, 190 insertions(+), 5 deletions(-)

--- a/include/linux/kgraft.h
+++ b/include/linux/kgraft.h
@@ -20,6 +20,7 @@
 #include <linux/bitops.h>
 #include <linux/compiler.h>
 #include <linux/kobject.h>
+#include <linux/list.h>
 #include <linux/ftrace.h>
 #include <linux/sched.h>
 
@@ -36,7 +37,8 @@ struct kgr_patch;
  *
  * @name: function to patch
  * @new_fun: function with the new body
- * @loc_old: cache of @name's function address
+ * @loc_name: cache of @name's function address
+ * @loc_old: cache of the last function address for @name in the patches list
  * @ftrace_ops_slow: ftrace ops for slow (temporary) stub
  * @ftrace_ops_fast: ftrace ops for fast () stub
  */
@@ -58,6 +60,7 @@ struct kgr_patch_fun {
 		KGR_PATCH_SKIPPED,
 	} state;
 
+	unsigned long loc_name;
 	unsigned long loc_old;
 
 	struct ftrace_ops ftrace_ops_slow;
@@ -68,7 +71,9 @@ struct kgr_patch_fun {
  * struct kgr_patch -- a kGraft patch
  *
  * @kobj: object representing the sysfs entry
+ * @list: member in patches list
  * @finish: waiting till it is safe to remove the module with the patch
+ * @refs: how many patches need to be reverted before this one
  * @name: name of the patch (to appear in sysfs)
  * @owner: module to refcount on patching
  * @patches: array of @kgr_patch_fun structures
@@ -76,7 +81,9 @@ struct kgr_patch_fun {
 struct kgr_patch {
 	/* internal state information */
 	struct kobject kobj;
+	struct list_head list;
 	struct completion finish;
+	unsigned int refs;
 
 	/* a patch shall set these */
 	const char *name;
--- a/kernel/kgraft.c
+++ b/kernel/kgraft.c
@@ -20,6 +20,7 @@
 #include <linux/hardirq.h> /* for in_interrupt() */
 #include <linux/kallsyms.h>
 #include <linux/kgraft.h>
+#include <linux/list.h>
 #include <linux/livepatch.h>
 #include <linux/module.h>
 #include <linux/percpu.h>
@@ -37,6 +38,7 @@ static void kgr_work_fn(struct work_stru
 static struct workqueue_struct *kgr_wq;
 static DECLARE_DELAYED_WORK(kgr_work, kgr_work_fn);
 static DEFINE_MUTEX(kgr_in_progress_lock);
+static LIST_HEAD(kgr_patches);
 static bool __percpu *kgr_irq_use_new;
 bool kgr_in_progress;
 static bool kgr_initialized;
@@ -89,19 +91,37 @@ static notrace void kgr_stub_slow(unsign
 	/* Redirect the function unless we continue with the original one. */
 	if (go_new)
 		klp_arch_set_pc(regs, (unsigned long)p->new_fun);
+	else if (p->loc_old != p->loc_name)
+		klp_arch_set_pc(regs, p->loc_old);
+}
+
+static void kgr_refs_inc(void)
+{
+	struct kgr_patch *p;
+
+	list_for_each_entry(p, &kgr_patches, list)
+		p->refs++;
+}
+
+static void kgr_refs_dec(void)
+{
+	struct kgr_patch *p;
+
+	list_for_each_entry(p, &kgr_patches, list)
+		p->refs--;
 }
 
 static int kgr_ftrace_enable(struct kgr_patch_fun *pf, struct ftrace_ops *fops)
 {
 	int ret;
 
-	ret = ftrace_set_filter_ip(fops, pf->loc_old, 0, 0);
+	ret = ftrace_set_filter_ip(fops, pf->loc_name, 0, 0);
 	if (ret)
 		return ret;
 
 	ret = register_ftrace_function(fops);
 	if (ret)
-		ftrace_set_filter_ip(fops, pf->loc_old, 1, 0);
+		ftrace_set_filter_ip(fops, pf->loc_name, 1, 0);
 
 	return ret;
 }
@@ -114,7 +134,7 @@ static int kgr_ftrace_disable(struct kgr
 	if (ret)
 		return ret;
 
-	ret = ftrace_set_filter_ip(fops, pf->loc_old, 1, 0);
+	ret = ftrace_set_filter_ip(fops, pf->loc_name, 1, 0);
 	if (ret)
 		register_ftrace_function(fops);
 
@@ -157,8 +177,11 @@ static void kgr_finalize(void)
 
 	free_percpu(kgr_irq_use_new);
 
-	if (kgr_revert)
+	if (kgr_revert) {
+		kgr_refs_dec();
 		module_put(kgr_patch->owner);
+	} else
+		list_add_tail(&kgr_patch->list, &kgr_patches);
 
 	kgr_patch = NULL;
 	kgr_in_progress = false;
@@ -266,6 +289,116 @@ static void kgr_handle_irqs(void)
 	schedule_on_each_cpu(kgr_handle_irq_cpu);
 }
 
+/*
+ * There might be different variants of a function in different patches.
+ * The patches are stacked in the order in which they are added. The variant
+ * of a function from a newer patch takes precedence over the older variants
+ * and makes the older variants unused.
+ *
+ * There might be an interim time when two variants of the same function
+ * might be used by the system. Therefore we split the patches into two
+ * categories.
+ *
+ * One patch might be in progress. It is either being added or being reverted.
+ * In each case, there might be threads that are using the code from this patch
+ * and also threads that are using the old code. Where the old code is the
+ * original code or the code from the previous patch if any. This patch
+ * might be found in the variable kgr_patch.
+ *
+ * The other patches are finalized. It means that the whole system started
+ * using them at some point. Note that some parts of the patches might be
+ * unused when there appeared new variants in newer patches. Also some threads
+ * might already started using the patch in progress. Anyway, the finalized
+ * patches might be found in the list kgr_patches.
+ *
+ * When manipulating the patches, we need to search and check the right variant
+ * of a function on the stack. The following types are used to define
+ * the requested variant.
+ */
+enum kgr_find_type {
+	/*
+	 * Find previous function variant in respect to stacking. Take
+	 * into account even the patch in progress that is considered to be
+	 * on top of the stack.
+	 */
+	KGR_PREVIOUS,
+	/* Find the last finalized variant of the function on the stack. */
+	KGR_LAST_FINALIZED,
+	/*
+	 * Find the last variant of the function on the stack. Take into
+	 * account even the patch in progress.
+	 */
+	KGR_LAST_EXISTING,
+	/* Find the variant of the function _only_ in the patch in progress. */
+	KGR_IN_PROGRESS,
+	/*
+	 * This is the first unused find type. It can be used to check for
+	 * invalid value.
+	 */
+	KGR_LAST_TYPE
+};
+
+/*
+ * This function takes information about the patched function from the given
+ * struct kgr_patch_fun and tries to find the requested variant of the
+ * function. It returns NULL when the requested variant cannot be found.
+ */
+static struct kgr_patch_fun *
+kgr_get_patch_fun(const struct kgr_patch_fun *patch_fun,
+		  enum kgr_find_type type)
+{
+	const char *name = patch_fun->name;
+	struct kgr_patch_fun *pf, *found_pf = NULL;
+	struct kgr_patch *p;
+
+	if (type < 0 || type >= KGR_LAST_TYPE) {
+		pr_warn("kgr_get_patch_fun: invalid find type: %d\n", type);
+		return NULL;
+	}
+
+	if (kgr_patch && (type == KGR_IN_PROGRESS || type == KGR_LAST_EXISTING))
+		kgr_for_each_patch_fun(kgr_patch, pf)
+			if (!strcmp(pf->name, name))
+				return pf;
+
+	if (type == KGR_IN_PROGRESS)
+		goto out;
+
+	list_for_each_entry(p, &kgr_patches, list) {
+		kgr_for_each_patch_fun(p, pf) {
+			if (type == KGR_PREVIOUS && pf == patch_fun)
+				goto out;
+
+			if (!strcmp(pf->name, name))
+				found_pf = pf;
+		}
+	}
+out:
+	return found_pf;
+}
+
+static unsigned long kgr_get_old_fun(const struct kgr_patch_fun *patch_fun)
+{
+	struct kgr_patch_fun *pf = kgr_get_patch_fun(patch_fun, KGR_PREVIOUS);
+
+	if (pf)
+		return (unsigned long)pf->new_fun;
+
+	return patch_fun->loc_name;
+}
+
+/*
+ * Obtain the "previous" (in the sense of patch stacking) value of ftrace_ops
+ * so that it can be put back properly in case of reverting the patch
+ */
+static struct ftrace_ops *
+kgr_get_old_fops(const struct kgr_patch_fun *patch_fun)
+{
+	struct kgr_patch_fun *pf = kgr_get_patch_fun(patch_fun, KGR_PREVIOUS);
+
+	return pf ? &pf->ftrace_ops_fast : NULL;
+}
+
 static int kgr_switch_fops(struct kgr_patch_fun *patch_fun,
 		struct ftrace_ops *new_fops, struct ftrace_ops *unreg_fops)
 {
@@ -313,6 +446,14 @@ static int kgr_init_ftrace_ops(struct kg
 	if (IS_ERR_VALUE(addr))
 		return addr;
 
+	pr_debug("kgr: storing %lx to loc_name for %s\n",
+			addr, patch_fun->name);
+	patch_fun->loc_name = addr;
+
+	addr = kgr_get_old_fun(patch_fun);
+	if (IS_ERR_VALUE(addr))
+		return addr;
+
 	pr_debug("kgr: storing %lx to loc_old for %s\n",
 			addr, patch_fun->name);
 	patch_fun->loc_old = addr;
@@ -353,6 +494,12 @@ static int kgr_patch_code(struct kgr_pat
 
 		next_state = KGR_PATCH_SLOW;
 		new_ops = &patch_fun->ftrace_ops_slow;
+		/*
+		 * If some previous patch already patched a function, the old
+		 * fops need to be disabled, otherwise the new redirection will
+		 * never be used.
+		 */
+		unreg_ops = kgr_get_old_fops(patch_fun);
 		break;
 	case KGR_PATCH_SLOW:
 		if (revert || !final)
@@ -373,6 +520,11 @@ static int kgr_patch_code(struct kgr_pat
 			return -EINVAL;
 		next_state = KGR_PATCH_REVERTED;
 		unreg_ops = &patch_fun->ftrace_ops_slow;
+		/*
+		 * Put back in place the old fops that were deregistered in
+		 * case of stacked patching (see the comment above).
+		 */
+		new_ops = kgr_get_old_fops(patch_fun);
 		break;
 	case KGR_PATCH_SKIPPED:
 		return 0;
@@ -406,12 +558,24 @@ int kgr_modify_kernel(struct kgr_patch *
 	}
 
 	mutex_lock(&kgr_in_progress_lock);
+	if (patch->refs) {
+		pr_err("kgr: can't patch, this patch is still referenced\n");
+		ret = -EBUSY;
+		goto err_unlock;
+	}
+
 	if (kgr_in_progress) {
 		pr_err("kgr: can't patch, another patching not yet finalized\n");
 		ret = -EAGAIN;
 		goto err_unlock;
 	}
 
+	if (revert && list_empty(&patch->list)) {
+		pr_err("kgr: can't patch, this one was already reverted\n");
+		ret = -EINVAL;
+		goto err_unlock;
+	}
+
 	kgr_irq_use_new = alloc_percpu(bool);
 	if (!kgr_irq_use_new) {
 		pr_err("kgr: can't patch, cannot allocate percpu data\n");
@@ -443,6 +607,10 @@ int kgr_modify_kernel(struct kgr_patch *
 	kgr_in_progress = true;
 	kgr_patch = patch;
 	kgr_revert = revert;
+	if (revert)
+		list_del_init(&patch->list); /* init for list_empty() above */
+	else
+		kgr_refs_inc();
 	mutex_unlock(&kgr_in_progress_lock);
 
 	kgr_handle_irqs();
--- a/kernel/kgraft_files.c
+++ b/kernel/kgraft_files.c
@@ -57,6 +57,14 @@ static ssize_t state_show(struct kobject
 	return size;
 }
 
+static ssize_t refs_show(struct kobject *kobj, struct kobj_attribute *attr,
+		char *buf)
+{
+	struct kgr_patch *p = kobj_to_patch(kobj);
+
+	return snprintf(buf, PAGE_SIZE, "%d\n", p->refs);
+}
+
 static ssize_t revert_store(struct kobject *kobj,
 		struct kobj_attribute *attr, const char *buf, size_t count)
 {
@@ -69,10 +77,12 @@ static ssize_t revert_store(struct kobje
 }
 
 static struct kobj_attribute kgr_attr_state = __ATTR_RO(state);
+static struct kobj_attribute kgr_attr_refs = __ATTR_RO(refs);
 static struct kobj_attribute kgr_attr_revert = __ATTR_WO(revert);
 
 static struct attribute *kgr_patch_sysfs_entries[] = {
 	&kgr_attr_state.attr,
+	&kgr_attr_refs.attr,
 	&kgr_attr_revert.attr,
 	NULL
 };
