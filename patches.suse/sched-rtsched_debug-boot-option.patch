Subject: sched: Add rtsched_debug boot option
From: Mike Galbraith <mgalbraith@suse.de>
Date: Thu Jul 28 13:39:19 CEST 2011
Patch-mainline: never, SUSE specific
References: bnc#708730

Provide a boot option which turns off runtime borrowing, turning
the throttle into a strict hard limit per runqueue, so users
can debug their realtime code without killing their box.

Emit notification when the hard limit has been reached, and very
scary warning if the runqueue is approaching 100% CPU.

[  104.949009] RT: throttling CPU2
[  104.949011] RT: Danger!  (mlock) is potential runaway.

Signed-off-by: Mike Galbraith <mgalbraith@suse.de>
---
 Documentation/kernel-parameters.txt |    3 ++
 kernel/sched/core.c                 |    2 -
 kernel/sched/rt.c                   |   44 +++++++++++++++++++++++++++++++-----
 3 files changed, 42 insertions(+), 7 deletions(-)

--- a/Documentation/kernel-parameters.txt
+++ b/Documentation/kernel-parameters.txt
@@ -2844,6 +2844,9 @@ bytes respectively. Such letter suffixes
 
 	sched_debug	[KNL] Enables verbose scheduler debug messages.
 
+	rtsched_debug   [KNL] Disallows borrowing of excess rt_runtime from neighboring runqueues.
+			  /proc/sys/kernel/sched_rt_runtime_us becomes a hard limit per runqueue.
+
 	skew_tick=	[KNL] Offset the periodic timer tick per cpu to mitigate
 			xtime_lock contention on larger systems, and/or RCU lock
 			contention on all systems with CONFIG_MAXSMP set.
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -296,8 +296,6 @@ __read_mostly int scheduler_running;
  */
 int sysctl_sched_rt_runtime = 950000;
 
-
-
 /*
  * __task_rq_lock - lock the rq @p resides on.
  */
--- a/kernel/sched/rt.c
+++ b/kernel/sched/rt.c
@@ -698,19 +698,46 @@ static void __enable_runtime(struct rq *
 	}
 }
 
+int __read_mostly sysctl_sched_rtsched_debug;
+
 static int balance_runtime(struct rt_rq *rt_rq)
 {
 	int more = 0;
 
-	if (!sched_feat(RT_RUNTIME_SHARE))
+	if (likely(rt_rq->rt_time <= rt_rq->rt_runtime))
+		return more;
+
+	if (unlikely(sysctl_sched_rtsched_debug)) {
+		struct rq *rq;
+		struct task_struct *p;
+		u64 period;
+
+		if (!printk_ratelimit())
+			return more;
+
+		rq = rq_of_rt_rq(rt_rq);
+		printk(KERN_WARNING "RT: throttling CPU%d\n", rq->cpu);
+
+		if (!rt_task(rq->curr))
+			return more;
+
+		period = sched_rt_period(rt_rq);
+		p = rq->curr;
+
+		if (rt_rq->rt_time > period - (period >> 3))
+			printk(KERN_WARNING "RT: Danger!  (%s) is potential runaway.\n", p->comm);
+
 		return more;
 
-	if (rt_rq->rt_time > rt_rq->rt_runtime) {
-		raw_spin_unlock(&rt_rq->rt_runtime_lock);
-		more = do_balance_runtime(rt_rq);
-		raw_spin_lock(&rt_rq->rt_runtime_lock);
 	}
 
+	if (!sched_feat(RT_RUNTIME_SHARE))
+		return more;
+
+	raw_spin_unlock(&rt_rq->rt_runtime_lock);
+	more = do_balance_runtime(rt_rq);
+	raw_spin_lock(&rt_rq->rt_runtime_lock);
+
 	return more;
 }
 #else /* !CONFIG_SMP */
@@ -2024,3 +2051,10 @@ void print_rt_stats(struct seq_file *m,
 	rcu_read_unlock();
 }
 #endif /* CONFIG_SCHED_DEBUG */
+
+static int __init parse_rtsched_debug(char *arg)
+{
+	sysctl_sched_rtsched_debug = 1;
+	return 0;
+}
+early_param("rtsched_debug", parse_rtsched_debug);
