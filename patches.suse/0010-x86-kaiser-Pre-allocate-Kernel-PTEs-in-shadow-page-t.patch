From 64ae76392aa809e9a092298311cce48d00bbf897 Mon Sep 17 00:00:00 2001
From: Joerg Roedel <jroedel@suse.de>
Date: Tue, 13 Mar 2018 15:41:39 +0100
Subject: [PATCH 10/17] x86/kaiser: Pre-allocate Kernel-PTEs in shadow
 page-table for 32 bit
References: bsc#1068032 CVE-2017-5754
Patch-mainline: No, different upstream implementation

This makes sure the second-level page-table pages get
propagated into every mm's shadow-page-table, even when the
page-table has only 2 levels.

This is needed to make the VDSO compat-mapping and the LDT
mapping work with legacy paging.

Signed-off-by: Joerg Roedel <jroedel@suse.de>
---
 arch/x86/mm/kaiser.c | 16 +++++++++++++++-
 1 file changed, 15 insertions(+), 1 deletion(-)

diff --git a/arch/x86/mm/kaiser.c b/arch/x86/mm/kaiser.c
index 156f75c..b7fe0a3 100644
--- a/arch/x86/mm/kaiser.c
+++ b/arch/x86/mm/kaiser.c
@@ -237,11 +237,12 @@ static int kaiser_add_user_map_ptrs(const void *start, const void *end, pteval_t
  */
 static void __init kaiser_init_all_pgds(void)
 {
+#ifdef CONFIG_X86_64
 	pgd_t *pgd;
 	int i = 0;
 
 	pgd = native_get_shadow_pgd(pgd_offset_k((unsigned long )0));
-	for (i = PTRS_PER_PGD / 2; i < PTRS_PER_PGD; i++) {
+	for (i = KERNEL_PGD_BOUNDARY; i < PTRS_PER_PGD; i++) {
 		pgd_t new_pgd;
 		pud_t *pud = pud_alloc_one(&init_mm,
 					   PAGE_OFFSET + i * PGDIR_SIZE);
@@ -259,6 +260,19 @@ static void __init kaiser_init_all_pgds(void)
 		}
 		set_pgd(pgd + i, new_pgd);
 	}
+#else
+	/*
+	 * On 32 bit we pre-allocate the PTEs for the kernel so that any
+	 * changes there get propagated into every existing page-table.
+	 */
+	unsigned long addr;
+
+	for (addr = PAGE_OFFSET;
+	     (addr >= PAGE_OFFSET) && (addr < FIXADDR_TOP);
+	    addr += PMD_SIZE) {
+		kaiser_pagetable_walk(addr);
+	}
+#endif
 }
 
 #define kaiser_add_user_map_early(start, size, flags) do {	\
-- 
1.8.5.6

