From: Jiri Slaby <jslaby@suse.cz>
Date: Fri, 21 Feb 2014 13:55:26 +0100
Subject: mm: reintroduce global_reclaimable_page
Patch-mainline: never
References: FATE309111

This reverts parts of commit 414f6b9f9fa13c636c7dc136f958cd1911fa62d0
(mm/page-writeback.c: do not count anon pages as dirtyable memory)
from 3.12.11, upstream commit a1c3bfb2f67ef766de03f1f56bdfff9c859. It
removed global_reclaimable_page function which we call from our
"Patch-mainline: never" patch:
patches.suse/pagecache-limit.patch

So add it back, but mark it static to be used solely by the patch. It
is too late to change the behaviour now.

mhocko comments:
Although using NR_{IN}ACTIVE_FILE should be OK here, let's stick with
the old behavior and change it later.

Signed-off-by: Jiri Slaby <jslaby@suse.cz>
Acked-by: Michal Hocko <mhocko@suse.cz>
---
 mm/vmscan.c |   21 +++++++++++++++++++++
 1 file changed, 21 insertions(+)

--- a/mm/vmscan.c
+++ b/mm/vmscan.c
@@ -3297,6 +3297,27 @@ void wakeup_kswapd(struct zone *zone, in
 	wake_up_interruptible(&pgdat->kswapd_wait);
 }
 
+/*
+ * The reclaimable count would be mostly accurate.
+ * The less reclaimable pages may be
+ * - mlocked pages, which will be moved to unevictable list when encountered
+ * - mapped pages, which may require several travels to be reclaimed
+ * - dirty pages, which is not "instantly" reclaimable
+ */
+static unsigned long global_reclaimable_pages(void)
+{
+	int nr;
+
+	nr = global_page_state(NR_ACTIVE_FILE) +
+	     global_page_state(NR_INACTIVE_FILE);
+
+	if (get_nr_swap_pages() > 0)
+		nr += global_page_state(NR_ACTIVE_ANON) +
+		      global_page_state(NR_INACTIVE_ANON);
+
+	return nr;
+}
+
 #ifdef CONFIG_HIBERNATION
 /*
  * Try to free `nr_to_reclaim' of memory, system-wide, and return the number of
