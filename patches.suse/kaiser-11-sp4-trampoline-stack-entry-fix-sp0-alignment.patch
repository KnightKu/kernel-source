From: Jiri Kosina <jkosina@suse.cz>
Subject: [PATCH v2] x86/kaiser: properly align trampoline stack
References: bsc#1087260
Patch-mainline: Never, SUSE specific

If 16-byte alignment of the trampoline stack top is not maintained, int 0x80
(or generally anything going through GATE_INTERRUPT) issued by compat 32bit
binary observes this exception stackframe on a trampoline stack:

	00000000f7ff382b 0000000000000023 0000000000000296 00000000ffffd554
	000000000000002b 000000000000002b 0000000000000000 0000000000000000
	0000000000000000 0000000000000000 0000000000000000 0000000000000000

This is wrong, as there is duplicated 0x2b (user stack segment) pushed at
tss->sp0, which confuses anything that assumes that pt_regs + iret frame
ends exactly at the address tss->sp0 points to (such as, but not strictly
limited to, task_pt_regs()).

This happens because

- on every INT instruction, it's guaranteed that the CPU aligns the stack
  to 16-bytes
- the places in the kernel that construct the iret frame manually (such
  as x86_64 SYSENTER) don't perform this re-alignment

Therefore the spurious 0x2b observed on the compat int 0x80 trampoline
stack is a leftover from e.g. previous 64bit SYSENTER, and anything that
starts processing pt_regs + iret frame from tss->sp0 gets immediately
8 bytes offset.

Once proper 16-byte alignment is established, the discrepancy is gone
and ia32_syscall observes correct entry stack layout

        00000000f7ff382b 0000000000000023 0000000000000296 00000000ffffd554
        000000000000002b 0000000000000000 0000000000000000 0000000000000000
        0000000000000000 0000000000000000 0000000000000000 0000000000000000

While inserting the padding, let's also install a build-time check whether
the alignment didn't get broken by mistake.

In addition to that, let's make sure that TSS is page-aligned, as that's
what Intel SDM requires (specifically, it requires first 104 bytes to be
in the same PFN, otherwise behavior is undefined).

Signed-off-by: Jiri Kosina <jkosina@suse.cz>
---

v1 -> v2: rewrite changelog to make more sense

 arch/x86/include/asm/processor.h |    6 ++++--
 arch/x86/kernel/cpu/common.c     |    7 +++++++
 arch/x86/kernel/init_task.c      |    2 +-
 3 files changed, 12 insertions(+), 3 deletions(-)

--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -262,12 +262,14 @@ struct tss_struct {
 	/*
 	 * .. and then another 0x100 bytes for the emergency kernel stack:
 	 */
+	/* IRQ stacks have to maintain 16-bytes alignment! */
+	u8			pad;
 	unsigned long		stack[64];
 
-} ____cacheline_aligned;
+} __attribute__((__aligned__(PAGE_SIZE)));
 
 #ifndef __GENKSYMS__
-DECLARE_PER_CPU_SHARED_ALIGNED_USER_MAPPED(struct tss_struct, init_tss);
+DECLARE_PER_CPU_PAGE_ALIGNED_USER_MAPPED(struct tss_struct, init_tss);
 #else
 DECLARE_PER_CPU_SHARED_ALIGNED(struct tss_struct, init_tss);
 #endif
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1345,6 +1345,13 @@ void __cpuinit cpu_init(void)
 	ACCESS_ONCE(t->x86_tss.sp0) = ACCESS_ONCE(current->thread.sp0);
 	current->thread.sp0 = v;        /* Restore original value */
 
+	/*
+	 * TSS is kept page-aligned (mandated by Intel SDM, Volume 3, 7.2.1)
+	 * and stack residing in it has to be 16-bytes aligned so that it
+	 * can serve as stack for IRQ handlers
+	 */
+	BUILD_BUG_ON(offsetofend(struct tss_struct, stack) % 16 != 0);
+
 	set_tss_desc(cpu, t);
 	load_TR_desc();
 	load_mm_ldt(&init_mm);
--- a/arch/x86/kernel/init_task.c
+++ b/arch/x86/kernel/init_task.c
@@ -39,7 +39,7 @@ EXPORT_SYMBOL(init_task);
  * on exact cacheline boundaries, to eliminate cacheline ping-pong.
  */
 #if defined(CONFIG_GENKSYMS)
-DEFINE_PER_CPU_SHARED_ALIGNED(struct tss_struct, init_tss) = INIT_TSS;
+DEFINE_PER_CPU_PAGE_ALIGNED(struct tss_struct, init_tss) = INIT_TSS;
 #else
 DEFINE_PER_CPU_SHARED_ALIGNED_USER_MAPPED(struct tss_struct, init_tss) = INIT_TSS;
 #endif
