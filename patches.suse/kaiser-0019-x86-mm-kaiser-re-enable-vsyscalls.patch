From ecd5a23ba4031f4cd1c9225d30ae5d210d65fc1b Mon Sep 17 00:00:00 2001
From: Andrea Arcangeli <aarcange@redhat.com>
Date: Tue, 5 Dec 2017 21:15:07 +0100
Subject: [PATCH 19/19] x86/mm/kaiser: re-enable vsyscalls

To avoid breaking the kernel ABI.

Signed-off-by: Andrea Arcangeli <aarcange@redhat.com>

hughd adjusted it to the 3.2.96 tree (leaving out the PVCLOCK_FIXMAP
user mapping, which does not apply to this tree); and for safety
added vsyscall_pgprot, and a BUG_ON if _PAGE_USER outside of FIXMAP.
---
 arch/x86/include/asm/vsyscall.h |    1 +
 arch/x86/kernel/hpet.c          |    3 +++
 arch/x86/kernel/vsyscall_64.c   |    4 +++-
 arch/x86/mm/kaiser.c            |   14 +++++++++++---
 4 files changed, 18 insertions(+), 4 deletions(-)

--- a/arch/x86/include/asm/vsyscall.h
+++ b/arch/x86/include/asm/vsyscall.h
@@ -26,6 +26,7 @@ enum vsyscall_num {
 /* kernel space (writeable) */
 extern int vgetcpu_mode;
 extern struct timezone sys_tz;
+extern unsigned long vsyscall_pgprot;
 
 #include <asm/vvar.h>
 
--- a/arch/x86/kernel/hpet.c
+++ b/arch/x86/kernel/hpet.c
@@ -10,6 +10,7 @@
 #include <linux/cpu.h>
 #include <linux/pm.h>
 #include <linux/io.h>
+#include <linux/kaiser.h>
 
 #include <asm/fixmap.h>
 #include <asm/i8253.h>
@@ -100,6 +101,8 @@ static inline void hpet_set_mapping(void
 	hpet_virt_address = ioremap_nocache(hpet_address, HPET_MMAP_SIZE);
 #ifdef CONFIG_X86_64
 	__set_fixmap(VSYSCALL_HPET, hpet_address, PAGE_KERNEL_VVAR_NOCACHE);
+	kaiser_add_mapping(__fix_to_virt(VSYSCALL_HPET), PAGE_SIZE,
+			   __PAGE_KERNEL_VVAR_NOCACHE);
 #endif
 }
 
--- a/arch/x86/kernel/vsyscall_64.c
+++ b/arch/x86/kernel/vsyscall_64.c
@@ -49,6 +49,8 @@
 		__attribute__ ((unused, __section__(".vsyscall_" #nr))) notrace
 #define __syscall_clobber "r11","cx","memory"
 
+unsigned long vsyscall_pgprot = __PAGE_KERNEL_VSYSCALL;
+
 DEFINE_VVAR(int, vgetcpu_mode);
 DEFINE_VVAR(struct vsyscall_gtod_data, vsyscall_gtod_data) =
 {
@@ -288,7 +290,7 @@ void __init map_vsyscall(void)
 	unsigned long physaddr_vvar_page = __pa_symbol(&__vvar_page);
 
 	/* Note that VSYSCALL_MAPPED_PAGES must agree with the code below. */
-	__set_fixmap(VSYSCALL_FIRST_PAGE, physaddr_page0, PAGE_KERNEL_VSYSCALL);
+	__set_fixmap(VSYSCALL_FIRST_PAGE, physaddr_page0, __pgprot(vsyscall_pgprot));
 	__set_fixmap(VVAR_PAGE, physaddr_vvar_page, PAGE_KERNEL_VVAR);
 	BUILD_BUG_ON((unsigned long)__fix_to_virt(VVAR_PAGE) !=
 		     (unsigned long)VVAR_ADDRESS);
--- a/arch/x86/mm/kaiser.c
+++ b/arch/x86/mm/kaiser.c
@@ -15,6 +15,7 @@ extern struct mm_struct init_mm;
 
 #include <asm/kaiser.h>
 #include <asm/tlbflush.h>	/* to verify its kaiser declarations */
+#include <asm/vsyscall.h>
 #include <asm/pgtable.h>
 #include <asm/pgalloc.h>
 #include <asm/desc.h>
@@ -135,7 +136,7 @@ static pte_t *kaiser_pagetable_walk(unsi
 			return NULL;
 		spin_lock(&shadow_table_allocation_lock);
 		if (pud_none(*pud)) {
-			set_pud(pud, __pud(_KERNPG_TABLE | __pa(new_pmd_page)));
+			set_pud(pud, __pud(_PAGE_TABLE | __pa(new_pmd_page)));
 			__inc_zone_page_state(virt_to_page((void *)
 						new_pmd_page), NR_KAISERTABLE);
 		} else
@@ -155,7 +156,7 @@ static pte_t *kaiser_pagetable_walk(unsi
 			return NULL;
 		spin_lock(&shadow_table_allocation_lock);
 		if (pmd_none(*pmd)) {
-			set_pmd(pmd, __pmd(_KERNPG_TABLE | __pa(new_pte_page)));
+			set_pmd(pmd, __pmd(_PAGE_TABLE | __pa(new_pte_page)));
 			__inc_zone_page_state(virt_to_page((void *)
 						new_pte_page), NR_KAISERTABLE);
 		} else
@@ -176,6 +177,9 @@ int kaiser_add_user_map(const void *__st
 	unsigned long end_addr = PAGE_ALIGN(start_addr + size);
 	unsigned long target_address;
 
+	if (flags & _PAGE_USER)
+		BUG_ON(address < FIXADDR_START || end_addr >= FIXADDR_TOP);
+
 	for (; address < end_addr; address += PAGE_SIZE) {
 		target_address = get_pa_from_mapping(address);
 		if (target_address == -1) {
@@ -229,7 +233,7 @@ static void __init kaiser_init_all_pgds(
 			break;
 		}
 		inc_zone_page_state(virt_to_page(pud), NR_KAISERTABLE);
-		new_pgd = __pgd(_KERNPG_TABLE |__pa(pud));
+		new_pgd = __pgd(_PAGE_TABLE |__pa(pud));
 		/*
 		 * Make sure not to stomp on some other pgd entry.
 		 */
@@ -288,6 +292,10 @@ void __init kaiser_init(void)
 	kaiser_add_user_map_early((void *)idt_descr.address,
 				  sizeof(gate_desc) * NR_VECTORS,
 				  __PAGE_KERNEL_RO);
+	kaiser_add_user_map_early((void *)VVAR_ADDRESS, PAGE_SIZE,
+				  __PAGE_KERNEL_VVAR);
+	kaiser_add_user_map_early((void *)VSYSCALL_START, PAGE_SIZE,
+				  vsyscall_pgprot);
 	kaiser_add_user_map_early(&x86_cr3_pcid_noflush,
 				  sizeof(x86_cr3_pcid_noflush),
 				  __PAGE_KERNEL);
