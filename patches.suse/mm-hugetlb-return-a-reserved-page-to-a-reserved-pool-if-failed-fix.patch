From ddbc38fba0a3daf89d178876845754e89acbf935 Mon Sep 17 00:00:00 2001
From: Roberto Sassu <rsassu@suse.de>
Date: Mon, 21 Mar 2016 14:50:20 +0100
Subject: [PATCH] mm/hugetlb: fix backport of upstream commit 07443a85ad
References: VM Functionality, bnc#971446
Patch-mainline: v3.12
Git-commit: 07443a85ad90c7b62fbe11dcd3d6a1de1e10516f

With the backport of commit 07443a85ad (patches.suse/
mm-hugetlb-return-a-reserved-page-to-a-reserved-pool-if-failed.patch)
the increment of h->resv_huge_pages is done without the lock
hugetlb_lock held. This can result in racy r-m-w cycle, and the
increment can be lost. Lost increment means it's then possible
to eventually decrement below zero.

This patch fixes the issue by moving the incrementing of h->resv_huge_pages
after the hugetlb_lock lock is held.

Signed-off-by: Roberto Sassu <rsassu@suse.de>
Signed-off-by: Vlastimil Babka <vbabka@suse.cz>
---
 mm/hugetlb.c |    6 ++++--
 1 file changed, 4 insertions(+), 2 deletions(-)

--- a/mm/hugetlb.c
+++ b/mm/hugetlb.c
@@ -629,16 +629,18 @@ static void free_huge_page(struct page *
 	int nid = page_to_nid(page);
 	struct hugepage_subpool *spool =
 		(struct hugepage_subpool *)page_private(page);
+	bool restore_reserve;
 
 	set_page_private(page, 0);
 	page->mapping = NULL;
 	BUG_ON(page_count(page));
 	BUG_ON(page_mapcount(page));
+	restore_reserve = PagePrivate(page);
 	INIT_LIST_HEAD(&page->lru);
-	if (PagePrivate(page))
+	spin_lock(&hugetlb_lock);
+	if (restore_reserve)
 		h->resv_huge_pages++;
 
-	spin_lock(&hugetlb_lock);
 	if (h->surplus_huge_pages_node[nid] && huge_page_order(h) < MAX_ORDER) {
 		update_and_free_page(h, page);
 		h->surplus_huge_pages--;
