From: Jiri Slaby <jslaby@suse.cz>
Date: Tue, 8 Jul 2014 11:09:24 +0200
Subject: kgr: allow stacking of patches
Patch-mainline: submitted for review
References: fate#313296

We may want to patch some function twice. kGraft does not support this
yet. So take care of this path correctly by remembering all patches in
a list and walking the list when adding another patch. If it patches
the same function, set the 'loc_old' to already patched function, not
the original one.

kgr_patch_code() also has to unregister the already existing
ftrace_ops on a given location. Otherwise it would make ftrace to
always redirect to the old function. Hence we need to set unreg_ops in
KGR_PATCH_INIT to remove the ftrace_ops redirection installed by some
previous patching run.

Symmetrically, we need to handle reverts properly as well, i.e
KGR_PATCH_REVERT_SLOW needs to set new_ops to the version of the ops that
has been there before second patch came in.

On the top of that ensure that patches are reverted in the correct
order by incrementing the reference count of previous patches.

js: do not chain ftrace locations, always jump from 'name'
jk: fix patch stacking
js: de-duplicate code and extract to kgr_get_last_pf
js: prepend kgr_ to the list of patches

Signed-off-by: Jiri Slaby <jslaby@suse.cz>
Signed-off-by: Jiri Kosina <jkosina@suse.cz>
---
 include/linux/kgraft.h |  9 ++++-
 kernel/kgraft.c        | 98 ++++++++++++++++++++++++++++++++++++++++++++++++--
 kernel/kgraft_files.c  | 10 ++++++
 3 files changed, 113 insertions(+), 4 deletions(-)

diff --git a/include/linux/kgraft.h b/include/linux/kgraft.h
index f7cc1f769e53..de21eec63745 100644
--- a/include/linux/kgraft.h
+++ b/include/linux/kgraft.h
@@ -20,6 +20,7 @@
 #include <linux/bitops.h>
 #include <linux/compiler.h>
 #include <linux/kobject.h>
+#include <linux/list.h>
 #include <linux/ftrace.h>
 #include <linux/sched.h>
 
@@ -36,7 +37,8 @@ struct kgr_patch;
  *
  * @name: function to patch
  * @new_fun: function with the new body
- * @loc_old: cache of @name's fentry
+ * @loc_name: cache of @name's fentry
+ * @loc_old: cache of the last entry for @name in the patches list
  * @loc_new: cache of @new_name's fentry
  * @ftrace_ops_slow: ftrace ops for slow (temporary) stub
  * @ftrace_ops_fast: ftrace ops for fast () stub
@@ -59,6 +61,7 @@ struct kgr_patch_fun {
 		KGR_PATCH_SKIPPED,
 	} state;
 
+	unsigned long loc_name;
 	unsigned long loc_old;
 	unsigned long loc_new;
 
@@ -70,8 +73,10 @@ struct kgr_patch_fun {
  * struct kgr_patch -- a kGraft patch
  *
  * @kobj: object representing the sysfs entry
+ * @list: member in patches list
  * @finish: waiting till it is safe to remove the module with the patch
  * @irq_use_new: per-cpu array to remember kGraft state for interrupts
+ * @refs: how many patches need to be reverted before this one
  * @name: name of the patch (to appear in sysfs)
  * @owner: module to refcount on patching
  * @patches: array of @kgr_patch_fun structures
@@ -79,8 +84,10 @@ struct kgr_patch_fun {
 struct kgr_patch {
 	/* internal state information */
 	struct kobject kobj;
+	struct list_head list;
 	struct completion finish;
 	bool __percpu *irq_use_new;
+	unsigned int refs;
 
 	/* a patch shall set these */
 	const char *name;
diff --git a/kernel/kgraft.c b/kernel/kgraft.c
index cf7f382e54ac..fc0f048637a7 100644
--- a/kernel/kgraft.c
+++ b/kernel/kgraft.c
@@ -18,6 +18,7 @@
 #include <linux/hardirq.h> /* for in_interrupt() */
 #include <linux/kallsyms.h>
 #include <linux/kgraft.h>
+#include <linux/list.h>
 #include <linux/module.h>
 #include <linux/percpu.h>
 #include <linux/sched.h>
@@ -34,6 +35,7 @@ static void kgr_work_fn(struct work_struct *work);
 static struct workqueue_struct *kgr_wq;
 static DECLARE_DELAYED_WORK(kgr_work, kgr_work_fn);
 static DEFINE_MUTEX(kgr_in_progress_lock);
+static LIST_HEAD(kgr_patches);
 bool kgr_in_progress;
 static bool kgr_initialized;
 static struct kgr_patch *kgr_patch;
@@ -77,17 +79,33 @@ static void kgr_stub_slow(unsigned long ip, unsigned long parent_ip,
 	}
 }
 
+static void kgr_refs_inc(void)
+{
+	struct kgr_patch *p;
+
+	list_for_each_entry(p, &kgr_patches, list)
+		p->refs++;
+}
+
+static void kgr_refs_dec(void)
+{
+	struct kgr_patch *p;
+
+	list_for_each_entry(p, &kgr_patches, list)
+		p->refs--;
+}
+
 static int kgr_ftrace_enable(struct kgr_patch_fun *pf, struct ftrace_ops *fops)
 {
 	int ret;
 
-	ret = ftrace_set_filter_ip(fops, pf->loc_old, 0, 0);
+	ret = ftrace_set_filter_ip(fops, pf->loc_name, 0, 0);
 	if (ret)
 		return ret;
 
 	ret = register_ftrace_function(fops);
 	if (ret)
-		ftrace_set_filter_ip(fops, pf->loc_old, 1, 0);
+		ftrace_set_filter_ip(fops, pf->loc_name, 1, 0);
 
 	return ret;
 }
@@ -100,7 +118,7 @@ static int kgr_ftrace_disable(struct kgr_patch_fun *pf, struct ftrace_ops *fops)
 	if (ret)
 		return ret;
 
-	ret = ftrace_set_filter_ip(fops, pf->loc_old, 1, 0);
+	ret = ftrace_set_filter_ip(fops, pf->loc_name, 1, 0);
 	if (ret)
 		register_ftrace_function(fops);
 
@@ -142,6 +160,11 @@ static void kgr_finalize(void)
 
 	mutex_lock(&kgr_in_progress_lock);
 	kgr_in_progress = false;
+	if (kgr_revert) {
+		list_del(&kgr_patch->list);
+		kgr_refs_dec();
+	} else
+		list_add_tail(&kgr_patch->list, &kgr_patches);
 	mutex_unlock(&kgr_in_progress_lock);
 }
 
@@ -236,6 +259,48 @@ static void kgr_handle_irqs(void)
 	schedule_on_each_cpu(kgr_handle_irq_cpu);
 }
 
+static struct kgr_patch_fun *
+kgr_get_last_pf(const struct kgr_patch_fun *patch_fun)
+{
+	const char *name = patch_fun->name;
+	struct kgr_patch_fun *pf, *last_pf = NULL;
+	struct kgr_patch *p;
+
+	list_for_each_entry(p, &kgr_patches, list) {
+		kgr_for_each_patch_fun(p, pf) {
+			if (pf->state != KGR_PATCH_APPLIED)
+				continue;
+
+			if (!strcmp(pf->name, name))
+				last_pf = pf;
+		}
+	}
+
+	return last_pf;
+}
+
+static unsigned long kgr_get_old_fun(const struct kgr_patch_fun *patch_fun)
+{
+	struct kgr_patch_fun *pf = kgr_get_last_pf(patch_fun);
+
+	if (pf)
+		return ftrace_function_to_fentry((unsigned long)pf->new_fun);
+
+	return patch_fun->loc_name;
+}
+
+/*
+ * Obtain the "previous" (in the sense of patch stacking) value of ftrace_ops
+ * so that it can be put back properly in case of reverting the patch
+ */
+static struct ftrace_ops *
+kgr_get_old_fops(const struct kgr_patch_fun *patch_fun)
+{
+	struct kgr_patch_fun *pf = kgr_get_last_pf(patch_fun);
+
+	return pf ? &pf->ftrace_ops_fast : NULL;
+}
+
 static int kgr_init_ftrace_ops(struct kgr_patch_fun *patch_fun)
 {
 	struct ftrace_ops *fops;
@@ -262,6 +327,14 @@ static int kgr_init_ftrace_ops(struct kgr_patch_fun *patch_fun)
 	if (IS_ERR_VALUE(fentry_loc))
 		return fentry_loc;
 
+	pr_debug("kgr: storing %lx to loc_name for %s\n",
+			fentry_loc, patch_fun->name);
+	patch_fun->loc_name = fentry_loc;
+
+	fentry_loc = kgr_get_old_fun(patch_fun);
+	if (IS_ERR_VALUE(fentry_loc))
+		return fentry_loc;
+
 	pr_debug("kgr: storing %lx to loc_old for %s\n",
 			fentry_loc, patch_fun->name);
 	patch_fun->loc_old = fentry_loc;
@@ -301,6 +374,12 @@ static int kgr_patch_code(struct kgr_patch_fun *patch_fun, bool final,
 
 		next_state = KGR_PATCH_SLOW;
 		new_ops = &patch_fun->ftrace_ops_slow;
+		/*
+		 * If some previous patch already patched a function, the old
+		 * fops need to be disabled, otherwise the new redirection will
+		 * never be used.
+		 */
+		unreg_ops = kgr_get_old_fops(patch_fun);
 		break;
 	case KGR_PATCH_SLOW:
 		if (revert || !final)
@@ -321,6 +400,11 @@ static int kgr_patch_code(struct kgr_patch_fun *patch_fun, bool final,
 			return -EINVAL;
 		next_state = KGR_PATCH_REVERTED;
 		unreg_ops = &patch_fun->ftrace_ops_slow;
+		/*
+		 * Put back in place the old fops that were deregistered in
+		 * case of stacked patching (see the comment above).
+		 */
+		new_ops = kgr_get_old_fops(patch_fun);
 		break;
 	case KGR_PATCH_SKIPPED:
 		return 0;
@@ -370,6 +454,12 @@ int kgr_modify_kernel(struct kgr_patch *patch, bool revert)
 	}
 
 	mutex_lock(&kgr_in_progress_lock);
+	if (patch->refs) {
+		pr_err("kgr: can't patch, this patch is still referenced\n");
+		ret = -EBUSY;
+		goto err_unlock;
+	}
+
 	if (kgr_in_progress) {
 		pr_err("kgr: can't patch, another patching not yet finalized\n");
 		ret = -EAGAIN;
@@ -406,6 +496,8 @@ int kgr_modify_kernel(struct kgr_patch *patch, bool revert)
 	kgr_in_progress = true;
 	kgr_patch = patch;
 	kgr_revert = revert;
+	if (!revert)
+		kgr_refs_inc();
 	mutex_unlock(&kgr_in_progress_lock);
 
 	kgr_handle_irqs();
diff --git a/kernel/kgraft_files.c b/kernel/kgraft_files.c
index f649ee5645ca..9bcc1b5ae317 100644
--- a/kernel/kgraft_files.c
+++ b/kernel/kgraft_files.c
@@ -56,6 +56,14 @@ static ssize_t state_show(struct kobject *kobj, struct kobj_attribute *attr,
 	return size;
 }
 
+static ssize_t refs_show(struct kobject *kobj, struct kobj_attribute *attr,
+		char *buf)
+{
+	struct kgr_patch *p = kobj_to_patch(kobj);
+
+	return snprintf(buf, PAGE_SIZE, "%d\n", p->refs);
+}
+
 static ssize_t revert_store(struct kobject *kobj,
 		struct kobj_attribute *attr, const char *buf, size_t count)
 {
@@ -68,10 +76,12 @@ static ssize_t revert_store(struct kobject *kobj,
 }
 
 static struct kobj_attribute kgr_attr_state = __ATTR_RO(state);
+static struct kobj_attribute kgr_attr_refs = __ATTR_RO(refs);
 static struct kobj_attribute kgr_attr_revert = __ATTR_WO(revert);
 
 static struct attribute *kgr_patch_sysfs_entries[] = {
 	&kgr_attr_state.attr,
+	&kgr_attr_refs.attr,
 	&kgr_attr_revert.attr,
 	NULL
 };
-- 
2.1.0

