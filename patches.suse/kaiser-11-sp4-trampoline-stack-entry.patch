From: Jiri Kosina <jkosina@suse.cz>
Subject: [PATCH 11-SP4] x86/kaiser: use trampoline stack for kernel entry
References: bsc#1077560
Patch-mainline: Never, different implementation

Currently we use kernel thread stack for kernel entry, and therefore this
has to be kept mapped in userspace part of CR3.

This is inconvenient; let's introduce switch to entering through a small
dedicated trampoline stack (in TSS), which is only used to switch over to
proper kernel stack once running in ring0, which allows the kernel thread
stack to be kept unmapped.

Nested NMIs are a bit tricky, as we have to make sure that we are interrupting
any context safely. Namely, there are situations where (nested/kernel) NMI would
be returning to kernel CS with *user* CR3 (in case we interrupted sysret path
that already did the CR3 switch, but before the actual sysret happened).
This is acomplished by carrying over the interrupted CR3 value in safe register
storage (%r14) over the time of (nested) NMI handling, and restore it to
whatever the previous version was.

Signed-off-by: Jiri Kosina <jkosina@suse.cz>
---

 arch/x86/ia32/ia32entry.S        |   36 +++++-
 arch/x86/include/asm/kaiser.h    |   31 +++++
 arch/x86/kernel/asm-offsets_64.c |    3 
 arch/x86/kernel/cpu/common.c     |    6 +
 arch/x86/kernel/entry_64.S       |  221 +++++++++++++++++++--------------------
 arch/x86/kernel/process_64.c     |    4 
 include/linux/stddef.h           |    9 +
 kernel/fork.c                    |    6 -
 8 files changed, 196 insertions(+), 120 deletions(-)

--- a/arch/x86/ia32/ia32entry.S
+++ b/arch/x86/ia32/ia32entry.S
@@ -194,7 +194,7 @@ sysexit_from_sys_call:
 
 	DISABLE_IBRS
 
-	SWITCH_USER_CR3
+	SWITCH_USER_CR3_NO_STACK
 	ENABLE_INTERRUPTS_SYSEXIT32
 
 #ifdef CONFIG_AUDITSYSCALL
@@ -356,7 +356,7 @@ sysretl_from_sys_call:
 
 	DISABLE_IBRS
 
-	SWITCH_USER_CR3
+	SWITCH_USER_CR3_NO_STACK
 	movl RSP-ARGOFFSET(%rsp),%esp
 	CFI_RESTORE rsp
 	USERGS_SYSRET32
@@ -430,6 +430,37 @@ ENTRY(ia32_syscall)
 	PARAVIRT_ADJUST_EXCEPTION_FRAME
 	SWAPGS
 	SWITCH_KERNEL_CR3_NO_STACK
+
+
+	/*
+	 * Check whether we are on a trampoline stack; if we are,
+	 * copy the frame to proper stack and switch to it
+	 *
+	cld
+	movq %rax, %r8
+	movq %rcx, %r9
+	movq PER_CPU_VAR(init_tss + TSS_sp0), %rcx
+	cmpq %rcx, %rsp
+	ja 1f
+	leaq -TSS_stack_size(%rcx), %rax
+	cmpq %rsp, %rax
+	ja 1f
+	pushq %rdi
+	pushq %rsi
+	subq %rsp, %rcx
+	movq PER_CPU_VAR(kernel_stack), %rdi
+	addq $KERNEL_STACK_OFFSET, %rdi
+	subq %rcx, %rdi
+	movq %rdi, %rax
+	movq %rsp, %rsi
+	rep movsb
+	movq %rax, %rsp
+	popq %rsi
+	popq %rdi
+	1:
+	movq %r8, %rax
+	movq %r9, %rcx
+
 	/*
 	 * No need to follow this irqs on/off section: the syscall
 	 * disabled irqs and here we enable it straight after entry:
@@ -437,7 +468,6 @@ ENTRY(ia32_syscall)
 	ENABLE_INTERRUPTS(CLBR_NONE)
 	movl %eax,%eax
 	pushq_cfi %rax
-	cld
 	/* note the registers are not zero extended to the sf.
 	   this could be a problem. */
 	SAVE_ARGS 0,0,1
--- a/arch/x86/include/asm/kaiser.h
+++ b/arch/x86/include/asm/kaiser.h
@@ -63,6 +63,14 @@ popq %rax
 .Lend_\@:
 .endm
 
+.macro SWITCH_USER_CR3_NO_STACK
+ALTERNATIVE "jmp .Lend_\@", "", X86_FEATURE_KAISER
+movq %rax, PER_CPU_VAR(unsafe_stack_register_backup)
+_SWITCH_TO_USER_CR3 %rax %al
+movq PER_CPU_VAR(unsafe_stack_register_backup), %rax
+.Lend_\@:
+.endm
+
 .macro SWITCH_KERNEL_CR3_NO_STACK
 ALTERNATIVE "jmp .Lend_\@", "", X86_FEATURE_KAISER
 movq %rax, PER_CPU_VAR(unsafe_stack_register_backup)
@@ -71,6 +79,25 @@ movq PER_CPU_VAR(unsafe_stack_register_b
 .Lend_\@:
 .endm
 
+/* Needed for preserving CR3 state throughout nested NMI */
+.macro SAVE_AND_SWITCH_TO_KERNEL_CR3 scratch_reg:req save_reg:req
+movq	%cr3, \save_reg
+ALTERNATIVE "jmp .Lend_\@", "", X86_FEATURE_KAISER
+movq	\save_reg, \scratch_reg
+_SWITCH_TO_KERNEL_CR3 \scratch_reg
+.Lend_\@:
+.endm
+
+.macro RESTORE_CR3 scratch_reg:req save_reg:req
+ALTERNATIVE "jmp .Lend_\@", "", X86_FEATURE_KAISER
+movq	%cr3, \scratch_reg
+testq \save_reg, \scratch_reg
+je .Lend_\@
+ALTERNATIVE ASM_NOP5, "bts $63, \save_reg", X86_FEATURE_PCID
+movq	\save_reg, %cr3
+.Lend_\@:
+.endm
+
 #else /* CONFIG_KAISER */
 
 .macro SWITCH_KERNEL_CR3
@@ -79,6 +106,10 @@ movq PER_CPU_VAR(unsafe_stack_register_b
 .endm
 .macro SWITCH_KERNEL_CR3_NO_STACK
 .endm
+.macro SAVE_AND_SWITCH_TO_KERNEL_CR3 scratch_reg:req save_reg:req
+.endm
+.macro RESTORE_CR3 save_reg:req
+.endm
 
 #endif /* CONFIG_KAISER */
 
--- a/arch/x86/kernel/asm-offsets_64.c
+++ b/arch/x86/kernel/asm-offsets_64.c
@@ -70,6 +70,9 @@ int main(void)
 #undef ENTRY
 
 	OFFSET(TSS_ist, tss_struct, x86_tss.ist);
+	OFFSET(TSS_sp0, tss_struct, x86_tss.sp0);
+	OFFSET(TSS_stack, tss_struct, stack);
+	DEFINE(TSS_stack_size, sizeof(((struct tss_struct *)0)->stack));
 	BLANK();
 
 	DEFINE(__NR_syscall_max, sizeof(syscalls) - 1);
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -11,6 +11,7 @@
 #include <linux/kgdb.h>
 #include <linux/smp.h>
 #include <linux/io.h>
+#include <linux/stddef.h>
 
 #include <asm/stackprotector.h>
 #include <asm/perf_event.h>
@@ -1334,7 +1335,12 @@ void __cpuinit cpu_init(void)
 	BUG_ON(me->mm);
 	enter_lazy_tlb(&init_mm, me);
 
+#ifdef CONFIG_X86_64
+	percpu_write(init_tss.x86_tss.sp0,
+			(unsigned long) t + offsetofend(struct tss_struct, stack));
+#else
 	load_sp0(t, &current->thread);
+#endif
 	set_tss_desc(cpu, t);
 	load_TR_desc();
 	load_mm_ldt(&init_mm);
--- a/arch/x86/kernel/entry_64.S
+++ b/arch/x86/kernel/entry_64.S
@@ -332,6 +332,27 @@ ENTRY(save_args)
 	je 1f
 	SWAPGS
 	SWITCH_KERNEL_CR3
+
+1:
+	/* Switch to proper stack if we are on trampoline stack */
+	movq PER_CPU_VAR(init_tss + TSS_sp0), %rcx
+	cmpq %rcx, %rsp
+	ja 1f
+	leaq -TSS_stack_size(%rcx), %rax
+	cmpq %rsp, %rax
+	ja 1f
+	pushq %rdi
+	subq %rsp, %rcx
+	movq PER_CPU_VAR(kernel_stack), %rdi
+	addq $KERNEL_STACK_OFFSET, %rdi
+	subq %rcx, %rdi
+	movq %rdi, %rax
+	movq %rsp, %rsi
+	rep movsb
+	movq %rax, %rsp
+	popq %rdi
+	leaq 8(%rsp), %rbp      /* Update rbp to the new value */
+
 	ENABLE_IBRS
 	/*
 	 * irq_count is used to check if a CPU is already on an interrupt stack
@@ -371,30 +392,24 @@ END(save_rest)
 
 /* save complete stack frame */
 	.pushsection .kprobes.text, "ax"
-/*
- * Return: ebx=0: needs swapgs but not SWITCH_USER_CR3 in paranoid_exit
- *         ebx=1: needs neither swapgs nor SWITCH_USER_CR3 in paranoid_exit
- *         ebx=2: needs both swapgs and SWITCH_USER_CR3 in paranoid_exit
- *         ebx=3: needs SWITCH_USER_CR3 but not swapgs in paranoid_exit
- */
 ENTRY(save_paranoid)
-	XCPT_FRAME offset=ORIG_RAX-R15+8
+	XCPT_FRAME 1 RDI+8
 	cld
-	movq %rdi, RDI+8(%rsp)
-	movq %rsi, RSI+8(%rsp)
+	movq_cfi rdi, RDI+8
+	movq_cfi rsi, RSI+8
 	movq_cfi rdx, RDX+8
 	movq_cfi rcx, RCX+8
 	movq_cfi rax, RAX+8
-	movq %r8, R8+8(%rsp)
-	movq %r9, R9+8(%rsp)
-	movq %r10, R10+8(%rsp)
-	movq %r11, R11+8(%rsp)
+	movq_cfi r8, R8+8
+	movq_cfi r9, R9+8
+	movq_cfi r10, R10+8
+	movq_cfi r11, R11+8
 	movq_cfi rbx, RBX+8
-	movq %rbp, RBP+8(%rsp)
-	movq %r12, R12+8(%rsp)
-	movq %r13, R13+8(%rsp)
-	movq %r14, R14+8(%rsp)
-	movq %r15, R15+8(%rsp)
+	movq_cfi rbp, RBP+8
+	movq_cfi r12, R12+8
+	movq_cfi r13, R13+8
+	movq_cfi r14, R14+8
+	movq_cfi r15, R15+8
 	movl $1,%ebx
 	movl $MSR_GS_BASE,%ecx
 	rdmsr
@@ -403,26 +418,7 @@ ENTRY(save_paranoid)
 	SWAPGS
 	xorl %ebx,%ebx
 1:
-#ifdef CONFIG_KAISER
-	/*
-	 * We might have come in between a swapgs and a SWITCH_KERNEL_CR3
-	 * on entry, or between a SWITCH_USER_CR3 and a swapgs on exit.
-	 * Do a conditional SWITCH_KERNEL_CR3: this could safely be done
-	 * unconditionally, but we need to find out whether the reverse
-	 * should be done on return (conveyed to paranoid_exit in %ebx).
-	 */
-	ALTERNATIVE "jmp 2f", "", X86_FEATURE_KAISER
-	movq %cr3, %rax
-	testl	$KAISER_SHADOW_PGD_OFFSET, %eax
-	jz	2f
-	orl	$2, %ebx
-	andq	$(~(X86_CR3_PCID_ASID_MASK | KAISER_SHADOW_PGD_OFFSET)), %rax
-	/* If PCID enabled, set X86_CR3_PCID_NOFLUSH_BIT */
-	ALTERNATIVE ASM_NOP5, "bts $63, %rax", X86_FEATURE_PCID
-	movq	%rax, %cr3
-2:
-#endif
-
+	SAVE_AND_SWITCH_TO_KERNEL_CR3 scratch_reg=%rax save_reg=%r14
 	ENABLE_IBRS
 	ret
 	CFI_ENDPROC
@@ -565,8 +561,8 @@ sysret_check:
 	 * switch CR3 in NMIs.  Normal interrupts are OK because
 	 * they are off here.
 	 */
-	SWITCH_USER_CR3
 	movq	PER_CPU_VAR(old_rsp), %rsp
+	SWITCH_USER_CR3_NO_STACK
 	USERGS_SYSRET64
 
 	CFI_RESTORE_STATE
@@ -913,11 +909,31 @@ retint_swapgs:		/* return to user-space
 	 * they are off here.
 	 */
 
+swapgs_restore_regs_and_return_to_usermode:
 	DISABLE_IBRS
-
-	SWITCH_USER_CR3
+	/*
+	 * Returning from an interrupt/NMI/exception to user space, currently
+	 * on the kernel task stack, which is not user-mapped.  Copy the iret
+	 * frame to the trampoline stack and switch to it before returning.
+	 */
+	RESTORE_ARGS 1,8,1
+	pushq %rax
+	movq %rsp, %rax
+	movq PER_CPU_VAR(init_tss + TSS_sp0), %rsp
+	pushq SS-ORIG_RAX(%rax)
+	pushq RSP-ORIG_RAX(%rax)
+	pushq EFLAGS-ORIG_RAX(%rax)
+	pushq CS-ORIG_RAX(%rax)
+	pushq RIP-ORIG_RAX(%rax)
+	movq (%rax), %rax
+	SWITCH_USER_CR3_NO_STACK
 	SWAPGS
-	jmp restore_args
+	jmp irq_return
+
+paranoid_userspace_restore_all:
+	TRACE_IRQS_IRETQ
+	RESTORE_REST
+	jmp swapgs_restore_regs_and_return_to_usermode
 
 retint_restore_args:	/* return to kernel space */
 	DISABLE_INTERRUPTS(CLBR_ANY)
@@ -925,7 +941,6 @@ retint_restore_args:	/* return to kernel
 	 * The iretq could re-enable interrupts:
 	 */
 	TRACE_IRQS_IRETQ
-restore_args:
 	RESTORE_ARGS 1,8,1
 
 irq_return:
@@ -1513,44 +1528,30 @@ paranoidzeroentry machine_check *machine
 	 * is fundamentally NMI-unsafe. (we cannot change the soft and
 	 * hard flags at once, atomically)
 	 */
-/*
- * On entry: ebx=0: needs swapgs but not SWITCH_USER_CR3
- *           ebx=1: needs neither swapgs nor SWITCH_USER_CR3
- *           ebx=2: needs both swapgs and SWITCH_USER_CR3
- *           ebx=3: needs SWITCH_USER_CR3 but not swapgs
- */
 ENTRY(paranoid_exit)
 	DEFAULT_FRAME
 	DISABLE_INTERRUPTS(CLBR_NONE)
 	TRACE_IRQS_OFF
-	movq	%rbx, %r12		/* paranoid_userspace uses %ebx */
-	testl	$3, CS(%rsp)
-	jnz	paranoid_userspace
-paranoid_kernel:
-	movq	%r12, %rbx		/* restore after paranoid_userspace */
-	TRACE_IRQS_IRETQ 0
-
-	DISABLE_IBRS
-
-#ifdef CONFIG_KAISER
-	/* No ALTERNATIVE for X86_FEATURE_KAISER: save_paranoid sets %ebx */
-	testl	$2, %ebx		/* SWITCH_USER_CR3 needed? */
-	jz	paranoid_exit_no_switch
-	SWITCH_USER_CR3
-paranoid_exit_no_switch:
-#endif
-	testl	$1, %ebx		/* swapgs needed? */
-	jnz	paranoid_exit_no_swapgs
-	SWAPGS_UNSAFE_STACK
-paranoid_exit_no_swapgs:
+	testl %ebx,%ebx				/* swapgs needed? */
+	jnz paranoid_noswapgs_restore
+	testl $3,CS(%rsp)
+	jnz   paranoid_userspace
+paranoid_swapgs_restore:
+	/* return to kernel with swapgs */
+	RESTORE_CR3 scratch_reg=%rax save_reg=%r14
 	RESTORE_ALL 8
-	jmp	irq_return
-
+	SWAPGS
+	jmp irq_return
+paranoid_noswapgs_restore:
+	/* return to kernel */
+	RESTORE_CR3 scratch_reg=%rax save_reg=%r14
+	RESTORE_ALL 8
+	jmp irq_return
 paranoid_userspace:
 	GET_THREAD_INFO(%rcx)
 	movl TI_flags(%rcx),%ebx
 	andl $_TIF_WORK_MASK,%ebx
-	jz paranoid_kernel
+	jz paranoid_userspace_restore_all
 	movq %rsp,%rdi			/* &pt_regs */
 	call sync_regs
 	movq %rax,%rsp			/* switch stack for scheduling */
@@ -1604,16 +1605,32 @@ ENTRY(error_entry)
 	 * calling TRACE_IRQS_*.  Just unconditionally switch to
 	 * the kernel CR3 here.
 	 */
-	SWITCH_KERNEL_CR3
-
-	ENABLE_IBRS
-
 	xorl %ebx,%ebx
 	testl $3,CS+8(%rsp)
 	je error_kernelspace
 error_swapgs:
 	SWAPGS
+	SWITCH_KERNEL_CR3
+	movq %rsp, %rsi
+	movq PER_CPU_VAR(kernel_stack), %rsp
+	ENABLE_IBRS
+	movq %rsi, %rsp
 error_sti:
+	movq PER_CPU_VAR(init_tss + TSS_sp0), %rcx
+	cmpq %rcx, %rsp
+	ja 1f
+	leaq -TSS_stack_size(%rcx), %rax
+	cmpq %rsp, %rax
+	ja 1f
+	subq %rsp, %rcx
+	movq PER_CPU_VAR(kernel_stack), %rdi
+	addq $KERNEL_STACK_OFFSET, %rdi
+	subq %rcx, %rdi
+	movq %rdi, %rax
+	movq %rsp, %rsi
+	rep movsb
+	movq %rax, %rsp
+1:
 	TRACE_IRQS_OFF
 	ret
 
@@ -1801,26 +1818,8 @@ ENTRY(nmi)
 	 * work, because we don't want to enable interrupts.  Fortunately,
 	 * do_nmi doesn't modify pt_regs.
 	 */
-	SWITCH_USER_CR3
-	SWAPGS
-
-	/*
-	 * Open-code the entire return process for compatibility with varying
-	 * register layouts across different kernel versions.
-	 */
-
-	addq	$6*8, %rsp	/* skip bx, bp, and r12-r15 */
-	popq	%r11		/* pt_regs->r11 */
-	popq	%r10		/* pt_regs->r10 */
-	popq	%r9		/* pt_regs->r9 */
-	popq	%r8		/* pt_regs->r8 */
-	popq	%rax		/* pt_regs->ax */
-	popq	%rcx		/* pt_regs->cx */
-	popq	%rdx		/* pt_regs->dx */
-	popq	%rsi		/* pt_regs->si */
-	popq	%rdi		/* pt_regs->di */
-	addq	$8, %rsp	/* skip orig_ax */
-	INTERRUPT_RETURN
+        addq    $6*8, %rsp      /* skip bx, bp, and r12-r15 */
+        jmp     swapgs_restore_regs_and_return_to_usermode
 
 .Lnmi_from_kernel:
 	/*
@@ -1954,29 +1953,33 @@ restart_nmi:
 	movq $-1,%rsi
 	call do_nmi
 
+	DISABLE_IBRS
+	RESTORE_CR3 scratch_reg=%rax save_reg=%r14
+
 	/* Did the NMI take a page fault? Restore cr2 if it did */
 	movq %cr2, %rcx
 	cmpq %rcx, %r12
 	je 1f
 	movq %r12, %cr2
 1:
-
-#ifdef CONFIG_KAISER
-	/* No ALTERNATIVE for X86_FEATURE_KAISER: save_paranoid sets %ebx */
-	testl	$2, %ebx		/* SWITCH_USER_CR3 needed? */
-	jz	nmi_swapgs
-	SWITCH_USER_CR3
-nmi_swapgs:
-#endif
-	testl $1,%ebx				/* swapgs needed? */
+	testl %ebx,%ebx                         /* swapgs needed? */
 	jnz nmi_restore
+nmi_swapgs:
 	SWAPGS_UNSAFE_STACK
 nmi_restore:
-	RESTORE_ALL 8
-	/* Clear the NMI executing stack variable */
-	movq $0, 10*8(%rsp)
-	jmp irq_return
+	/* Skip the additional iret frame */
+	RESTORE_ALL 6*8
+
+	movq    $0, 5*8(%rsp)           /* clear "NMI executing" */
+
+	/*
+	 * INTERRUPT_RETURN reads the "iret" frame and exits the NMI
+	 * stack in a single instruction.  We are returning to kernel
+	 * mode, so this cannot result in a fault.
+	 */
+	INTERRUPT_RETURN
 	CFI_ENDPROC
+
 END(nmi)
 
 	/*
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -381,10 +381,10 @@ __switch_to(struct task_struct *prev_p,
 	fpu_switch_t fpu;
 
 	fpu = switch_fpu_prepare(prev_p, next_p);
-
+#ifdef CONFIG_X86_32
 	/* Reload esp0 and ss1. */
 	load_sp0(tss, next);
-
+#endif
 	/* We must save %fs and %gs before load_TLS() because
 	 * %fs and %gs may be cleared by load_TLS().
 	 *
--- a/include/linux/stddef.h
+++ b/include/linux/stddef.h
@@ -23,6 +23,15 @@ enum {
 #else
 #define offsetof(TYPE, MEMBER) ((size_t) &((TYPE *)0)->MEMBER)
 #endif
+/**
+ * offsetofend(TYPE, MEMBER)
+ *
+ * @TYPE: The type of the structure
+ * @MEMBER: The member within the structure to get the end offset of
+ */
+#define offsetofend(TYPE, MEMBER) \
+	(offsetof(TYPE, MEMBER) + sizeof(((TYPE *)0)->MEMBER))
+
 #endif /* __KERNEL__ */
 
 #endif
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -59,7 +59,6 @@
 #include <linux/tsacct_kern.h>
 #include <linux/cn_proc.h>
 #include <linux/freezer.h>
-#include <linux/kaiser.h>
 #include <linux/delayacct.h>
 #include <linux/taskstats_kern.h>
 #include <linux/random.h>
@@ -138,7 +137,6 @@ static struct thread_info *alloc_thread_
 
 static inline void free_thread_info(struct thread_info *ti)
 {
-	kaiser_unmap_thread_stack(ti);
 	free_pages((unsigned long)ti, THREAD_SIZE_ORDER);
 }
 #endif
@@ -288,10 +286,6 @@ static struct task_struct *dup_task_stru
 	if (err)
 		goto out;
 
-	err = kaiser_map_thread_stack(tsk->stack);
-	if (err)
-		goto out;
-
 	setup_thread_stack(tsk, orig);
 	clear_user_return_notifier(tsk);
 	clear_tsk_need_resched(tsk);
