From: Joerg Roedel <jroedel@suse.de>
Subject: [PATCH] KVM: x86: kABI workaround for PKRU fixes
Patch-mainline: never, kabi
References: bsc#1055935

This patch partially reverts commit

	b9dd21e104bc KVM: x86: simplify handling of PKRU

and brings kABI back into place.

Signed-off-by: Joerg Roedel <jroedel@suse.de>
---
 arch/x86/include/asm/kvm_host.h |  4 +++-
 arch/x86/kvm/kvm_cache_regs.h   | 12 ++++++++++++
 arch/x86/kvm/mmu.h              |  2 +-
 arch/x86/kvm/svm.c              | 25 +++++++++++++++++++++++--
 arch/x86/kvm/vmx.c              | 30 ++++++++++++++++++++++++------
 arch/x86/kvm/x86.c              | 25 ++++++++++++++++---------
 6 files changed, 79 insertions(+), 19 deletions(-)

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 5bd1bd5..5332c08 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -426,7 +426,6 @@ struct kvm_vcpu_arch {
 	unsigned long cr4;
 	unsigned long cr4_guest_owned_bits;
 	unsigned long cr8;
-	u32 pkru;
 	u32 hflags;
 	u64 efer;
 	u64 apic_base;
@@ -982,6 +981,9 @@ struct kvm_arch_async_pf {
 
 extern struct kvm_x86_ops *kvm_x86_ops;
 
+/* kABI hack */
+extern void (*kvm_set_pkru)(struct kvm_vcpu *vcpu, u32 pkru);
+
 int kvm_mmu_module_init(void);
 void kvm_mmu_module_exit(void);
 
diff --git a/arch/x86/kvm/kvm_cache_regs.h b/arch/x86/kvm/kvm_cache_regs.h
index e1e89ee..9f5d128 100644
--- a/arch/x86/kvm/kvm_cache_regs.h
+++ b/arch/x86/kvm/kvm_cache_regs.h
@@ -84,6 +84,18 @@ static inline u64 kvm_read_edx_eax(struct kvm_vcpu *vcpu)
 		| ((u64)(kvm_register_read(vcpu, VCPU_REGS_RDX) & -1u) << 32);
 }
 
+static inline u32 kvm_read_pkru(struct kvm_vcpu *vcpu)
+{
+	return kvm_x86_ops->get_pkru(vcpu);
+}
+
+/* kABI hack */
+static inline void kvm_write_pkru(struct kvm_vcpu *vcpu, u32 pkru)
+{
+	if (kvm_set_pkru)
+		kvm_set_pkru(vcpu, pkru);
+}
+
 static inline void enter_guest_mode(struct kvm_vcpu *vcpu)
 {
 	vcpu->arch.hflags |= HF_GUEST_MASK;
diff --git a/arch/x86/kvm/mmu.h b/arch/x86/kvm/mmu.h
index dbba8ce..ecbce64 100644
--- a/arch/x86/kvm/mmu.h
+++ b/arch/x86/kvm/mmu.h
@@ -182,7 +182,7 @@ static inline bool permission_fault(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu,
 		* index of the protection domain, so pte_pkey * 2 is
 		* is the index of the first bit for the domain.
 		*/
-		pkru_bits = (vcpu->arch.pkru >> (pte_pkey * 2)) & 3;
+		pkru_bits = (kvm_read_pkru(vcpu) >> (pte_pkey * 2)) & 3;
 
 		/* clear present bit, replace PFEC.RSVD with ACC_USER_MASK. */
 		offset = pfec - 1 +
diff --git a/arch/x86/kvm/svm.c b/arch/x86/kvm/svm.c
index 0d843af..aa62c61 100644
--- a/arch/x86/kvm/svm.c
+++ b/arch/x86/kvm/svm.c
@@ -1748,6 +1748,16 @@ static void svm_set_rflags(struct kvm_vcpu *vcpu, unsigned long rflags)
 	to_svm(vcpu)->vmcb->save.rflags = rflags;
 }
 
+static u32 svm_get_pkru(struct kvm_vcpu *vcpu)
+{
+       return 0;
+}
+
+static void svm_set_pkru(struct kvm_vcpu *vcpu, u32 pkru)
+{
+	/* Not supported */
+}
+
 static void svm_cache_reg(struct kvm_vcpu *vcpu, enum kvm_reg reg)
 {
 	switch (reg) {
@@ -5352,6 +5362,8 @@ static struct kvm_x86_ops svm_x86_ops = {
 	.get_rflags = svm_get_rflags,
 	.set_rflags = svm_set_rflags,
 
+	.get_pkru = svm_get_pkru,
+
 	.fpu_activate = svm_fpu_activate,
 	.fpu_deactivate = svm_fpu_deactivate,
 
@@ -5421,8 +5433,17 @@ static struct kvm_x86_ops svm_x86_ops = {
 
 static int __init svm_init(void)
 {
-	return kvm_init(&svm_x86_ops, sizeof(struct vcpu_svm),
-			__alignof__(struct vcpu_svm), THIS_MODULE);
+	int r;
+
+	kvm_set_pkru = svm_set_pkru;
+
+	r = kvm_init(&svm_x86_ops, sizeof(struct vcpu_svm),
+		     __alignof__(struct vcpu_svm), THIS_MODULE);
+
+	if (r)
+		kvm_set_pkru = NULL;
+
+	return r;
 }
 
 static void __exit svm_exit(void)
diff --git a/arch/x86/kvm/vmx.c b/arch/x86/kvm/vmx.c
index c5a2d2a..a7e5e07 100644
--- a/arch/x86/kvm/vmx.c
+++ b/arch/x86/kvm/vmx.c
@@ -601,6 +601,7 @@ struct vcpu_vmx {
 
 	u64 current_tsc_ratio;
 
+	u32 guest_pkru;
 	u32 host_pkru;
 
 	/*
@@ -2224,6 +2225,16 @@ static void vmx_set_rflags(struct kvm_vcpu *vcpu, unsigned long rflags)
 	vmcs_writel(GUEST_RFLAGS, rflags);
 }
 
+static u32 vmx_get_pkru(struct kvm_vcpu *vcpu)
+{
+	return to_vmx(vcpu)->guest_pkru;
+}
+
+static void vmx_set_pkru(struct kvm_vcpu *vcpu, u32 pkru)
+{
+	to_vmx(vcpu)->guest_pkru = pkru;
+}
+
 static u32 vmx_get_interrupt_shadow(struct kvm_vcpu *vcpu)
 {
 	u32 interruptibility = vmcs_read32(GUEST_INTERRUPTIBILITY_INFO);
@@ -8614,8 +8625,8 @@ static void __noclone vmx_vcpu_run(struct kvm_vcpu *vcpu)
 
 	if (static_cpu_has(X86_FEATURE_PKU) &&
 	    kvm_read_cr4_bits(vcpu, X86_CR4_PKE) &&
-	    vcpu->arch.pkru != vmx->host_pkru)
-		__write_pkru(vcpu->arch.pkru);
+	    vmx->guest_pkru != vmx->host_pkru)
+		__write_pkru(vmx->guest_pkru);
 
 	atomic_switch_perf_msrs(vmx);
 	debugctlmsr = get_debugctlmsr();
@@ -8763,8 +8774,8 @@ static void __noclone vmx_vcpu_run(struct kvm_vcpu *vcpu)
 	 */
 	if (static_cpu_has(X86_FEATURE_PKU) &&
 	    kvm_read_cr4_bits(vcpu, X86_CR4_PKE)) {
-		vcpu->arch.pkru = __read_pkru();
-		if (vcpu->arch.pkru != vmx->host_pkru)
+		vmx->guest_pkru = __read_pkru();
+		if (vmx->guest_pkru != vmx->host_pkru)
 			__write_pkru(vmx->host_pkru);
 	}
 
@@ -10903,6 +10914,8 @@ static struct kvm_x86_ops vmx_x86_ops = {
 	.get_rflags = vmx_get_rflags,
 	.set_rflags = vmx_set_rflags,
 
+	.get_pkru = vmx_get_pkru,
+
 	.fpu_activate = vmx_fpu_activate,
 	.fpu_deactivate = vmx_fpu_deactivate,
 
@@ -10985,10 +10998,15 @@ static struct kvm_x86_ops vmx_x86_ops = {
 
 static int __init vmx_init(void)
 {
-	int r = kvm_init(&vmx_x86_ops, sizeof(struct vcpu_vmx),
+	int r;
+
+	kvm_set_pkru = vmx_set_pkru;
+	r = kvm_init(&vmx_x86_ops, sizeof(struct vcpu_vmx),
                      __alignof__(struct vcpu_vmx), THIS_MODULE);
-	if (r)
+	if (r) {
+		kvm_set_pkru = NULL;
 		return r;
+	}
 
 #ifdef CONFIG_KEXEC_CORE
 	rcu_assign_pointer(crash_vmclear_loaded_vmcss,
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index 3a86958..4f80680 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -101,6 +101,10 @@ static void __kvm_set_rflags(struct kvm_vcpu *vcpu, unsigned long rflags);
 struct kvm_x86_ops *kvm_x86_ops __read_mostly;
 EXPORT_SYMBOL_GPL(kvm_x86_ops);
 
+/* kABI hack */
+void (*kvm_set_pkru)(struct kvm_vcpu *vcpu, u32 pkru);
+EXPORT_SYMBOL_GPL(kvm_set_pkru);
+
 static bool __read_mostly ignore_msrs = 0;
 module_param(ignore_msrs, bool, S_IRUGO | S_IWUSR);
 
@@ -3102,12 +3106,12 @@ static void fill_xsave(u8 *dest, struct kvm_vcpu *vcpu)
 			u32 size, offset, ecx, edx;
 			cpuid_count(XSTATE_CPUID, index,
 				    &size, &offset, &ecx, &edx);
-			if (feature == XFEATURE_MASK_PKRU)
-				memcpy(dest + offset, &vcpu->arch.pkru,
-				       sizeof(vcpu->arch.pkru));
-			else
+			if (feature == XFEATURE_MASK_PKRU) {
+				u32 pkru = kvm_read_pkru(vcpu);
+				memcpy(dest + offset, &pkru, sizeof(pkru));
+			} else {
 				memcpy(dest + offset, src, size);
-
+			}
 		}
 
 		valid -= feature;
@@ -3145,11 +3149,14 @@ static void load_xsave(struct kvm_vcpu *vcpu, u8 *src)
 			u32 size, offset, ecx, edx;
 			cpuid_count(XSTATE_CPUID, index,
 				    &size, &offset, &ecx, &edx);
-			if (feature == XFEATURE_MASK_PKRU)
-				memcpy(&vcpu->arch.pkru, src + offset,
-				       sizeof(vcpu->arch.pkru));
-			else
+			if (feature == XFEATURE_MASK_PKRU) {
+				u32 pkru;
+
+				memcpy(&pkru, src + offset, sizeof(pkru));
+				kvm_write_pkru(vcpu, pkru);
+			} else {
 				memcpy(dest, src + offset, size);
+			}
 		}
 
 		valid -= feature;
-- 
1.8.5.6

