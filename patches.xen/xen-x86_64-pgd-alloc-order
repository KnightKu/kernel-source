From: jbeulich@novell.com
Subject: don't require order-1 allocations for pgd-s
Patch-mainline: Never, SUSE-Xen specific
References: none

At the same time remove the useless user mode pair of init_top_pgt.

--- a/arch/x86/include/mach-xen/asm/hypervisor.h
+++ b/arch/x86/include/mach-xen/asm/hypervisor.h
@@ -98,8 +98,8 @@ void do_hypervisor_callback(struct pt_re
  * be MACHINE addresses.
  */
 
-void xen_pt_switch(unsigned long ptr);
-void xen_new_user_pt(unsigned long ptr); /* x86_64 only */
+void xen_pt_switch(pgd_t *);
+void xen_new_user_pt(pgd_t *); /* x86_64 only */
 void xen_load_gs(unsigned int selector); /* x86_64 only */
 void xen_tlb_flush(void);
 void xen_invlpg(unsigned long ptr);
@@ -107,8 +107,8 @@ void xen_invlpg(unsigned long ptr);
 void xen_l1_entry_update(pte_t *ptr, pte_t val);
 void xen_l2_entry_update(pmd_t *ptr, pmd_t val);
 void xen_l3_entry_update(pud_t *ptr, pud_t val); /* x86_64/PAE */
-void xen_l4_entry_update(p4d_t *ptr, bool user, p4d_t val); /* x86_64 only */
-void xen_l5_entry_update(pgd_t *ptr, bool user, pgd_t val); /* 5-level paging only */
+void xen_l4_entry_update(p4d_t *ptr, p4d_t val); /* x86_64 only */
+void xen_l5_entry_update(pgd_t *ptr, pgd_t val); /* 5-level paging only */
 void xen_pgd_pin(pgd_t *);
 void xen_pgd_unpin(pgd_t *);
 
--- a/arch/x86/include/mach-xen/asm/pgalloc.h
+++ b/arch/x86/include/mach-xen/asm/pgalloc.h
@@ -132,7 +132,7 @@ static inline void p4d_populate(struct m
 
 	paravirt_alloc_pud(mm, __pa(pud) >> PAGE_SHIFT);
 	if (unlikely(PagePinned(virt_to_page(p4d))))
-		xen_l4_entry_update(p4d, true, ent);
+		xen_l4_entry_update(p4d, ent);
 	else {
 		*p4d = ent;
 #if CONFIG_PGTABLE_LEVELS == 4
@@ -168,7 +168,7 @@ static inline void pgd_populate(struct m
 
 	paravirt_alloc_p4d(mm, __pa(p4d) >> PAGE_SHIFT);
 	if (unlikely(PagePinned(virt_to_page(pgd))))
-		xen_l5_entry_update(pgd, true, ent);
+		xen_l5_entry_update(pgd, ent);
 	else
 		*__user_pgd(pgd) = *pgd = ent;
 }
--- a/arch/x86/include/mach-xen/asm/pgtable_64.h
+++ b/arch/x86/include/mach-xen/asm/pgtable_64.h
@@ -140,7 +140,7 @@ static inline pud_t xen_pudp_get_and_cle
 
 static inline void xen_set_p4d(p4d_t *p4dp, p4d_t p4d)
 {
-	xen_l4_entry_update(p4dp, false, p4d);
+	xen_l4_entry_update(p4dp, p4d);
 }
 
 #ifdef CONFIG_X86_5LEVEL
@@ -148,7 +148,7 @@ static inline void xen_set_p4d(p4d_t *p4
 ({							\
 	p4d_t *p4dp_ = (p4d);				\
 	PagePinned(virt_to_page(p4dp_))			\
-	? xen_l4_entry_update(p4dp_, true, xen_make_p4d(0)) \
+	? xen_l4_entry_update(p4dp_, xen_make_p4d(0))	\
 	: (void)(*p4dp_ = xen_make_p4d(0));		\
 })
 #else
@@ -157,23 +157,27 @@ static inline void xen_set_p4d(p4d_t *p4
 	p4d_t *p4dp_ = (p4d);				\
 	p4d_t p4de_ = (p4d_t){ .pgd = xen_make_pgd(0) }; \
 	PagePinned(virt_to_page(p4dp_))			\
-	? xen_l4_entry_update(p4dp_, true, p4de_)	\
+	? xen_l4_entry_update(p4dp_, p4de_)		\
 	: (void)(*__user_p4d(p4dp_) = *p4dp_ = p4de_);	\
 })
 #endif
 
-#define __user_pgd(pgd) ((pgd) + PTRS_PER_PGD)
+#define __user_pgd(pgd) \
+	(((unsigned long)(pgd) & PAGE_MASK) != (unsigned long)init_top_pgt \
+	 ? (typeof(pgd))(page_private(virt_to_page(pgd)) \
+			 + ((unsigned long)(pgd) & ~PAGE_MASK)) \
+	 : (typeof(pgd))NULL)
 
 static inline void xen_set_pgd(pgd_t *pgdp, pgd_t pgd)
 {
-	xen_l5_entry_update(pgdp, false, pgd);
+	xen_l5_entry_update(pgdp, pgd);
 }
 
 #define xen_pgd_clear(pgd)					\
 ({								\
 	pgd_t *pgdp_ = (pgd);					\
 	PagePinned(virt_to_page(pgdp_))				\
-	? xen_l5_entry_update(pgdp_, true, xen_make_pgd(0))	\
+	? xen_l5_entry_update(pgdp_, xen_make_pgd(0))		\
 	: (void)(*__user_pgd(pgdp_) = *pgdp_ = xen_make_pgd(0)); \
 })
 
--- a/arch/x86/kernel/cpu/common-xen.c
+++ b/arch/x86/kernel/cpu/common-xen.c
@@ -1392,8 +1392,7 @@ DEFINE_PER_CPU_FIRST(union irq_stack_uni
 void xen_switch_pt(void)
 {
 #ifdef CONFIG_XEN
-	xen_pt_switch(__pa_symbol(init_top_pgt));
-	xen_new_user_pt(__pa_symbol(__user_pgd(init_top_pgt)));
+	xen_pt_switch(init_top_pgt);
 #endif
 }
 
--- a/arch/x86/kernel/head_64-xen.S
+++ b/arch/x86/kernel/head_64-xen.S
@@ -63,14 +63,6 @@ GLOBAL(name)
 	__PAGE_ALIGNED_BSS
 NEXT_PAGE(init_top_pgt)
 	.fill	512,8,0
-        /*
-         * We update two pgd entries to make kernel and user pgd consistent
-         * at pgd_populate(). It can be used for kernel modules. So we place 
-         * this page here for those cases to avoid memory corruption.
-         * We also use this page to establish the initial mapping for the
-         * vsyscall area.
-         */
-	.fill	512,8,0
 
 #ifdef CONFIG_X86_5LEVEL
 NEXT_PAGE(level4_kernel_pgt)
--- a/arch/x86/mm/hypervisor.c
+++ b/arch/x86/mm/hypervisor.c
@@ -519,7 +519,7 @@ void xen_l3_entry_update(pud_t *ptr, pud
 #endif
 
 #ifdef CONFIG_X86_64
-void xen_l4_entry_update(p4d_t *ptr, bool user, p4d_t val)
+void xen_l4_entry_update(p4d_t *ptr, p4d_t val)
 {
 	mmu_update_t u[2];
 	struct page *page = NULL;
@@ -533,8 +533,11 @@ void xen_l4_entry_update(p4d_t *ptr, boo
 	u[0].ptr = virt_to_machine(ptr);
 	u[0].val = __p4d_val(val);
 #ifndef CONFIG_X86_5LEVEL
-	if (user) {
-		u[1].ptr = virt_to_machine(__user_p4d(ptr));
+	if (((unsigned long)ptr & ~PAGE_MASK)
+	    <= pgd_index(TASK_SIZE_MAX) * sizeof(*ptr)) {
+		ptr = __user_p4d(ptr);
+		BUG_ON(!ptr);
+		u[1].ptr = virt_to_machine(ptr);
 		u[1].val = __p4d_val(val);
 		do_lN_entry_update(u, 2, page);
 	} else
@@ -544,7 +547,7 @@ void xen_l4_entry_update(p4d_t *ptr, boo
 #endif /* CONFIG_X86_64 */
 
 #ifdef CONFIG_X86_5LEVEL
-void xen_l5_entry_update(pgd_t *ptr, bool user, pgd_t val)
+void xen_l5_entry_update(pgd_t *ptr, pgd_t val)
 {
 	mmu_update_t u[2];
 	struct page *page = NULL;
@@ -557,8 +560,11 @@ void xen_l5_entry_update(pgd_t *ptr, boo
 	}
 	u[0].ptr = virt_to_machine(ptr);
 	u[0].val = __pgd_val(val);
-	if (user) {
-		u[1].ptr = virt_to_machine(__user_pgd(ptr));
+	if (((unsigned long)ptr & ~PAGE_MASK)
+	    <= pgd_index(TASK_SIZE_MAX) * sizeof(*ptr)) {
+		ptr = __user_pgd(ptr);
+		BUG_ON(!ptr);
+		u[1].ptr = virt_to_machine(ptr);
 		u[1].val = __pgd_val(val);
 		do_lN_entry_update(u, 2, page);
 	} else
@@ -566,21 +572,25 @@ void xen_l5_entry_update(pgd_t *ptr, boo
 }
 #endif /* CONFIG_X86_5LEVEL */
 
-void xen_pt_switch(unsigned long ptr)
+#ifdef CONFIG_X86_64
+void xen_pt_switch(pgd_t *pgd)
 {
 	struct mmuext_op op;
 	op.cmd = MMUEXT_NEW_BASEPTR;
-	op.arg1.mfn = pfn_to_mfn(ptr >> PAGE_SHIFT);
+	op.arg1.mfn = virt_to_mfn(pgd);
 	BUG_ON(HYPERVISOR_mmuext_op(&op, 1, NULL, DOMID_SELF) < 0);
 }
 
-void xen_new_user_pt(unsigned long ptr)
+void xen_new_user_pt(pgd_t *pgd)
 {
 	struct mmuext_op op;
+
+	pgd = __user_pgd(pgd);
 	op.cmd = MMUEXT_NEW_USER_BASEPTR;
-	op.arg1.mfn = pfn_to_mfn(ptr >> PAGE_SHIFT);
+	op.arg1.mfn = pgd ? virt_to_mfn(pgd) : 0;
 	BUG_ON(HYPERVISOR_mmuext_op(&op, 1, NULL, DOMID_SELF) < 0);
 }
+#endif
 
 void xen_tlb_flush(void)
 {
@@ -652,6 +662,7 @@ EXPORT_SYMBOL_GPL(xen_invlpg_mask);
 void xen_pgd_pin(pgd_t *pgd)
 {
 	struct mmuext_op op[NR_PGD_PIN_OPS];
+	unsigned int nr = NR_PGD_PIN_OPS;
 
 	op[0].cmd = MMUEXT_PIN_L3_TABLE;
 	op[0].arg1.mfn = virt_to_mfn(pgd);
@@ -661,9 +672,26 @@ void xen_pgd_pin(pgd_t *pgd)
 # else
 	op[1].cmd = op[0].cmd = MMUEXT_PIN_L4_TABLE;
 # endif
-	op[1].arg1.mfn = virt_to_mfn(__user_pgd(pgd));
+	pgd = __user_pgd(pgd);
+	if (pgd)
+		op[1].arg1.mfn = virt_to_mfn(pgd);
+	else {
+# ifdef CONFIG_X86_VSYSCALL_EMULATION
+#  ifdef CONFIG_X86_5LEVEL
+		op[1].cmd = MMUEXT_PIN_L4_TABLE;
+		op[1].arg1.mfn = pfn_to_mfn(__pa_symbol(level4_user_pgt)
+					    >> PAGE_SHIFT);
+#  else
+		op[1].cmd = MMUEXT_PIN_L3_TABLE;
+		op[1].arg1.mfn = pfn_to_mfn(__pa_symbol(level3_user_pgt)
+					    >> PAGE_SHIFT);
+#  endif
+# else
+		nr = 1;
+# endif
+	}
 #endif
-	if (HYPERVISOR_mmuext_op(op, NR_PGD_PIN_OPS, NULL, DOMID_SELF) < 0)
+	if (HYPERVISOR_mmuext_op(op, nr, NULL, DOMID_SELF) < 0)
 		BUG();
 }
 
@@ -674,8 +702,10 @@ void xen_pgd_unpin(pgd_t *pgd)
 	op[0].cmd = MMUEXT_UNPIN_TABLE;
 	op[0].arg1.mfn = virt_to_mfn(pgd);
 #ifdef CONFIG_X86_64
+	pgd = __user_pgd(pgd);
+	BUG_ON(!pgd);
 	op[1].cmd = MMUEXT_UNPIN_TABLE;
-	op[1].arg1.mfn = virt_to_mfn(__user_pgd(pgd));
+	op[1].arg1.mfn = virt_to_mfn(pgd);
 #endif
 	if (HYPERVISOR_mmuext_op(op, NR_PGD_PIN_OPS, NULL, DOMID_SELF) < 0)
 		BUG();
--- a/arch/x86/mm/init_64-xen.c
+++ b/arch/x86/mm/init_64-xen.c
@@ -298,8 +298,7 @@ static p4d_t *fill_p4d(pgd_t *pgd, unsig
 		p4d_t *p4d = (p4d_t *)spp_getpage();
 		if (!after_bootmem) {
 			make_page_readonly(p4d, XENFEAT_writable_page_tables);
-			xen_l5_entry_update(pgd, false,
-			                    __pgd(__pa(p4d) | _PAGE_TABLE));
+			xen_l5_entry_update(pgd, __pgd(__pa(p4d) | _PAGE_TABLE));
 		} else
 			pgd_populate(&init_mm, pgd, p4d);
 		if (p4d != p4d_offset(pgd, 0))
@@ -315,8 +314,7 @@ static pud_t *fill_pud(p4d_t *p4d, unsig
 		pud_t *pud = (pud_t *)spp_getpage();
 		if (!after_bootmem) {
 			make_page_readonly(pud, XENFEAT_writable_page_tables);
-			xen_l4_entry_update(p4d, false,
-			                    __p4d(__pa(pud) | _PAGE_TABLE));
+			xen_l4_entry_update(p4d, __p4d(__pa(pud) | _PAGE_TABLE));
 		} else
 			p4d_populate(&init_mm, p4d, pud);
 		if (pud != pud_offset(p4d, 0))
@@ -894,15 +892,6 @@ void __init xen_init_pt(void)
 	       (PTRS_PER_PUD - pud_index(__START_KERNEL_map))
 	       * sizeof(*level3_kernel_pgt));
 
-#ifdef CONFIG_X86_VSYSCALL_EMULATION
-	__user_pgd(init_top_pgt)[pgd_index(VSYSCALL_ADDR)] =
-# ifdef CONFIG_X86_5LEVEL
-		__pgd(__pa_symbol(level4_user_pgt) | _PAGE_TABLE);
-# else
-		__pgd(__pa_symbol(level3_user_pgt) | _PAGE_TABLE);
-# endif
-#endif
-
 	/* Do an early initialization of the fixmap area. */
 	addr = __fix_to_virt(FIX_EARLYCON_MEM_BASE);
 	if (pud_present(level3_kernel_pgt[pud_index(addr)])) {
@@ -977,8 +966,6 @@ void __init xen_init_pt(void)
 #else
 	early_make_page_readonly(init_top_pgt,
 				 XENFEAT_writable_page_tables);
-	early_make_page_readonly(__user_pgd(init_top_pgt),
-				 XENFEAT_writable_page_tables);
 	early_make_page_readonly(level3_kernel_pgt,
 				 XENFEAT_writable_page_tables);
 #ifdef CONFIG_X86_VSYSCALL_EMULATION
@@ -1074,10 +1061,9 @@ kernel_physical_mapping_init(unsigned lo
 		if (!after_bootmem) {
 			make_page_readonly(p4d, XENFEAT_writable_page_tables);
 			if (IS_ENABLED(CONFIG_X86_5LEVEL))
-				xen_l5_entry_update(pgd, false,
-						    __pgd(__pa(p4d) | _PAGE_TABLE));
+				xen_l5_entry_update(pgd, __pgd(__pa(p4d) | _PAGE_TABLE));
 			else
-				xen_l4_entry_update(p4d_offset(pgd, vaddr), false,
+				xen_l4_entry_update(p4d_offset(pgd, vaddr),
 						    __p4d(__pa(p4d) | _PAGE_TABLE));
 		} else {
 			spin_lock(&init_mm.page_table_lock);
--- a/arch/x86/mm/pgtable-xen.c
+++ b/arch/x86/mm/pgtable-xen.c
@@ -352,9 +352,11 @@ static void pgd_walk(pgd_t *pgd_base, pg
 			BUG();
 		seq = 0;
 	}
+	pgd = __user_pgd(pgd_base);
+	BUG_ON(!pgd);
 	MULTI_update_va_mapping(mcl + seq,
-	       (unsigned long)__user_pgd(pgd_base),
-	       pfn_pte(virt_to_phys(__user_pgd(pgd_base))>>PAGE_SHIFT, flags),
+	       (unsigned long)pgd,
+	       pfn_pte(virt_to_phys(pgd)>>PAGE_SHIFT, flags),
 	       0);
 	MULTI_update_va_mapping(mcl + seq + 1,
 	       (unsigned long)pgd_base,
@@ -773,12 +775,6 @@ static void pgd_prepopulate_pmd(struct m
 	}
 }
 
-#ifdef CONFIG_X86_64
-/* We allocate two contiguous pages for kernel and user. */
-#define PGD_ORDER 1
-#else
-#define PGD_ORDER 0
-#endif
 
 /*
  * Xen paravirt assumes pgd table should be in one page. 64 bit kernel also
@@ -846,12 +842,31 @@ static inline void _pgd_free(pgd_t *pgd)
 #else
 static inline pgd_t *_pgd_alloc(void)
 {
-	return (pgd_t *)__get_free_pages(PGALLOC_GFP, PGD_ORDER);
+	pgd_t *pgd = (void *)__get_free_page(PGALLOC_GFP);
+
+#ifdef CONFIG_X86_64
+	if (pgd) {
+		pgd_t *upgd = (void *)__get_free_page(PGALLOC_GFP);
+
+		if (upgd)
+			set_page_private(virt_to_page(pgd),
+					 (unsigned long)upgd);
+		else {
+			free_page((unsigned long)pgd);
+			pgd = NULL;
+		}
+	}
+#endif
+
+	return pgd;
 }
 
 static inline void _pgd_free(pgd_t *pgd)
 {
-	free_pages((unsigned long)pgd, PGD_ORDER);
+#ifdef CONFIG_X86_64
+	free_page(page_private(virt_to_page(pgd)));
+#endif
+	free_page((unsigned long)pgd);
 }
 #endif /* CONFIG_X86_PAE */
 
--- a/arch/x86/mm/tlb-xen.c
+++ b/arch/x86/mm/tlb-xen.c
@@ -28,6 +28,9 @@ void switch_mm_irqs_off(struct mm_struct
 #else
 	struct mmuext_op _op[2 + (sizeof(long) > 4)], *op = _op;
 #endif
+#ifdef CONFIG_X86_64_XEN
+	pgd_t *upgd;
+#endif
 
 	/*
 	 * NB: The scheduler will call us with prev == next when
@@ -109,9 +112,10 @@ void switch_mm_irqs_off(struct mm_struct
 	op++;
 
 #ifdef CONFIG_X86_64_XEN
-	/* xen_new_user_pt(__pa(__user_pgd(next->pgd))) */
+	/* xen_new_user_pt(next->pgd) */
 	op->cmd = MMUEXT_NEW_USER_BASEPTR;
-	op->arg1.mfn = virt_to_mfn(__user_pgd(next->pgd));
+	upgd = __user_pgd(next->pgd);
+	op->arg1.mfn = likely(upgd) ? virt_to_mfn(upgd) : 0;
 	op++;
 #endif
 
--- a/drivers/xen/core/machine_reboot.c
+++ b/drivers/xen/core/machine_reboot.c
@@ -174,8 +174,7 @@ static int take_machine_down(void *_susp
 		 * in fast-suspend mode as that implies a new enough Xen.
 		 */
 		if (!suspend->fast_suspend)
-			xen_new_user_pt(__pa(__user_pgd(
-				current->active_mm->pgd)));
+			xen_new_user_pt(current->active_mm->pgd);
 #endif
 	}
 
