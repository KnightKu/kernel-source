From: Josh Poimboeuf <jpoimboe@redhat.com>
Date: Tue, 11 Jul 2017 10:33:44 -0500
Subject: xen/x86/entry/64: Add unwind hint annotations
Patch-mainline: Never, SUSE-Xen specific
References: bnc#1018348

Add unwind hint annotations to entry_64.S.  This will enable the ORC
unwinder to unwind through any location in the entry code including
syscalls, interrupts, and exceptions.

Signed-off-by: Josh Poimboeuf <jpoimboe@redhat.com>
Cc: Andy Lutomirski <luto@kernel.org>
Cc: Borislav Petkov <bp@alien8.de>
Cc: Brian Gerst <brgerst@gmail.com>
Cc: Denys Vlasenko <dvlasenk@redhat.com>
Cc: H. Peter Anvin <hpa@zytor.com>
Cc: Jiri Slaby <jslaby@suse.cz>
Cc: Linus Torvalds <torvalds@linux-foundation.org>
Cc: Mike Galbraith <efault@gmx.de>
Cc: Peter Zijlstra <peterz@infradead.org>
Cc: Thomas Gleixner <tglx@linutronix.de>
Cc: live-patching@vger.kernel.org
Link: http://lkml.kernel.org/r/b9f6d478aadf68ba57c739dcfac34ec0dc021c4c.1499786555.git.jpoimboe@redhat.com
Signed-off-by: Ingo Molnar <mingo@kernel.org>
Signed-off-by: Jiri Slaby <jslaby@suse.cz>
Automatically created from "patches.suse/0007-x86-entry-64-Add-unwind-hint-annotations.patch" by xen-port-patches.py

--- a/arch/x86/entry/entry_64-xen.S
+++ b/arch/x86/entry/entry_64-xen.S
@@ -39,6 +39,7 @@
 #include <asm/smap.h>
 #include <asm/pgtable_types.h>
 #include <asm/export.h>
+#include <asm/frame.h>
 #include <linux/err.h>
 #include <xen/interface/xen.h>
 #include <xen/interface/features.h>
@@ -163,6 +164,7 @@ NMI_MASK = 0x80000000
  */
 
 ENTRY(entry_SYSCALL_64)
+	UNWIND_HINT_EMPTY
 	/*
 	 * Interrupts are enabled on entry.
 	 */
@@ -180,6 +182,7 @@ ENTRY(entry_SYSCALL_64)
 	pushq	%r10				/* pt_regs->r10 */
 	pushq	%r11				/* pt_regs->r11 */
 	sub	$(6*8), %rsp			/* pt_regs->bp, bx, r12-15 not saved */
+	UNWIND_HINT_REGS extra=0
 
 	/*
 	 * If we need to do entry work or if we guess we'll need to do
@@ -230,6 +233,7 @@ entry_SYSCALL_64_fastpath:
 	TRACE_IRQS_ON		/* user mode is traced as IRQs on */
 	RESTORE_C_REGS_EXCEPT_RCX_R11
 	REMOVE_PT_GPREGS_FROM_STACK 8
+	UNWIND_HINT_EMPTY
 	xor	%ecx, %ecx
 	xor	%r11, %r11
 	HYPERVISOR_IRET VGCF_IN_SYSCALL
@@ -274,6 +278,7 @@ ENTRY(stub_ptregs_64)
 	 * Called from fast path -- pop return address and jump to slow path
 	 */
 	popq	%rax
+	UNWIND_HINT_REGS extra=0
 	jmp	entry_SYSCALL64_slow_path
 
 1:
@@ -282,6 +287,7 @@ END(stub_ptregs_64)
 
 .macro ptregs_stub func
 ENTRY(ptregs_\func)
+	UNWIND_HINT_FUNC
 	leaq	\func(%rip), %rax
 	jmp	stub_ptregs_64
 END(ptregs_\func)
@@ -298,6 +304,7 @@ END(ptregs_\func)
  * %rsi: next task
  */
 ENTRY(__switch_to_asm)
+	UNWIND_HINT_FUNC
 	/*
 	 * Save callee-saved registers
 	 * This must match the order in inactive_task_frame
@@ -337,6 +344,7 @@ END(__switch_to_asm)
  * r12: kernel thread arg
  */
 ENTRY(ret_from_fork)
+	UNWIND_HINT_EMPTY
 	movq	%rax, %rdi
 	call	schedule_tail			/* rdi: 'prev' task parameter */
 
@@ -344,6 +352,7 @@ ENTRY(ret_from_fork)
 	jnz	1f				/* kernel threads are uncommon */
 
 2:
+	UNWIND_HINT_REGS
 	movq	%rsp, %rdi
 	call	syscall_return_slowpath	/* returns with IRQs disabled */
 	jmp	restore_regs_and_iret
@@ -381,9 +390,14 @@ END(ret_from_fork)
  *
  * The invariant is that, if irq_count != -1, then the IRQ stack is in use.
  */
-.macro ENTER_IRQ_STACK old_rsp
+.macro ENTER_IRQ_STACK regs=1 old_rsp
 	DEBUG_ENTRY_ASSERT_IRQS_OFF
 	movq	%rsp, \old_rsp
+
+	.if \regs
+	UNWIND_HINT_REGS base=\old_rsp
+	.endif
+
 	incl	PER_CPU_VAR(irq_count)
 	jnz	.Lirq_stack_push_old_rsp_\@
 
@@ -420,16 +434,24 @@ END(ret_from_fork)
 
 .Lirq_stack_push_old_rsp_\@:
 	pushq	\old_rsp
+
+	.if \regs
+	UNWIND_HINT_REGS indirect=1
+	.endif
 .endm
 
 /*
  * Undoes ENTER_IRQ_STACK.
  */
-.macro LEAVE_IRQ_STACK
+.macro LEAVE_IRQ_STACK regs=1
 	DEBUG_ENTRY_ASSERT_IRQS_OFF
 	/* We need to be off the IRQ stack before decrementing irq_count. */
 	popq	%rsp
 
+	.if \regs
+	UNWIND_HINT_REGS
+	.endif
+
 	/*
 	 * As in ENTER_IRQ_STACK, irq_count == 0, we are still claiming
 	 * the irq stack but we're not on it.
@@ -491,6 +513,8 @@ END(retint_kernel)
 
 .macro idtentry sym do_sym has_error_code:req paranoid=0 shift_ist=-1
 ENTRY(\sym)
+	UNWIND_HINT_IRET_REGS offset=8
+
 	/* Sanity check */
 	.if \shift_ist != -1 && \paranoid == 0
 	.error "using shift_ist requires paranoid=1"
@@ -516,6 +540,7 @@ ENTRY(\sym)
 	.else
 	call	error_entry
 	.endif
+	UNWIND_HINT_REGS
 	/* returned flag: ebx=0: need swapgs on exit, ebx=1: don't need it */
 
 	.if \paranoid
@@ -610,7 +635,9 @@ idtentry \sym \do_sym has_error_code=\ha
 ENTRY(do_hypervisor_callback)   # do_hypervisor_callback(struct *pt_regs)
 # Since we don't modify %rdi, evtchn_do_upall(struct *pt_regs) will
 # see the correct pointer to the pt_regs
+	UNWIND_HINT_FUNC
 	movq	%rdi, %rsp		# we don't return, adjust the stack frame
+	UNWIND_HINT_REGS
 11:
 	ENTER_IRQ_STACK old_rsp=%r10
 	call	evtchn_do_upcall
@@ -699,12 +726,12 @@ idtentry simd_coprocessor_error		do_simd
 ENTRY(do_softirq_own_stack)
 	pushq	%rbp
 	mov	%rsp, %rbp
-	ENTER_IRQ_STACK old_rsp=%r11
+	ENTER_IRQ_STACK regs=0 old_rsp=%r11
 	call	__do_softirq
-	LEAVE_IRQ_STACK
+	LEAVE_IRQ_STACK regs=0
 	leaveq
 	ret
-END(do_softirq_own_stack)
+ENDPROC(do_softirq_own_stack)
 
 idtentry debug			do_debug		has_error_code=0
 idtentry nmi			do_nmi_callback		has_error_code=0
@@ -729,6 +756,7 @@ idtentry machine_check					has_error_cod
  * Return: ebx=0: need swapgs on exit, ebx=1: otherwise
  */
 ENTRY(paranoid_entry)
+	UNWIND_HINT_FUNC
 	cld
 	SAVE_C_REGS 8
 	SAVE_EXTRA_REGS 8
@@ -756,6 +784,7 @@ END(paranoid_entry)
  * On entry, ebx is "no swapgs" flag (1: don't need swapgs, 0: need it)
  */
 ENTRY(paranoid_exit)
+	UNWIND_HINT_REGS
 	DISABLE_INTERRUPTS(CLBR_ANY)
 	TRACE_IRQS_OFF_DEBUG
 	testl	%ebx, %ebx			/* swapgs needed? */
@@ -778,6 +807,7 @@ END(paranoid_exit)
  * Return: EBX=0: came from user mode; EBX=1: otherwise
  */
 ENTRY(error_entry)
+	UNWIND_HINT_FUNC
 	cld
 	SAVE_C_REGS 8
 	SAVE_EXTRA_REGS 8
@@ -861,6 +891,7 @@ END(error_entry)
 
 
 ENTRY(error_exit)
+	UNWIND_HINT_REGS
 	DISABLE_INTERRUPTS(CLBR_ANY)
 	TRACE_IRQS_OFF
 	testb	$3, CS(%rsp)
@@ -896,18 +927,20 @@ END(do_nmi_callback)
 ENTRY(ignore_sysret)
 	popq	%rcx
 	popq	%r11
+	UNWIND_HINT_EMPTY
 	mov	$-ENOSYS, %eax
 	HYPERVISOR_IRET VGCF_i387_valid
 END(ignore_sysret)
 #endif
 
 ENTRY(rewind_stack_do_exit)
+	UNWIND_HINT_FUNC
 	/* Prevent any naive code from trying to unwind to our caller. */
 	xorl	%ebp, %ebp
 
 	movq	PER_CPU_VAR(cpu_current_top_of_stack), %rax
-	leaq	-TOP_OF_KERNEL_STACK_PADDING-PTREGS_SIZE(%rax), %rsp
+	leaq	-PTREGS_SIZE(%rax), %rsp
+	UNWIND_HINT_FUNC sp_offset=PTREGS_SIZE
 
 	call	do_exit
-1:	jmp 1b
 END(rewind_stack_do_exit)
