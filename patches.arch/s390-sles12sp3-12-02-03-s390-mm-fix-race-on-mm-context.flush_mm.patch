From: Gerald Schaefer <gerald.schaefer@de.ibm.com>
Subject: s390/mm: fix race on mm->context.flush_mm
Patch-mainline: v4.14-rc1
Git-commit: 60f07c8ec5fae06c23e9fd7bab67dabce92b3414
References: bnc#1088324, LTC#166470

Description:  s390/mm: fix races in TLB flushing code
Symptom:      Kernel panic (e.g. "Low-address protection") or possible data
              corruption.
Problem:      There are two race conditions in the TLB flushing code,
              related to mm_cpumask reset vs. mm detach, and unserialized
              setting of the mm->context.flush_mm bit.
Solution:     Add a spinlock to serialize __tlb_flush_mm_lazy, and fix
              the ordering of mm_cpumask reset.
Reproduction: TLB flushing bugs are hard to hit, there are no known steps
              to reproduce.

Upstream-Description:

              s390/mm: fix race on mm->context.flush_mm

              The order in __tlb_flush_mm_lazy is to flush TLB first and then clear
              the mm->context.flush_mm bit. This can lead to missed flushes as the
              bit can be set anytime, the order needs to be the other way aronud.

              But this leads to a different race, __tlb_flush_mm_lazy may be called
              on two CPUs concurrently. If mm->context.flush_mm is cleared first then
              another CPU can bypass __tlb_flush_mm_lazy although the first CPU has
              not done the flush yet. In a virtualized environment the time until the
              flush is finally completed can be arbitrarily long.

              Add a spinlock to serialize __tlb_flush_mm_lazy and use the function
              in finish_arch_post_lock_switch as well.

              Cc: <stable@vger.kernel.org>
              Reviewed-by: Heiko Carstens <heiko.carstens@de.ibm.com>
              Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>


Signed-off-by: Gerald Schaefer <gerald.schaefer@de.ibm.com>
Acked-by: Johannes Thumshirn <jthumshirn@suse.de>
---
 arch/s390/include/asm/mmu.h         |    4 ++++
 arch/s390/include/asm/mmu_context.h |    4 ++--
 arch/s390/include/asm/tlbflush.h    |    4 +++-
 3 files changed, 9 insertions(+), 3 deletions(-)

--- a/arch/s390/include/asm/mmu.h
+++ b/arch/s390/include/asm/mmu.h
@@ -20,9 +20,13 @@ typedef struct {
 	unsigned int has_pgste:1;
 	/* The mmu context uses storage keys. */
 	unsigned int use_skey:1;
+#ifndef __GENKSYMS__
+	spinlock_t lock;
+#endif
 } mm_context_t;
 
 #define INIT_MM_CONTEXT(name)						      \
+	.context.lock = __SPIN_LOCK_UNLOCKED(name.context.lock),	      \
 	.context.list_lock    = __SPIN_LOCK_UNLOCKED(name.context.list_lock), \
 	.context.pgtable_list = LIST_HEAD_INIT(name.context.pgtable_list),    \
 	.context.gmap_list = LIST_HEAD_INIT(name.context.gmap_list),
--- a/arch/s390/include/asm/mmu_context.h
+++ b/arch/s390/include/asm/mmu_context.h
@@ -15,6 +15,7 @@
 static inline int init_new_context(struct task_struct *tsk,
 				   struct mm_struct *mm)
 {
+	spin_lock_init(&mm->context.lock);
 	spin_lock_init(&mm->context.list_lock);
 	INIT_LIST_HEAD(&mm->context.pgtable_list);
 	INIT_LIST_HEAD(&mm->context.gmap_list);
@@ -114,8 +115,7 @@ static inline void finish_arch_post_lock
 			cpu_relax();
 
 		cpumask_set_cpu(smp_processor_id(), mm_cpumask(mm));
-		if (mm->context.flush_mm)
-			__tlb_flush_mm(mm);
+		__tlb_flush_mm_lazy(mm);
 		preempt_enable();
 	}
 	set_fs(current->thread.mm_segment);
--- a/arch/s390/include/asm/tlbflush.h
+++ b/arch/s390/include/asm/tlbflush.h
@@ -97,10 +97,12 @@ static inline void __tlb_flush_mm(struct
 
 static inline void __tlb_flush_mm_lazy(struct mm_struct * mm)
 {
+	spin_lock(&mm->context.lock);
 	if (mm->context.flush_mm) {
-		__tlb_flush_mm(mm);
 		mm->context.flush_mm = 0;
+		__tlb_flush_mm(mm);
 	}
+	spin_unlock(&mm->context.lock);
 }
 
 /*
