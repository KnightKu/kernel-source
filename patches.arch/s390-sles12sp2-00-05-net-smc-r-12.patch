From: Ursula Braun <ubraun@linux.vnet.ibm.com>
Subject: smc: send data (through RDMA)
Patch-mainline: not yet, IBM pushing upstream
References: bsc#978258,FATE#319593,LTC#131290

Summary:     net/smc: Shared Memory Communications - RDMA
Description: Initial part of the implementation of the "Shared Memory
             Communications-RDMA" (SMC-R) protocol. The protocol is defined
             in RFC7609 [1]. It allows transparent transformation of TCP
             connections using the "Remote Direct Memory Access over
             Converged Ethernet" (RoCE) feature of certain communication
             hardware for data center environments. Tested on s390 and x86
             using Mellanox ConnectX-3 cards.

             A new socket protocol family PF_SMC is being introduced. A
             preload shared library will be offered to enable TCP-based
             applications to use SMC-R without changes or recompilation.

             References:
             [1] SMC-R Informational RFC:
             https://tools.ietf.org/rfc/rfc7609

Upstream-Description:

              smc: send data (through RDMA)

              copy data to kernel send buffer, and trigger RDMA write

              Signed-off-by: Ursula Braun <ubraun@linux.vnet.ibm.com>

Signed-off-by: Ursula Braun <ubraun@linux.vnet.ibm.com>
Acked-by: John Jolly <jjolly@suse.de>
---
 net/smc/Makefile  |    2 
 net/smc/af_smc.c  |    5 
 net/smc/smc.h     |    1 
 net/smc/smc_cdc.c |    7 
 net/smc/smc_tx.c  |  449 ++++++++++++++++++++++++++++++++++++++++++++++++++++++
 net/smc/smc_tx.h  |   24 ++
 6 files changed, 484 insertions(+), 4 deletions(-)

--- a/net/smc/Makefile
+++ b/net/smc/Makefile
@@ -1,2 +1,2 @@
 obj-$(CONFIG_SMC)	+= smc.o
-smc-y := af_smc.o smc_pnet.o smc_ib.o smc_clc.o smc_core.o smc_wr.o smc_llc.o smc_cdc.o
+smc-y := af_smc.o smc_pnet.o smc_ib.o smc_clc.o smc_core.o smc_wr.o smc_llc.o smc_cdc.o smc_tx.o
--- a/net/smc/af_smc.c
+++ b/net/smc/af_smc.c
@@ -35,6 +35,7 @@
 #include "smc_core.h"
 #include "smc_ib.h"
 #include "smc_pnet.h"
+#include "smc_tx.h"
 
 static DEFINE_MUTEX(smc_create_lgr_pending);	/* serialize link group
 						 * creation
@@ -433,6 +434,7 @@ static int smc_connect_rdma(struct smc_s
 out_connected:
 	smc_copy_sock_settings_to_clc(smc);
 	smc->sk.sk_state = SMC_ACTIVE;
+	smc_tx_init(smc);
 
 	return rc ? rc : local_contact;
 
@@ -771,6 +773,7 @@ static void smc_listen_worker(struct wor
 out_connected:
 	sk_refcnt_debug_inc(newsmcsk);
 	newsmcsk->sk_state = SMC_ACTIVE;
+	smc_tx_init(new_smc);
 enqueue:
 	if (local_contact == SMC_FIRST_CONTACT)
 		mutex_unlock(&smc_create_lgr_pending);
@@ -942,7 +945,7 @@ static int smc_sendmsg(struct socket *so
 	if (smc->use_fallback)
 		rc = smc->clcsock->ops->sendmsg(smc->clcsock, msg, len);
 	else
-		rc = sock_no_sendmsg(sock, msg, len);
+		rc = smc_tx_sendmsg(smc, msg, len);
 out:
 	release_sock(sk);
 	return rc;
--- a/net/smc/smc.h
+++ b/net/smc/smc.h
@@ -135,6 +135,7 @@ struct smc_connection {
 	atomic_t		sndbuf_space;	/* remaining space in sndbuf */
 	u16			tx_cdc_seq;	/* sequence # for CDC send */
 	spinlock_t		send_lock;	/* protect wr_sends */
+	struct delayed_work	tx_work;	/* retry of smc_cdc_msg_send */
 
 	struct smc_host_cdc_msg	local_rx_ctrl;	/* filled during event_handl.
 						 * .prod cf. TCP rcv_nxt
--- a/net/smc/smc_cdc.c
+++ b/net/smc/smc_cdc.c
@@ -14,6 +14,7 @@
 #include "smc.h"
 #include "smc_wr.h"
 #include "smc_cdc.h"
+#include "smc_tx.h"
 
 struct smc_cdc_tx_pend {
 	struct smc_connection	*conn;		/* socket connection */
@@ -45,7 +46,7 @@ static void smc_cdc_tx_handler(struct sm
 		xchg(&cdcpend->conn->tx_curs_fin.acurs,
 		     cdcpend->cursor.acurs);
 	}
-	/* subsequent patch: wake if send buffer space available */
+	smc_tx_sndbuf_nonfull(smc);
 	bh_unlock_sock(&smc->sk);
 }
 
@@ -149,7 +150,9 @@ static void smc_cdc_msg_recv_action(stru
 	}
 
 	/* piggy backed tx info */
-	/* subsequent patch: wake receivers if receive buffer space available */
+	/* trigger sndbuf consumer: RDMA write into peer RMBE and CDC */
+	if (diff_cons)
+		smc_tx_sndbuf_nonempty(conn);
 
 	/* subsequent patch: trigger socket release if connection closed */
 
--- /dev/null
+++ b/net/smc/smc_tx.c
@@ -0,0 +1,449 @@
+/*
+ * Shared Memory Communications over RDMA (SMC-R) and RoCE
+ *
+ * Manage send buffer.
+ * Producer:
+ * Copy user space data into send buffer, if send buffer space available.
+ * Consumer:
+ * Trigger RDMA write into RMBE of peer and send CDC, if RMBE space available.
+ *
+ * Copyright IBM Corp. 2016
+ *
+ * Author(s):  Ursula Braun <ursula.braun@de.ibm.com>
+ */
+
+#include <linux/net.h>
+#include <linux/rcupdate.h>
+#include <net/sock.h>
+
+#include "smc.h"
+#include "smc_wr.h"
+#include "smc_cdc.h"
+#include "smc_tx.h"
+
+/***************************** sndbuf producer *******************************/
+
+/* callback implementation for sk.sk_write_space()
+ * to wakeup sndbuf producers that blocked with smc_tx_wait_memory()
+ */
+static void smc_tx_write_space(struct sock *sk)
+{
+	struct socket *sock = sk->sk_socket;
+	struct smc_sock *smc = smc_sk(sk);
+	struct socket_wq *wq;
+
+	/* similar to sk_stream_write_space */
+	if (atomic_read(&smc->conn.sndbuf_space) && sock) {
+		clear_bit(SOCK_NOSPACE, &sock->flags);
+		rcu_read_lock();
+		wq = rcu_dereference(sk->sk_wq);
+		if (wq_has_sleeper(wq))
+			wake_up_interruptible_poll(&wq->wait,
+						   POLLOUT | POLLWRNORM |
+						   POLLWRBAND);
+		if (wq && wq->fasync_list && !(sk->sk_shutdown & SEND_SHUTDOWN))
+			sock_wake_async(wq, SOCK_WAKE_SPACE, POLL_OUT);
+		rcu_read_unlock();
+	}
+}
+
+/* Wakeup sndbuf producers that blocked with smc_tx_wait_memory().
+ * Cf. tcp_data_snd_check()=>tcp_check_space()=>tcp_new_space().
+ */
+void smc_tx_sndbuf_nonfull(struct smc_sock *smc)
+{
+	if (smc->sk.sk_socket &&
+	    atomic_read(&smc->conn.sndbuf_space) &&
+	    test_bit(SOCK_NOSPACE, &smc->sk.sk_socket->flags))
+		smc->sk.sk_write_space(&smc->sk);
+}
+
+/* sndbuf producer */
+static inline int smc_tx_give_up_send(struct smc_sock *smc, int copied)
+{
+	struct smc_connection *conn = &smc->conn;
+
+	if (smc->sk.sk_shutdown & SEND_SHUTDOWN ||
+	    conn->local_tx_ctrl.conn_state_flags.abnormal_close)
+		return -EPIPE;
+	if (conn->local_rx_ctrl.conn_state_flags.abnormal_close ||
+	    conn->local_rx_ctrl.conn_state_flags.closed_conn)
+		return copied ? copied : -ECONNRESET;
+	return 0;
+}
+
+/* blocks sndbuf producer until at least one byte of free space available */
+static int smc_tx_wait_memory(struct smc_sock *smc, int flags)
+{
+	struct smc_connection *conn = &smc->conn;
+	struct sock *sk = &smc->sk;
+	DEFINE_WAIT(wait);
+	long timeo;
+	int rc = 0;
+
+	/* similar to sk_stream_wait_memory */
+	timeo = sock_sndtimeo(sk, flags & MSG_DONTWAIT);
+	sk_set_bit(SOCKWQ_ASYNC_NOSPACE, sk);
+	prepare_to_wait(sk_sleep(sk), &wait, TASK_INTERRUPTIBLE);
+	if (sk->sk_err || (sk->sk_shutdown & SEND_SHUTDOWN) ||
+	    conn->local_tx_ctrl.conn_state_flags.sending_done) {
+		rc = -EPIPE;
+		goto out;
+	}
+	if (conn->local_rx_ctrl.conn_state_flags.abnormal_close) {
+		rc = -ECONNRESET;
+		goto out;
+	}
+	if (!timeo) {
+		rc = -EAGAIN;
+		goto out;
+	}
+	if (signal_pending(current)) {
+		rc = -EINTR;
+		goto out;
+	}
+	sk_clear_bit(SOCKWQ_ASYNC_NOSPACE, sk);
+	if (atomic_read(&conn->sndbuf_space))
+		goto out;
+	set_bit(SOCK_NOSPACE, &sk->sk_socket->flags);
+	rc = sk_wait_event(sk, &timeo,
+			   sk->sk_err ||
+			   (sk->sk_shutdown & SEND_SHUTDOWN) ||
+			   smc_stop_received(conn) ||
+			   atomic_read(&conn->sndbuf_space));
+	clear_bit(SOCK_NOSPACE, &sk->sk_socket->flags);
+	if (sk->sk_err || (sk->sk_shutdown & SEND_SHUTDOWN) ||
+	    conn->local_tx_ctrl.conn_state_flags.sending_done) {
+		rc = -EPIPE;
+	}
+	if (conn->local_rx_ctrl.conn_state_flags.abnormal_close)
+		rc = -ECONNRESET;
+out:
+	finish_wait(sk_sleep(sk), &wait);
+	return rc;
+}
+
+/* sndbuf producer: main API called by socket layer */
+int smc_tx_sendmsg(struct smc_sock *smc, struct msghdr *msg, size_t len)
+{
+	size_t chunk_len, send_done = 0, send_remaining = len;
+	struct smc_connection *conn = &smc->conn;
+	union smc_host_cursor_ovl prep;
+	int tx_top, tx_bot;
+	char *sndbuf_base;
+	int tx_cnt_prep;
+	int writespace;
+	int rc;
+
+again:
+	if (smc->sk.sk_state == SMC_INIT)
+		return -ENOTCONN;
+	if (smc->sk.sk_state != SMC_ACTIVE)
+		return -ECONNRESET;
+	rc = smc_tx_give_up_send(smc, send_done);
+	if (rc)
+		return rc;
+	/* what to do in case of smc->sk.sk_err ??? */
+
+	writespace = atomic_read(&conn->sndbuf_space);
+	if (!writespace) {
+		int wait_rc;
+
+		wait_rc = smc_tx_wait_memory(smc, msg->msg_flags);
+		if (wait_rc < 0)
+			return ((send_done && (wait_rc == -EAGAIN))
+			       ? send_done : wait_rc);
+		if (!wait_rc) {
+			rc = smc_tx_give_up_send(smc, send_done);
+			if (rc)
+				return rc;
+			if (msg->msg_flags & MSG_DONTWAIT)
+				return send_done ? send_done : -EAGAIN;
+			if (smc->sk.sk_err)
+				return send_done ? send_done : -EPIPE;
+			goto again;
+		}
+	}
+	if (smc->sk.sk_err)
+		return -EPIPE;
+	rc = smc_tx_give_up_send(smc, send_done);
+	if (rc)
+		return rc;
+
+	/* re-calc, could be just 1 byte after smc_tx_wait_memory above */
+	writespace = atomic_read(&conn->sndbuf_space);
+	chunk_len = min_t(size_t, send_remaining, writespace);
+	/* determine start of sndbuf */
+	prep.acurs = smc_curs_read(conn->tx_curs_prep.acurs);
+	tx_cnt_prep = prep.curs.count;
+	sndbuf_base = conn->sndbuf_desc->cpu_addr;
+	/* determine sndbuf chunks - top and bottom of sndbuf */
+	if (tx_cnt_prep + chunk_len <= conn->sndbuf_size) {
+		tx_top = 0;
+		tx_bot = chunk_len;
+		if (memcpy_from_msg(sndbuf_base + tx_cnt_prep, msg, chunk_len))
+			return -EFAULT;
+	} else {
+		tx_bot = conn->sndbuf_size - tx_cnt_prep;
+		tx_top = chunk_len - tx_bot;
+		if (memcpy_from_msg(sndbuf_base + tx_cnt_prep, msg, tx_bot))
+			return -EFAULT;
+		if (memcpy_from_msg(sndbuf_base, msg, tx_top))
+			return -EFAULT;
+	}
+	smc_curs_add(conn->sndbuf_size, &prep.curs, chunk_len);
+	xchg(&conn->tx_curs_prep.acurs, prep.acurs);
+	smp_mb__before_atomic();
+	atomic_sub(chunk_len, &conn->sndbuf_space);
+	smp_mb__after_atomic();
+
+	/* since we just produced more new data into sndbuf,
+	 * trigger sndbuf consumer: RDMA write into peer RMBE and CDC
+	 */
+	rc = smc_tx_sndbuf_nonempty(conn);
+	if (rc)
+		return rc;
+
+	send_done += chunk_len;
+	send_remaining -= chunk_len;
+	if (send_done < len)
+		goto again;
+
+	return send_done;
+}
+
+/***************************** sndbuf consumer *******************************/
+
+/* sndbuf consumer: actual data transfer of one target chunk with RDMA write */
+static int smc_tx_rdma_write(struct smc_connection *conn, int peer_rmbe_offset,
+			     int num_sges, struct ib_sge sges[])
+{
+	struct smc_link_group *lgr = conn->lgr;
+	struct ib_send_wr *failed_wr = NULL;
+	struct ib_rdma_wr rdma_wr;
+	struct smc_link *link;
+	int i, rc;
+
+	memset(&rdma_wr, 0, sizeof(rdma_wr));
+	link = &lgr->lnk[SMC_SINGLE_LINK];
+	for (i = 0; i < num_sges; i++) {
+		sges[i].addr =
+			conn->sndbuf_desc->dma_addr[SMC_SINGLE_LINK] +
+			sges[i].addr;
+		sges[i].lkey = link->mr_tx->lkey;
+	}
+	rdma_wr.wr.wr_id = smc_wr_tx_get_next_wr_id(link);
+	rdma_wr.wr.sg_list = sges;
+	rdma_wr.wr.num_sge = num_sges;
+	rdma_wr.wr.opcode = IB_WR_RDMA_WRITE;
+	rdma_wr.remote_addr =
+		lgr->rtokens[conn->rtoken_idx][SMC_SINGLE_LINK].dma_addr +
+		peer_rmbe_offset +
+		((conn->peer_conn_idx - 1) * (conn->peer_rmbe_len));
+	rdma_wr.rkey = lgr->rtokens[conn->rtoken_idx][SMC_SINGLE_LINK].rkey;
+	rc = ib_post_send(link->roce_qp, &rdma_wr.wr, &failed_wr);
+	if (rc)
+		conn->local_tx_ctrl.conn_state_flags.abnormal_close = 1;
+	return rc;
+}
+
+/* sndbuf consumer */
+static inline void smc_tx_fill_sges(int *num_sges, struct ib_sge sges[],
+				    u64 sge_offset1, u32 sge_len1,
+				    u64 sge_offset2, u32 sge_len2)
+{
+	memset(sges, 0, SMC_IB_MAX_SEND_SGE * sizeof(sges[0]));
+	sges[0].addr = sge_offset1;
+	sges[0].length = sge_len1;
+	if (sge_len2) {
+		*num_sges = 2;
+		sges[1].addr = sge_offset2;
+		sges[1].length = sge_len2;
+	} else {
+		*num_sges = 1;
+	}
+}
+
+/* sndbuf consumer */
+static inline void smc_tx_advance_cursors(struct smc_connection *conn,
+					  union smc_host_cursor_ovl *prod,
+					  union smc_host_cursor_ovl *sent,
+					  size_t len)
+{
+	smc_curs_add(conn->peer_rmbe_len, &prod->curs, len);
+	smp_mb__before_atomic();
+	/* data in flight reduces usable snd_wnd */
+	atomic_sub(len, &conn->peer_rmbe_space);
+	smp_mb__after_atomic();
+	smc_curs_add(conn->sndbuf_size, &sent->curs, len);
+}
+
+/* sndbuf consumer: prepare all necessary (src&dst) chunks of data transmit;
+ * usable snd_wnd as max transmit
+ */
+static int smc_tx_rdma_writes(struct smc_connection *conn)
+{
+	union smc_host_cursor_ovl sent, prep, prod, cons;
+	size_t to_copy, space1, space2, send_len;
+	struct ib_sge sges[SMC_IB_MAX_SEND_SGE];
+	size_t tx_top1, tx_top2;
+	size_t tx_bot1, tx_bot2;
+	size_t tx_top,  tx_bot;
+	int to_send, rmbespace;
+	int num_sges;
+	int rc;
+
+	sent.acurs = smc_curs_read(conn->tx_curs_sent.acurs);
+	prep.acurs = smc_curs_read(conn->tx_curs_prep.acurs);
+
+	/* cf. wmem_alloc - (snd_max - snd_una) */
+	to_send = smc_curs_diff(conn->sndbuf_size, &sent, &prep);
+	if (to_send <= 0)
+		return 0;
+
+	/* cf. snd_wnd */
+	rmbespace = atomic_read(&conn->peer_rmbe_space);
+	if (rmbespace <= 0)
+		return 0;
+
+	if (to_send >= rmbespace)
+		conn->local_tx_ctrl.prod_flags.write_blocked = 1;
+	else
+		conn->local_tx_ctrl.prod_flags.write_blocked = 0;
+
+	/* cf. usable snd_wnd */
+	to_copy = min(to_send, rmbespace);
+
+	if (sent.curs.count + to_copy <= conn->peer_rmbe_len) {
+		tx_top = 0;
+		tx_bot = to_copy;
+	} else {
+		tx_bot = conn->sndbuf_size - sent.curs.count;
+		tx_top = to_copy - tx_bot;
+	}
+	prod.acurs = smc_curs_read(conn->local_tx_ctrl.prod.acurs);
+	cons.acurs = smc_curs_read(conn->local_rx_ctrl.cons.acurs);
+	if (prod.curs.wrap == cons.curs.wrap) {
+		space1 = conn->peer_rmbe_len - prod.curs.count;
+		space2 = cons.curs.count;
+
+		send_len = min(to_copy, space1);
+		if (send_len <= tx_bot) {
+			tx_bot1 = send_len;
+			tx_bot2 = tx_bot - tx_bot1;
+			tx_top1 = 0;
+			tx_top2 = tx_top;
+		} else {
+			tx_bot1 = tx_bot;
+			tx_bot2 = 0;
+			tx_top1 = send_len - tx_bot;
+			tx_top2 = tx_top - tx_top1;
+		}
+		smc_tx_fill_sges(&num_sges, sges, sent.curs.count, tx_bot1, 0,
+				 tx_top1);
+		rc = smc_tx_rdma_write(conn, prod.curs.count, num_sges, sges);
+		if (rc)
+			return rc;
+		to_copy -= send_len;
+		smc_tx_advance_cursors(conn, &prod, &sent, send_len);
+
+		if (to_copy && space2 && (tx_bot2 + tx_top2 > 0)) {
+			send_len = min(to_copy, space2);
+			if (tx_bot2 > send_len) {
+				tx_bot2 = send_len;
+				tx_top2 = 0;
+			} else {
+				if (tx_bot2 + tx_top2 > send_len)
+					tx_top2 = send_len - tx_bot2;
+			}
+			if (tx_bot2)
+				smc_tx_fill_sges(&num_sges, sges,
+						 sent.curs.count,
+						 tx_bot2, tx_top1, tx_top2);
+			else if (tx_top2)
+				smc_tx_fill_sges(&num_sges, sges, tx_top1,
+						 tx_top2, 0, 0);
+			rc = smc_tx_rdma_write(conn, 0, num_sges, sges);
+			if (rc)
+				return rc;
+			smc_tx_advance_cursors(conn, &prod, &sent,
+					       tx_bot2 + tx_top2);
+		}
+	} else {
+		space1 = cons.curs.count - prod.curs.count;
+		send_len = min(to_copy, space1);
+		if (send_len <= tx_bot) {
+			tx_bot = send_len;
+			tx_top = 0;
+		} else {
+			if ((send_len - tx_bot) <= tx_top)
+				tx_top = send_len - tx_bot;
+		}
+		smc_tx_fill_sges(&num_sges, sges, sent.curs.count, tx_bot, 0,
+				 tx_top);
+		rc = smc_tx_rdma_write(conn, prod.curs.count, num_sges, sges);
+		if (rc)
+			return rc;
+		smc_tx_advance_cursors(conn, &prod, &sent, send_len);
+	}
+	xchg(&conn->local_tx_ctrl.prod.acurs, prod.acurs);
+	xchg(&conn->tx_curs_sent.acurs, sent.acurs);
+
+	return 0;
+}
+
+/* Wakeup sndbuf consumers from any context (IRQ or process)
+ * since there is more data to transmit; usable snd_wnd as max transmit
+ */
+int smc_tx_sndbuf_nonempty(struct smc_connection *conn)
+{
+	struct smc_cdc_tx_pend *pend;
+	struct smc_wr_buf *wr_buf;
+	int rc;
+
+	spin_lock_bh(&conn->send_lock);
+	rc = smc_cdc_get_free_slot(&conn->lgr->lnk[SMC_SINGLE_LINK], &wr_buf,
+				   &pend);
+	if (rc < 0) {
+		schedule_delayed_work(&conn->tx_work, HZ / 10);
+		goto out_unlock;
+	}
+
+	rc = smc_tx_rdma_writes(conn);
+	if (rc) {
+		smc_wr_tx_put_slot(&conn->lgr->lnk[SMC_SINGLE_LINK],
+				   (struct smc_wr_tx_pend_priv *)pend);
+		goto out_unlock;
+	}
+
+	rc = smc_cdc_msg_send(conn, wr_buf, pend);
+
+out_unlock:
+	spin_unlock_bh(&conn->send_lock);
+	return rc;
+}
+
+/* Wakeup sndbuf consumers from process context
+ * since there is more data to transmit
+ */
+static void smc_tx_worker(struct work_struct *work)
+{
+	struct smc_connection *conn = container_of(to_delayed_work(work),
+						   struct smc_connection,
+						   tx_work);
+	struct smc_sock *smc = container_of(conn, struct smc_sock, conn);
+
+	lock_sock(&smc->sk);
+	smc_tx_sndbuf_nonempty(conn);
+	release_sock(&smc->sk);
+}
+
+/***************************** send initialize *******************************/
+
+/* Initialize send properties on connection establishment. NB: not __init! */
+void smc_tx_init(struct smc_sock *smc)
+{
+	smc->sk.sk_write_space = smc_tx_write_space;
+	INIT_DELAYED_WORK(&smc->conn.tx_work, smc_tx_worker);
+	spin_lock_init(&smc->conn.send_lock);
+}
--- /dev/null
+++ b/net/smc/smc_tx.h
@@ -0,0 +1,24 @@
+/*
+ * Shared Memory Communications over RDMA (SMC-R) and RoCE
+ *
+ * Manage send buffer
+ *
+ * Copyright IBM Corp. 2016
+ *
+ * Author(s):  Ursula Braun <ursula.braun@de.ibm.com>
+ */
+
+#ifndef SMC_TX_H
+#define SMC_TX_H
+
+#include <linux/socket.h>
+#include <linux/types.h>
+
+#include "smc.h"
+
+void smc_tx_init(struct smc_sock *);
+int smc_tx_sendmsg(struct smc_sock *, struct msghdr *, size_t);
+int smc_tx_sndbuf_nonempty(struct smc_connection *);
+void smc_tx_sndbuf_nonfull(struct smc_sock *);
+
+#endif /* SMC_TX_H */
