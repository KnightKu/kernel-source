From: Ursula Braun <ubraun@linux.vnet.ibm.com>
Subject: smc: socket closing and linkgroup cleanup
Patch-mainline: not yet, IBM pushing upstream
References: bsc#978258,FATE#319593,LTC#131290

Summary:     net/smc: Shared Memory Communications - RDMA
Description: Initial part of the implementation of the "Shared Memory
             Communications-RDMA" (SMC-R) protocol. The protocol is defined
             in RFC7609 [1]. It allows transparent transformation of TCP
             connections using the "Remote Direct Memory Access over
             Converged Ethernet" (RoCE) feature of certain communication
             hardware for data center environments. Tested on s390 and x86
             using Mellanox ConnectX-3 cards.

             A new socket protocol family PF_SMC is being introduced. A
             preload shared library will be offered to enable TCP-based
             applications to use SMC-R without changes or recompilation.

             References:
             [1] SMC-R Informational RFC:
             https://tools.ietf.org/rfc/rfc7609

Upstream-Description:

              smc: socket closing and linkgroup cleanup

              smc_shutdown() and smc_release() handling
              delayed linkgroup cleanup for linkgroups without connections

              Signed-off-by: Ursula Braun <ubraun@linux.vnet.ibm.com>

Signed-off-by: Ursula Braun <ubraun@linux.vnet.ibm.com>
Acked-by: John Jolly <jjolly@suse.de>
---
 net/smc/af_smc.c   |  412 +++++++++++++++++++++++++++++++++++++++++++++++++++--
 net/smc/smc.h      |   13 +
 net/smc/smc_cdc.c  |   48 ++++--
 net/smc/smc_cdc.h  |    2 
 net/smc/smc_core.h |    1 
 net/smc/smc_ib.c   |   11 +
 net/smc/smc_rx.c   |    5 
 net/smc/smc_rx.h   |    1 
 net/smc/smc_tx.c   |   35 ++++
 net/smc/smc_tx.h   |   11 +
 net/smc/smc_wr.c   |   10 -
 net/smc/smc_wr.h   |    9 +
 12 files changed, 521 insertions(+), 37 deletions(-)

--- a/net/smc/af_smc.c
+++ b/net/smc/af_smc.c
@@ -38,9 +38,11 @@
 #include "smc_tx.h"
 #include "smc_rx.h"
 
-static DEFINE_MUTEX(smc_create_lgr_pending);	/* serialize link group
-						 * creation
-						 */
+#define SMC_LISTEN_WORK_WAIT		20
+#define SMC_WAIT_TX_PENDS_TIME		(5 * HZ)
+#define SMC_TIMEWAIT_LEN		TCP_TIMEWAIT_LEN
+
+DEFINE_MUTEX(smc_create_lgr_pending);	/* serialize link group creation */
 
 struct smc_lgr_list smc_lgr_list = {		/* established link groups */
 	.lock = __SPIN_LOCK_UNLOCKED(smc_lgr_list.lock),
@@ -64,19 +66,220 @@ static struct proto smc_proto = {
 	.slab_flags	= SLAB_DESTROY_BY_RCU,
 };
 
+static void smc_destruct_non_accepted(struct sock *sk);
+static struct sock *smc_accept_dequeue(struct sock *, struct socket *);
+
+static void smc_sock_cleanup_listen(struct sock *parent)
+{
+	struct sock *sk;
+
+	/* Close non-accepted connections */
+	while ((sk = smc_accept_dequeue(parent, NULL)))
+		smc_destruct_non_accepted(sk);
+}
+
+static int smc_wait_tx_pends(struct smc_sock *smc)
+{
+	struct smc_connection *conn = &smc->conn;
+	struct sock *sk = &smc->sk;
+	signed long timeout;
+	DEFINE_WAIT(wait);
+	int rc = 0;
+
+	timeout = SMC_WAIT_TX_PENDS_TIME;
+	if (smc_cdc_wr_tx_pends(conn) && !(current->flags & PF_EXITING)) {
+		set_bit(SOCK_NOSPACE, &sk->sk_socket->flags);
+		do {
+			prepare_to_wait(sk_sleep(sk), &wait,
+					TASK_INTERRUPTIBLE);
+			if (sk_wait_event(sk, &timeout,
+					  !smc_cdc_wr_tx_pends(conn)))
+				break;
+		} while (!signal_pending(current) && timeout);
+		clear_bit(SOCK_NOSPACE, &sk->sk_socket->flags);
+		finish_wait(sk_sleep(sk), &wait);
+	}
+	if (!timeout) {		/* timeout reached, kill tx_pends */
+		smc_cdc_put_conn_slots(conn);
+		rc = -ETIME;
+	}
+	return rc;
+}
+
+static void smc_wait_close_tx_prepared(struct smc_sock *smc, long timeout)
+{
+	struct sock *sk = &smc->sk;
+
+	if (timeout) {
+		DEFINE_WAIT(wait);
+
+		do {
+			prepare_to_wait(sk_sleep(sk), &wait,
+					TASK_INTERRUPTIBLE);
+			if (sk_wait_event(sk, &timeout,
+					  !smc_tx_prepared_sends(&smc->conn)))
+			break;
+		} while (!signal_pending(current) && timeout);
+
+		finish_wait(sk_sleep(sk), &wait);
+	}
+}
+
+void smc_wake_close_tx_prepared(struct smc_sock *smc)
+{
+	if (smc->sk.sk_state == SMC_PEERCLW1)
+		/* wake up socket closing */
+		smc->sk.sk_state_change(&smc->sk);
+}
+
+static inline int smc_stream_closing(struct smc_connection *conn)
+{
+	return (!smc_cdc_wr_tx_pends(conn) &&
+		smc_close_received(conn));
+}
+
+static void smc_stream_wait_close(struct smc_sock *smc, long lingertime)
+{
+	struct sock *sk = &smc->sk;
+
+	if (lingertime) {
+		DEFINE_WAIT(wait);
+
+		do {
+			prepare_to_wait(sk_sleep(sk), &wait,
+					TASK_INTERRUPTIBLE);
+			if (sk_wait_event(sk, &lingertime,
+					  smc_stream_closing(&smc->conn)))
+				break;
+		} while (!signal_pending(current) && lingertime);
+
+		finish_wait(sk_sleep(sk), &wait);
+	}
+}
+
+static int smc_conn_release(struct smc_sock *smc)
+{
+	struct smc_connection *conn = &smc->conn;
+	long timeout = MAX_SCHEDULE_TIMEOUT;
+	struct sock *sk = &smc->sk;
+	long lingertime = 0;
+	int old_state;
+	int rc = 0;
+
+	if (sock_flag(sk, SOCK_LINGER) &&
+	    !(current->flags & PF_EXITING)) {
+		lingertime = sk->sk_lingertime;
+		timeout = sk->sk_lingertime;
+	}
+
+	old_state = sk->sk_state;
+	switch (old_state) {
+	case SMC_INIT:
+		sk->sk_state = SMC_CLOSED;
+		schedule_delayed_work(&smc->fin_work, SMC_TIMEWAIT_LEN);
+		break;
+	case SMC_LISTEN:
+		sk->sk_state = SMC_CLOSED;
+		sk->sk_state_change(sk);
+		old_state = SMC_CLOSED;
+		if (smc->clcsock && smc->clcsock->sk) {
+			rc = kernel_sock_shutdown(smc->clcsock, SHUT_RDWR);
+			/* wake up kernel_accept of smc_tcp_listen_worker */
+			smc->clcsock->sk->sk_data_ready(smc->clcsock->sk);
+		}
+		release_sock(sk);
+		smc_sock_cleanup_listen(sk);
+		flush_work(&smc->tcp_listen_work);
+		flush_work(&smc->smc_listen_work);
+		lock_sock_nested(sk, SINGLE_DEPTH_NESTING);
+		schedule_delayed_work(&smc->fin_work, SMC_TIMEWAIT_LEN);
+		break;
+	case SMC_ACTIVE:
+		/* active close */
+		/* wait for sndbuf data being posted */
+		/* SLD: postpone smc_tx_close, return immediately, no wait ???*/
+		smc_wait_close_tx_prepared(smc, timeout);
+		/* wait for confirmation of previous postings */
+		smc_wait_tx_pends(smc);
+		/* send close request */
+		rc = smc_tx_close(conn);
+		if (conn->local_rx_ctrl.conn_state_flags.sending_done)
+			sk->sk_state = SMC_PEERCLW2;
+		else
+			sk->sk_state = SMC_PEERCLW1;
+		/* fall through */
+	case SMC_PEERCLW1:
+	case SMC_PEERCLW2:
+		/* wait for confirmation of close request posting */
+		smc_wait_tx_pends(smc);
+		/* wait for close request from peer - comparable to
+		 * sk_stream_wait_close call of tcp
+		 */
+		smc_stream_wait_close(smc, lingertime);
+		if (smc_close_received(conn)) {
+			sk->sk_state = SMC_CLOSED;
+			schedule_delayed_work(&smc->fin_work, SMC_TIMEWAIT_LEN);
+		}
+		break;
+	case SMC_APPLFINCLW:
+		/* socket already shutdown wr or both (active close) */
+		sk->sk_state = SMC_CLOSED;
+		schedule_delayed_work(&smc->fin_work, SMC_TIMEWAIT_LEN);
+		break;
+	case SMC_APPLCLW1:
+	case SMC_APPLCLW2:
+		/* passive close */
+		if (!smc_close_received(conn))
+			/* wait for sndbuf data being posted */
+			smc_wait_close_tx_prepared(smc, timeout);
+		/* wait for confirmation of previous postings */
+		smc_wait_tx_pends(smc);
+		/* confirm close from peer */
+		rc = smc_tx_close(conn);
+		/* wait for confirmation of close request posting */
+		smc_wait_tx_pends(smc);
+		if (smc_close_received(conn)) {
+			sk->sk_state = SMC_CLOSED;
+			schedule_delayed_work(&smc->fin_work, SMC_TIMEWAIT_LEN);
+		} else {
+			sk->sk_state = SMC_PEERFINCLW;
+		}
+		break;
+	case SMC_PEERFINCLW:
+	case SMC_CLOSED:
+	default:
+		break;
+	}
+
+	if (old_state != sk->sk_state)
+		sk->sk_state_change(&smc->sk);
+	return rc;
+}
+
 static int smc_release(struct socket *sock)
 {
 	struct sock *sk = sock->sk;
 	struct smc_sock *smc;
+	int rc = 0;
 
 	if (!sk || (sk->sk_state == SMC_DESTRUCT))
 		goto out;
 
 	smc = smc_sk(sk);
 	sock_hold(sk);
-	lock_sock(sk);
+	if (sk->sk_state == SMC_LISTEN)
+		lock_sock_nested(sk, SINGLE_DEPTH_NESTING);
+	else
+		lock_sock(sk);
 
-	sk->sk_state = SMC_CLOSED;
+	if (smc->use_fallback) {
+		sk->sk_state = SMC_CLOSED;
+		sk->sk_state_change(sk);
+	} else {
+		sock_set_flag(sk, SOCK_DEAD);
+		sk->sk_shutdown = SHUTDOWN_MASK;
+		rc = smc_conn_release(smc);
+	}
 	if (smc->clcsock) {
 		sock_release(smc->clcsock);
 		smc->clcsock = NULL;
@@ -90,7 +293,80 @@ static int smc_release(struct socket *so
 
 	sock_put(sk);
 out:
-	return 0;
+	return rc;
+}
+
+static void smc_accept_unlink(struct sock *);
+
+/* some kind of closing has been received - normal, abnormal, or sending_done */
+void smc_conn_release_handler(struct smc_sock *smc)
+{
+	struct smc_connection *conn = &smc->conn;
+	struct sock *sk = &smc->sk;
+	int old_state;
+
+	old_state = sk->sk_state;
+	switch (sk->sk_state) {
+	/* Normal termination - Passive close part */
+	case SMC_INIT:
+	case SMC_ACTIVE:
+		if (conn->local_rx_ctrl.conn_state_flags.sending_done ||
+		    conn->local_rx_ctrl.conn_state_flags.closed_conn) {
+			/* complete any outstanding recv with zero-length
+			 * if peerclosedconn and pending data to be written
+			 * then reset conn
+			 */
+			sk->sk_state = SMC_APPLCLW1;
+		}
+		break;
+	case SMC_PEERFINCLW:
+		if (conn->local_rx_ctrl.conn_state_flags.closed_conn)
+			sk->sk_state = SMC_CLOSED;
+		break;
+		/* Normal termination - Active close part */
+	case SMC_PEERCLW1:
+		if (conn->local_rx_ctrl.conn_state_flags.sending_done) {
+			/* complete any outstanding recv with zero-length */
+			sk->sk_state = SMC_PEERCLW2;
+		} /* fall through */
+	case SMC_PEERCLW2:
+		if (conn->local_rx_ctrl.conn_state_flags.closed_conn) {
+			struct smc_host_cdc_msg *tx_ctrl = &conn->local_tx_ctrl;
+			/* complete any outstanding recv with zero-length */
+			if (sk->sk_shutdown == SHUTDOWN_MASK &&
+			    (tx_ctrl->conn_state_flags.closed_conn ||
+			     tx_ctrl->conn_state_flags.abnormal_close)) {
+				sk->sk_state = SMC_CLOSED;
+			} else {
+				sk->sk_state = SMC_APPLFINCLW;
+			}
+		}
+		break;
+	default:
+		break;
+	}
+
+	sock_set_flag(&smc->sk, SOCK_DONE);
+	if (smc_stop_received(conn)) {
+		sk->sk_shutdown = sk->sk_shutdown | RCV_SHUTDOWN;
+		if (smc->clcsock && smc->clcsock->sk) {
+			struct sock *tcpsk;
+
+			tcpsk = smc->clcsock->sk;
+			tcpsk->sk_shutdown = tcpsk->sk_shutdown | RCV_SHUTDOWN;
+		}
+	}
+	if (smc_close_received(conn) &&
+	    (sk->sk_state == SMC_CLOSED) &&
+	    sock_flag(sk, SOCK_DEAD) &&
+	    !smc_cdc_wr_tx_pends(conn)) /* make sure socket is freed */
+		schedule_delayed_work(&smc->fin_work, SMC_TIMEWAIT_LEN);
+	if ((old_state != sk->sk_state) &&
+	    (old_state != SMC_INIT))
+		sk->sk_state_change(sk);
+
+	smc->sk.sk_data_ready(&smc->sk);
+	smc->sk.sk_write_space(&smc->sk);
 }
 
 static void smc_destruct(struct sock *sk)
@@ -108,11 +384,21 @@ static void smc_destruct(struct sock *sk
 	}
 
 	sk->sk_state = SMC_DESTRUCT;
-	smc_conn_free(&smc->conn);
+	if (smc->conn.lgr)
+		smc_conn_free(&smc->conn);
 
 	sk_refcnt_debug_dec(sk);
 }
 
+static void smc_fin_worker(struct work_struct *work)
+{
+	struct smc_sock *smc =
+		container_of(work, struct smc_sock, fin_work.work);
+
+	cancel_delayed_work(&smc->fin_work);
+	sock_put(&smc->sk);
+}
+
 static struct sock *smc_sock_alloc(struct net *net, struct socket *sock)
 {
 	struct smc_sock *smc;
@@ -137,6 +423,7 @@ static struct sock *smc_sock_alloc(struc
 	INIT_WORK(&smc->tcp_listen_work, smc_tcp_listen_worker);
 	INIT_LIST_HEAD(&smc->accept_q);
 	spin_lock_init(&smc->accept_q_lock);
+	INIT_DELAYED_WORK(&smc->fin_work, smc_fin_worker);
 
 	return sk;
 }
@@ -529,6 +816,8 @@ static int smc_clcsock_accept(struct smc
 	lock_sock(&lsmc->sk);
 	if  (rc < 0) {
 		lsmc->sk.sk_err = -rc;
+		new_sk->sk_state = SMC_CLOSED;
+		sock_set_flag(sk, SOCK_DEAD);
 		sock_put(new_sk);
 		*new_smc = NULL;
 		goto out;
@@ -536,6 +825,8 @@ static int smc_clcsock_accept(struct smc
 	if (lsmc->sk.sk_state == SMC_CLOSED) {
 		if (new_clcsock)
 			sock_release(new_clcsock);
+		new_sk->sk_state = SMC_CLOSED;
+		sock_set_flag(sk, SOCK_DEAD);
 		sock_put(new_sk);
 		*new_smc = NULL;
 		goto out;
@@ -602,6 +893,11 @@ static void smc_destruct_non_accepted(st
 	struct smc_sock *smc = smc_sk(sk);
 
 	sock_hold(sk);
+	lock_sock(sk);
+	if (!sk->sk_lingertime)
+		/* wait long for peer closing */
+		sk->sk_lingertime = MAX_SCHEDULE_TIMEOUT;
+	smc_conn_release(smc);
 	if (smc->clcsock) {
 		struct socket *tcp;
 
@@ -609,7 +905,9 @@ static void smc_destruct_non_accepted(st
 		smc->clcsock = NULL;
 		sock_release(tcp);
 	}
-	/* more closing stuff to be added with socket closing patch */
+	release_sock(sk);
+	sock_set_flag(sk, SOCK_ZAPPED);
+	sock_set_flag(sk, SOCK_DEAD);
 	sock_put(sk);
 }
 
@@ -806,6 +1104,7 @@ decline_rdma:
 
 out_err:
 	newsmcsk->sk_state = SMC_CLOSED;
+	schedule_delayed_work(&new_smc->fin_work, TCP_TIMEWAIT_LEN);
 	goto enqueue; /* queue new sock with sk_err set */
 }
 
@@ -963,7 +1262,13 @@ static int smc_recvmsg(struct socket *so
 
 	smc = smc_sk(sk);
 	lock_sock(sk);
-	if ((sk->sk_state != SMC_ACTIVE) && (sk->sk_state != SMC_CLOSED))
+	if ((sk->sk_state != SMC_ACTIVE) &&
+	    (sk->sk_state != SMC_PEERCLW1) &&
+	    (sk->sk_state != SMC_PEERCLW2) &&
+	    (sk->sk_state != SMC_APPLCLW1) &&
+	    (sk->sk_state != SMC_APPLCLW2) &&
+	    (sk->sk_state != SMC_PEERABORTW) &&
+	    (sk->sk_state != SMC_PROCESSABORT))
 		goto out;
 
 	if (smc->use_fallback)
@@ -1029,12 +1334,72 @@ static unsigned int smc_poll(struct file
 			mask |= smc_accept_poll(sk);
 		if (sk->sk_err)
 			mask |= POLLERR;
-		/* for now - to be enhanced in follow-on patch */
+		if ((sk->sk_shutdown == SHUTDOWN_MASK) ||
+		    (sk->sk_state == SMC_CLOSED))
+			mask |= POLLHUP;
+		if (sk->sk_shutdown & RCV_SHUTDOWN)
+			mask |= POLLIN | POLLRDNORM | POLLRDHUP;
+		if (atomic_read(&smc->conn.bytes_to_rcv))
+			mask |= POLLIN | POLLRDNORM; /* in earlier patch */
+		if (sk->sk_state == SMC_APPLCLW1)
+			mask |= POLLIN;
+		if (!(sk->sk_shutdown & SEND_SHUTDOWN)) { /* in earlier patch */
+			if (atomic_read(&smc->conn.sndbuf_space)) {
+				mask |= POLLOUT | POLLWRNORM;
+			} else {
+				sk_set_bit(SOCKWQ_ASYNC_NOSPACE, sk);
+				set_bit(SOCK_NOSPACE, &sk->sk_socket->flags);
+			}
+		} else {
+			mask |= POLLOUT | POLLWRNORM;
+		}
 	}
 
 	return mask;
 }
 
+static int smc_conn_shutdown_write(struct smc_sock *smc)
+{
+	struct smc_connection *conn = &smc->conn;
+	long timeout = MAX_SCHEDULE_TIMEOUT;
+	struct sock *sk = &smc->sk;
+	int old_state;
+	int rc = 0;
+
+	if (sock_flag(sk, SOCK_LINGER))
+		timeout = sk->sk_lingertime;
+
+	old_state = sk->sk_state;
+	switch (old_state) {
+	case SMC_ACTIVE:
+		/* active close */
+		/* wait for sndbuf data being posted */
+		smc_wait_close_tx_prepared(smc, timeout);
+		rc = smc_tx_close_wr(conn);
+		if (conn->local_rx_ctrl.conn_state_flags.sending_done)
+			sk->sk_state = SMC_PEERCLW2;
+		else
+			sk->sk_state = SMC_PEERCLW1;
+		sk->sk_state_change(sk);
+		break;
+	case SMC_APPLCLW1:
+		/* passive close */
+		if (!smc_close_received(conn))
+			/* wait for sndbuf data being posted */
+			smc_wait_close_tx_prepared(smc, timeout);
+		/* confirm close from peer */
+		rc = smc_tx_close_wr(conn);
+		sk->sk_state = SMC_APPLCLW2;
+		break;
+	default:
+		break;
+	}
+
+	if (old_state != sk->sk_state)
+		sk->sk_state_change(&smc->sk);
+	return rc;
+}
+
 static int smc_shutdown(struct socket *sock, int how)
 {
 	struct sock *sk = sock->sk;
@@ -1049,7 +1414,11 @@ static int smc_shutdown(struct socket *s
 	lock_sock(sk);
 
 	rc = -ENOTCONN;
-	if (sk->sk_state == SMC_CLOSED)
+	if ((sk->sk_state != SMC_ACTIVE) &&
+	    (sk->sk_state != SMC_PEERCLW1) &&
+	    (sk->sk_state != SMC_PEERCLW2) &&
+	    (sk->sk_state != SMC_APPLCLW1) &&
+	    (sk->sk_state != SMC_APPLCLW2))
 		goto out;
 	if (smc->use_fallback) {
 		rc = kernel_sock_shutdown(smc->clcsock, how);
@@ -1057,7 +1426,18 @@ static int smc_shutdown(struct socket *s
 		if (sk->sk_shutdown == SHUTDOWN_MASK)
 			sk->sk_state = SMC_CLOSED;
 	} else {
-		rc = sock_no_shutdown(sock, how);
+		switch (how) {
+		case SHUT_RDWR:		/* shutdown in both directions */
+			rc = smc_conn_release(smc);
+			break;
+		case SHUT_WR:
+			rc = smc_conn_shutdown_write(smc);
+			break;
+		case SHUT_RD:
+			break;
+		}
+		rc = kernel_sock_shutdown(smc->clcsock, how);
+		sk->sk_shutdown |= ++how;
 	}
 
 out:
@@ -1265,14 +1645,18 @@ out_pnet:
 
 static void __exit smc_exit(void)
 {
+	LIST_HEAD(lgr_freeing_list);
 	struct smc_link_group *lgr, *lg;
 
 	spin_lock(&smc_lgr_list.lock);
-	list_for_each_entry_safe(lgr, lg, &smc_lgr_list.list, list) {
+	if (!list_empty(&smc_lgr_list.list))
+		list_splice_init(&smc_lgr_list.list, &lgr_freeing_list);
+	spin_unlock(&smc_lgr_list.lock);
+	list_for_each_entry_safe(lgr, lg, &lgr_freeing_list, list) {
+		cancel_delayed_work_sync(&lgr->free_work);
 		list_del_init(&lgr->list);
 		smc_lgr_free(lgr); /* free link group */
 	}
-	spin_unlock(&smc_lgr_list.lock);
 	smc_ib_unregister_client();
 	sock_unregister(PF_SMC);
 	proto_unregister(&smc_proto);
--- a/net/smc/smc.h
+++ b/net/smc/smc.h
@@ -32,6 +32,14 @@ enum smc_state {		/* possible states of
 	SMC_INIT	= 2,
 	SMC_CLOSED	= 7,
 	SMC_LISTEN	= 10,
+	SMC_PEERCLW1	= 20,
+	SMC_PEERCLW2	= 21,
+	SMC_APPLCLW1	= 22,
+	SMC_APPLCLW2	= 23,
+	SMC_APPLFINCLW	= 24,
+	SMC_PEERFINCLW	= 25,
+	SMC_PEERABORTW	= 26,
+	SMC_PROCESSABORT = 27,
 	SMC_DESTRUCT	= 32
 };
 
@@ -163,6 +171,7 @@ struct smc_sock {				/* smc sock contain
 	struct work_struct	smc_listen_work;/* prepare new accept socket */
 	struct list_head	accept_q;	/* sockets to be accepted */
 	spinlock_t		accept_q_lock;	/* protects accept_q */
+	struct delayed_work	fin_work;	/* final socket freeing */
 	u8			use_fallback : 1, /* fallback to tcp */
 				clc_started : 1;/* smc_connect_rdma ran */
 };
@@ -176,6 +185,8 @@ static inline struct smc_sock *smc_sk(co
 
 extern u8	local_systemid[SMC_SYSTEMID_LEN]; /* unique system identifier */
 
+extern struct mutex smc_create_lgr_pending;
+
 /* convert an u32 value into network byte order, store it into a 3 byte field */
 static inline void hton24(u8 *net, u32 host)
 {
@@ -236,5 +247,7 @@ int smc_netinfo_by_tcpsk(struct socket *
 void smc_conn_free(struct smc_connection *);
 int smc_conn_create(struct smc_sock *, __be32, struct smc_ib_device *, u8,
 		    struct smc_clc_msg_local *, int);
+void smc_conn_release_handler(struct smc_sock *);
+void smc_wake_close_tx_prepared(struct smc_sock *);
 
 #endif	/* _SMC_H */
--- a/net/smc/smc_cdc.c
+++ b/net/smc/smc_cdc.c
@@ -102,6 +102,35 @@ out:
 	return rc;
 }
 
+int smc_cdc_wr_tx_pends(struct smc_connection *conn)
+{
+	struct smc_link *link = &conn->lgr->lnk[SMC_SINGLE_LINK];
+	int i;
+
+	for_each_set_bit(i, link->wr_tx_mask, link->wr_tx_cnt) {
+		struct smc_cdc_tx_pend *tx_pend;
+
+		tx_pend = (struct smc_cdc_tx_pend *)&link->wr_tx_pends[i].priv;
+		if (tx_pend->conn == conn)
+			return 1;
+	}
+	return 0;
+}
+
+void smc_cdc_put_conn_slots(struct smc_connection *conn)
+{
+	struct smc_link *link = &conn->lgr->lnk[SMC_SINGLE_LINK];
+	int i;
+
+	for_each_set_bit(i, link->wr_tx_mask, link->wr_tx_cnt) {
+		struct smc_wr_tx_pend_priv *tx_pend;
+
+		tx_pend = &link->wr_tx_pends[i].priv;
+		if (((struct smc_cdc_tx_pend *)tx_pend)->conn == conn)
+			smc_wr_tx_put_slot(link, tx_pend);
+	}
+}
+
 static inline bool smc_cdc_before(u16 seq1, u16 seq2)
 {
 	return (s16)(seq1 - seq2) < 0;
@@ -131,6 +160,7 @@ static void smc_cdc_msg_recv_action(stru
 		smp_mb__before_atomic();
 		atomic_add(diff_cons, &conn->peer_rmbe_space);
 		smp_mb__after_atomic();
+		smc_rx_handler(smc);
 	}
 
 	diff_prod = smc_curs_diff(conn->rmbe_size, &prod_old,
@@ -143,19 +173,15 @@ static void smc_cdc_msg_recv_action(stru
 
 	if (conn->local_rx_ctrl.conn_state_flags.abnormal_close)
 		smc->sk.sk_err = ECONNRESET;
-	if (smc_stop_received(conn)) {
-		smc->sk.sk_shutdown |= RCV_SHUTDOWN;
-		sock_set_flag(&smc->sk, SOCK_DONE);
-
-		/* subsequent patch: terminate connection */
-	}
+	if (smc_stop_received(conn))
+		smc_conn_release_handler(smc);
 
 	/* piggy backed tx info */
 	/* trigger sndbuf consumer: RDMA write into peer RMBE and CDC */
-	if (diff_cons)
+	if (diff_cons && smc_tx_prepared_sends(conn)) {
 		smc_tx_sndbuf_nonempty(conn);
-
-	/* subsequent patch: trigger socket release if connection closed */
+		smc_wake_close_tx_prepared(smc);
+	}
 
 	/* socket connected but not accepted */
 	if (!smc->sk.sk_socket)
@@ -165,10 +191,6 @@ static void smc_cdc_msg_recv_action(stru
 	if ((conn->local_rx_ctrl.prod_flags.write_blocked) ||
 	    (conn->local_rx_ctrl.prod_flags.cons_curs_upd_req))
 		smc_tx_consumer_update(conn);
-	if (diff_prod ||
-	    smc_stop_received(conn) ||
-	    smc->sk.sk_shutdown & RCV_SHUTDOWN)
-		smc->sk.sk_data_ready(&smc->sk);
 }
 
 /* called under tasklet context */
--- a/net/smc/smc_cdc.h
+++ b/net/smc/smc_cdc.h
@@ -153,8 +153,10 @@ struct smc_cdc_tx_pend;
 
 int smc_cdc_get_free_slot(struct smc_link *, struct smc_wr_buf **,
 			  struct smc_cdc_tx_pend **);
+void smc_cdc_put_conn_slots(struct smc_connection *conn);
 int smc_cdc_msg_send(struct smc_connection *, struct smc_wr_buf *,
 		     struct smc_cdc_tx_pend *);
+int smc_cdc_wr_tx_pends(struct smc_connection *);
 int smc_cdc_init(void) __init;
 
 #endif /* SMC_CDC_H */
--- a/net/smc/smc_core.h
+++ b/net/smc/smc_core.h
@@ -131,6 +131,7 @@ struct smc_link_group {
 						/* used rtoken elements */
 
 	u32			id;		/* unique lgr id */
+	struct delayed_work	free_work;	/* delayed freeing of an lgr */
 };
 
 /* Find the connection associated with the given alert token in the link group.
--- a/net/smc/smc_ib.c
+++ b/net/smc/smc_ib.c
@@ -390,6 +390,14 @@ out:
 	return rc;
 }
 
+static void smc_ib_cleanup_per_ibdev(struct smc_ib_device *smcibdev)
+{
+	ib_destroy_cq(smcibdev->roce_cq_send);
+	ib_destroy_cq(smcibdev->roce_cq_recv);
+	ib_unregister_event_handler(&smcibdev->event_handler);
+	smc_wr_remove_dev(smcibdev);
+}
+
 static struct ib_client smc_ib_client;
 
 /* callback function for ib_register_client() */
@@ -436,8 +444,9 @@ static void smc_ib_remove_dev(struct ib_
 	struct smc_ib_device *smcibdev;
 
 	smcibdev = ib_get_client_data(ibdev, &smc_ib_client);
-	smc_wr_remove_dev(smcibdev);
 	ib_set_client_data(ibdev, &smc_ib_client, NULL);
+	if (smcibdev->initialized)
+		smc_ib_cleanup_per_ibdev(smcibdev);
 	spin_lock(&smc_ib_devices.lock);
 	list_del_init(&smcibdev->list); /* remove from smc_ib_devices */
 	spin_unlock(&smc_ib_devices.lock);
--- a/net/smc/smc_rx.c
+++ b/net/smc/smc_rx.c
@@ -177,6 +177,11 @@ check_repeat:
 	return read_done;
 }
 
+void smc_rx_handler(struct smc_sock *smc)
+{
+	smc->sk.sk_data_ready(&smc->sk);
+}
+
 /* Initialize receive properties on connection establishment. NB: not __init! */
 void smc_rx_init(struct smc_sock *smc)
 {
--- a/net/smc/smc_rx.h
+++ b/net/smc/smc_rx.h
@@ -19,5 +19,6 @@
 void smc_rx_init(struct smc_sock *);
 int smc_rx_to_read(struct smc_connection *);
 int smc_rx_recvmsg(struct smc_sock *, struct msghdr *, size_t, int);
+void smc_rx_handler(struct smc_sock *);
 
 #endif /* SMC_RX_H */
--- a/net/smc/smc_tx.c
+++ b/net/smc/smc_tx.c
@@ -423,6 +423,41 @@ out_unlock:
 	return rc;
 }
 
+int smc_tx_close_wr(struct smc_connection *conn)
+{
+	struct smc_cdc_tx_pend *pend;
+	struct smc_wr_buf *wr_buf;
+	int rc;
+
+	conn->local_tx_ctrl.conn_state_flags.sending_done = 1;
+
+	rc = smc_cdc_get_free_slot(&conn->lgr->lnk[SMC_SINGLE_LINK], &wr_buf,
+				   &pend);
+
+	rc = smc_cdc_msg_send(conn, wr_buf, pend);
+
+	return rc;
+}
+
+int smc_tx_close(struct smc_connection *conn)
+{
+	struct smc_cdc_tx_pend *pend;
+	struct smc_wr_buf *wr_buf;
+	int rc;
+
+	if (atomic_read(&conn->bytes_to_rcv))
+		conn->local_tx_ctrl.conn_state_flags.abnormal_close = 1;
+	else
+		conn->local_tx_ctrl.conn_state_flags.closed_conn = 1;
+
+	rc = smc_cdc_get_free_slot(&conn->lgr->lnk[SMC_SINGLE_LINK], &wr_buf,
+				   &pend);
+
+	rc = smc_cdc_msg_send(conn, wr_buf, pend);
+
+	return rc;
+}
+
 /* Wakeup sndbuf consumers from process context
  * since there is more data to transmit
  */
--- a/net/smc/smc_tx.h
+++ b/net/smc/smc_tx.h
@@ -16,10 +16,21 @@
 
 #include "smc.h"
 
+static inline int smc_tx_prepared_sends(struct smc_connection *conn)
+{
+	union smc_host_cursor_ovl sent, prep;
+
+	sent.acurs = smc_curs_read(conn->tx_curs_sent.acurs);
+	prep.acurs = smc_curs_read(conn->tx_curs_prep.acurs);
+	return smc_curs_diff(conn->sndbuf_size, &sent, &prep);
+}
+
 void smc_tx_init(struct smc_sock *);
 int smc_tx_sendmsg(struct smc_sock *, struct msghdr *, size_t);
 int smc_tx_sndbuf_nonempty(struct smc_connection *);
 void smc_tx_sndbuf_nonfull(struct smc_sock *);
 void smc_tx_consumer_update(struct smc_connection *);
+int smc_tx_close(struct smc_connection *);
+int smc_tx_close_wr(struct smc_connection *);
 
 #endif /* SMC_TX_H */
--- a/net/smc/smc_wr.c
+++ b/net/smc/smc_wr.c
@@ -32,15 +32,6 @@
 static DEFINE_HASHTABLE(smc_wr_rx_hash, SMC_WR_RX_HASH_BITS);
 static DEFINE_SPINLOCK(smc_wr_rx_hash_lock);
 
-struct smc_wr_tx_pend {	/* control data for a pending send request */
-	u64			wr_id;		/* work request id sent */
-	smc_wr_tx_handler	handler;
-	enum ib_wc_status	wc_status;	/* CQE status */
-	struct smc_link		*link;
-	u32			idx;
-	struct smc_wr_tx_pend_priv priv;
-};
-
 static bool smc_wr_tx_pending_on_link(struct smc_link *link)
 {
 	return find_first_bit(link->wr_tx_mask, link->wr_tx_cnt)
@@ -214,6 +205,7 @@ int smc_wr_tx_put_slot(struct smc_link *
 	pend = container_of(wr_pend_priv, struct smc_wr_tx_pend, priv);
 	if (pend->idx < link->wr_tx_cnt) {
 		test_and_clear_bit(pend->idx, link->wr_tx_mask);
+		memset(&pend, 0, sizeof(pend));
 		return 1;
 	}
 
--- a/net/smc/smc_wr.h
+++ b/net/smc/smc_wr.h
@@ -40,6 +40,15 @@ struct smc_wr_rx_handler {
 	u8			type;
 };
 
+struct smc_wr_tx_pend {		/* control data for a pending send request */
+	u64			wr_id;		/* work request id sent */
+	smc_wr_tx_handler	handler;
+	enum ib_wc_status	wc_status;	/* CQE status */
+	struct smc_link		*link;
+	u32			idx;
+	struct smc_wr_tx_pend_priv priv;
+};
+
 /* Only used by RDMA write WRs.
  * All other WRs (CDC/LLC) use smc_wr_tx_send handling WR_ID implicitly
  */
