From: Tariq Toukan <tariqt@mellanox.com>
Date: Thu, 15 Sep 2016 16:08:37 +0300
Subject: net/mlx5e: Introduce API for RX mapped pages
Patch-mainline: v4.9-rc1
Git-commit: a5a0c590166e39fa399940775e7bfd8e1a9356da
References: bsc#1015342 FATE#321688 bsc#1015343 FATE#321689

Manage the allocation and deallocation of mapped RX pages only
through dedicated API functions.

Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
Signed-off-by: David S. Miller <davem@davemloft.net>
Acked-by: Benjamin Poirier <bpoirier@suse.com>
---
 drivers/net/ethernet/mellanox/mlx5/core/en_rx.c |   47 +++++++++++++-----------
 1 file changed, 27 insertions(+), 20 deletions(-)

--- a/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
@@ -305,26 +305,32 @@ static inline void mlx5e_post_umr_wqe(st
 	mlx5e_tx_notify_hw(sq, &wqe->ctrl, 0);
 }
 
-static inline int mlx5e_alloc_and_map_page(struct mlx5e_rq *rq,
-					   struct mlx5e_mpw_info *wi,
-					   int i)
+static inline int mlx5e_page_alloc_mapped(struct mlx5e_rq *rq,
+					  struct mlx5e_dma_info *dma_info)
 {
 	struct page *page = dev_alloc_page();
+
 	if (unlikely(!page))
 		return -ENOMEM;
 
-	wi->umr.dma_info[i].page = page;
-	wi->umr.dma_info[i].addr = dma_map_page(rq->pdev, page, 0, PAGE_SIZE,
-						PCI_DMA_FROMDEVICE);
-	if (unlikely(dma_mapping_error(rq->pdev, wi->umr.dma_info[i].addr))) {
+	dma_info->page = page;
+	dma_info->addr = dma_map_page(rq->pdev, page, 0, PAGE_SIZE,
+				      DMA_FROM_DEVICE);
+	if (unlikely(dma_mapping_error(rq->pdev, dma_info->addr))) {
 		put_page(page);
 		return -ENOMEM;
 	}
-	wi->umr.mtt[i] = cpu_to_be64(wi->umr.dma_info[i].addr | MLX5_EN_WR);
 
 	return 0;
 }
 
+static inline void mlx5e_page_release(struct mlx5e_rq *rq,
+				      struct mlx5e_dma_info *dma_info)
+{
+	dma_unmap_page(rq->pdev, dma_info->addr, PAGE_SIZE, DMA_FROM_DEVICE);
+	put_page(dma_info->page);
+}
+
 static int mlx5e_alloc_rx_umr_mpwqe(struct mlx5e_rq *rq,
 				    struct mlx5e_rx_wqe *wqe,
 				    u16 ix)
@@ -336,11 +342,13 @@ static int mlx5e_alloc_rx_umr_mpwqe(stru
 	int i;
 
 	for (i = 0; i < MLX5_MPWRQ_PAGES_PER_WQE; i++) {
-		err = mlx5e_alloc_and_map_page(rq, wi, i);
+		struct mlx5e_dma_info *dma_info = &wi->umr.dma_info[i];
+
+		err = mlx5e_page_alloc_mapped(rq, dma_info);
 		if (unlikely(err))
 			goto err_unmap;
-		atomic_add(pg_strides,
-			   &wi->umr.dma_info[i].page->_count);
+		wi->umr.mtt[i] = cpu_to_be64(dma_info->addr | MLX5_EN_WR);
+		atomic_add(pg_strides, &dma_info->page->_count);
 		wi->skbs_frags[i] = 0;
 	}
 
@@ -351,11 +359,10 @@ static int mlx5e_alloc_rx_umr_mpwqe(stru
 
 err_unmap:
 	while (--i >= 0) {
-		dma_unmap_page(rq->pdev, wi->umr.dma_info[i].addr, PAGE_SIZE,
-			       PCI_DMA_FROMDEVICE);
-		atomic_sub(pg_strides,
-			   &wi->umr.dma_info[i].page->_count);
-		put_page(wi->umr.dma_info[i].page);
+		struct mlx5e_dma_info *dma_info = &wi->umr.dma_info[i];
+
+		atomic_sub(pg_strides, &dma_info->page->_count);
+		mlx5e_page_release(rq, dma_info);
 	}
 
 	return err;
@@ -367,11 +374,11 @@ void mlx5e_free_rx_mpwqe(struct mlx5e_rq
 	int i;
 
 	for (i = 0; i < MLX5_MPWRQ_PAGES_PER_WQE; i++) {
-		dma_unmap_page(rq->pdev, wi->umr.dma_info[i].addr, PAGE_SIZE,
-			       PCI_DMA_FROMDEVICE);
+		struct mlx5e_dma_info *dma_info = &wi->umr.dma_info[i];
+
 		atomic_sub(pg_strides - wi->skbs_frags[i],
-			   &wi->umr.dma_info[i].page->_count);
-		put_page(wi->umr.dma_info[i].page);
+			   &dma_info->page->_count);
+		mlx5e_page_release(rq, dma_info);
 	}
 }
 
