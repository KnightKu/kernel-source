From: NeilBrown <neilb@suse.com>
Subject: [PATCH] nfs: improve shinking of access cache.
Patch-mainline: not yet, under development
References: bsc#1012917

This patch contains 3 changes to help keep the per-inode
access cache at a reasonable size.

1/ The shinker shouldn't round the current total size
   down to a multiple of 100.  If it then discards
   fewer than 100 entries it could report no change
   which is confusing.

2/ The shrinker should keep shrinking until it
   has achieved the goal, or cannot.  Currently it
   discards are most one entry per file.  If there
   are few files, each with many entries, this isn't
   very effective.

3/ When adding an entry, remove the oldest entry if
   has already expired.  This keeps the size smaller
   even when no shrinking happens.

Acked-by: NeilBrown <neilb@suse.com>
Signed-off-by: Neil Brown <neilb@suse.com>

---
 fs/nfs/dir.c |   34 ++++++++++++++++++++++++++++++----
 1 file changed, 30 insertions(+), 4 deletions(-)

--- a/fs/nfs/dir.c
+++ b/fs/nfs/dir.c
@@ -2175,14 +2175,17 @@ int nfs_access_cache_shrinker(struct shr
 	struct nfs_access_entry *cache;
 	int nr_to_scan = sc->nr_to_scan;
 	gfp_t gfp_mask = sc->gfp_mask;
+	int keep_going;
 
 	if ((gfp_mask & GFP_KERNEL) != GFP_KERNEL)
 		return (nr_to_scan == 0) ? 0 : -1;
 
 	if (nr_to_scan == 0)
-		return (atomic_long_read(&nfs_access_nr_entries) / 100) * sysctl_vfs_cache_pressure;
+		return (atomic_long_read(&nfs_access_nr_entries) * sysctl_vfs_cache_pressure) / 100;
 	spin_lock(&nfs_access_lru_lock);
-	list_for_each_entry_safe(nfsi, next, &nfs_access_lru_list, access_cache_inode_lru) {
+	do {
+	    keep_going = 0;
+	    list_for_each_entry_safe(nfsi, next, &nfs_access_lru_list, access_cache_inode_lru) {
 		struct inode *inode;
 
 		if (nr_to_scan-- == 0)
@@ -2191,6 +2194,7 @@ int nfs_access_cache_shrinker(struct shr
 		spin_lock(&inode->i_lock);
 		if (list_empty(&nfsi->access_cache_entry_lru))
 			goto remove_lru_entry;
+		keep_going = 1;
 		cache = list_entry(nfsi->access_cache_entry_lru.next,
 				struct nfs_access_entry, lru);
 		list_move(&cache->lru, &head);
@@ -2206,10 +2210,11 @@ remove_lru_entry:
 			smp_mb__after_clear_bit();
 		}
 		spin_unlock(&inode->i_lock);
-	}
+	    }
+	} while(keep_going && nr_to_scan > 0);
 	spin_unlock(&nfs_access_lru_lock);
 	nfs_access_free_list(&head);
-	return (atomic_long_read(&nfs_access_nr_entries) / 100) * sysctl_vfs_cache_pressure;
+	return (atomic_long_read(&nfs_access_nr_entries) * sysctl_vfs_cache_pressure) / 100;
 }
 
 static void __nfs_access_zap_cache(struct nfs_inode *nfsi, struct list_head *head)
@@ -2368,6 +2373,27 @@ void nfs_access_add_cache(struct inode *
 	struct nfs_access_entry *cache = kmalloc(sizeof(*cache), GFP_KERNEL);
 	if (cache == NULL)
 		return;
+	/* If there is an old entry, remove it first to avoid cache getting
+	 * too large
+	 */
+	if (!list_empty(&NFS_I(inode)->access_cache_entry_lru)) {
+		struct nfs_access_entry *old;
+		spin_lock(&inode->i_lock);
+		old = list_first_entry_or_null(&NFS_I(inode)->access_cache_entry_lru,
+					       struct nfs_access_entry, lru);
+		if (old &&
+		    !nfs_have_delegated_attributes(inode) &&
+		    !time_in_range_open(jiffies, old->jiffies,
+					old->jiffies + NFS_I(inode)->attrtimeo)) {
+			list_del_init(&old->lru);
+			rb_erase(&old->rb_node, &NFS_I(inode)->access_cache);
+		} else
+			old = NULL;
+		spin_unlock(&inode->i_lock);
+		if (old)
+			nfs_access_free_entry(old);
+	}
+
 	RB_CLEAR_NODE(&cache->rb_node);
 	cache->jiffies = set->jiffies;
 	cache->cred = get_rpccred(set->cred);
