From: Coly Li <colyli@suse.de>
Subject: [PATCH] RAID1: avoid unnecessary spin locks in I/O barrier code
Git-commit: 824e47daddbfc6ebe1006b8659f080620472a136
Patch-mainline: v4.11-rc6
References: bsc#982783,bsc#1026260

We should avoid taking a lock in the hotpath during I/O submission;
this hurts performance very bad when running on fast storage.

NOTE: Mainline code has much difference from SLE11-SP4 code, therefore
I rewrite this patch with similar idea of mainline patch, just
for SLE12-SP1 kernel code.

Signed-off-by: Coly Li <colyli@suse.de>
---
 drivers/md/raid1.c |  116 +++++++++++++++++++++++++++++++++++------------------
 drivers/md/raid1.h |    6 +-
 2 files changed, 80 insertions(+), 42 deletions(-)

--- a/drivers/md/raid1.c
+++ b/drivers/md/raid1.c
@@ -633,15 +633,26 @@ static void raise_barrier(conf_t *conf)
 	spin_lock_irq(&conf->resync_lock);
 
 	/* Wait until no block IO is waiting */
-	wait_event_lock_irq(conf->wait_barrier, !conf->nr_waiting,
+	wait_event_lock_irq(conf->wait_barrier,
+			    !atomic_read(&conf->nr_waiting),
 			    conf->resync_lock, );
 
 	/* block any new IO from starting */
-	conf->barrier++;
+	atomic_inc(&conf->barrier);
+	/*
+	 * In raise_barrier() we firstly increase conf->barrier then
+	 * check conf->nr_pending. In wait_barrier() we firstly
+	 * increase conf->nr_pending then check conf->barrier.
+	 * A memory barrier here to make sure conf->nr_pending won't
+	 * be fetched before conf->barrier is increased. Otherwise
+	 * there will be a race between raise_barrier() and wait_barrier().
+	 */
+	smp_mb__after_atomic_inc();
 
 	/* Now wait for all pending IO to complete */
 	wait_event_lock_irq(conf->wait_barrier,
-			    !conf->nr_pending && conf->barrier < RESYNC_DEPTH,
+			    !atomic_read(&conf->nr_pending) &&
+			    atomic_read(&conf->barrier) < RESYNC_DEPTH,
 			    conf->resync_lock, );
 
 	spin_unlock_irq(&conf->resync_lock);
@@ -649,47 +660,74 @@ static void raise_barrier(conf_t *conf)
 
 static void lower_barrier(conf_t *conf)
 {
-	unsigned long flags;
-	BUG_ON(conf->barrier <= 0);
-	spin_lock_irqsave(&conf->resync_lock, flags);
-	conf->barrier--;
-	spin_unlock_irqrestore(&conf->resync_lock, flags);
+	BUG_ON(atomic_read(&conf->barrier) <= 0);
+	atomic_dec(&conf->barrier);
 	wake_up(&conf->wait_barrier);
 }
 
 static void wait_barrier(conf_t *conf)
 {
+	/*
+	 * We need to increase conf->nr_pending very early here,
+	 * then raise_barrier() can be blocked when it waits for
+	 * conf->nr_pending to be 0. Then we can avoid holding
+	 * conf->resync_lock when there is no barrier raised in same
+	 * barrier unit bucket. Also if the array is frozen, I/O
+	 * should be blocked until array is unfrozen.
+	 */
+	atomic_inc(&conf->nr_pending);
+	/*
+	 * In wait_barrier() we firstly increase conf->nr_pending, then
+	 * check conf->barrier. In raise_barrier() we firstly increase
+	 * conf->barrier, then check conf->nr_pending. A memory
+	 * barrier is necessary here to make sure conf->barrier won't be
+	 * fetched before conf->nr_pending is increased. Otherwise there
+	 * will be a race between wait_barrier() and raise_barrier().
+	 */
+	smp_mb__after_atomic_inc();
+
+	if (!atomic_read(&conf->barrier))
+		return;
+
+	/*
+	 * After holding conf->resync_lock, conf->nr_pending
+	 * should be decreased before waiting for barrier to drop.
+	 * Otherwise, we may encounter a race condition because
+	 * raise_barrer() might be waiting for conf->nr_pending
+	 * to be 0 at same time.
+	 */
 	spin_lock_irq(&conf->resync_lock);
-	if (conf->barrier) {
-		conf->nr_waiting++;
-		/* Wait for the barrier to drop.
-		 * However if there are already pending
-		 * requests (preventing the barrier from
-		 * rising completely), and the
-		 * pre-process bio queue isn't empty,
-		 * then don't wait, as we need to empty
-		 * that queue to get the nr_pending
-		 * count down.
-		 */
-		wait_event_lock_irq(conf->wait_barrier,
-				    !conf->barrier ||
-				    (conf->nr_pending &&
-				     current->bio_list &&
-				     !bio_list_empty(current->bio_list)),
-				    conf->resync_lock,
+	atomic_inc(&conf->nr_waiting);
+	atomic_dec(&conf->nr_pending);
+	/*
+	 * In case freeze_array() is waiting for
+	 * get_unqueued_pending() == extra
+	 */
+        wake_up(&conf->wait_barrier);
+	/* Wait for the barrier to drop.
+	 * However if there are already pending
+	 * requests (preventing the barrier from
+	 * rising completely), and the
+	 * pre-process bio queue isn't empty,
+	 * then don't wait, as we need to empty
+	 * that queue to get the nr_pending
+	 * count down.
+	 */
+	wait_event_lock_irq(conf->wait_barrier,
+			    !atomic_read(&conf->barrier) ||
+			    (atomic_read(&conf->nr_pending) &&
+			     current->bio_list &&
+			     !bio_list_empty(current->bio_list)),
+			    conf->resync_lock,
 			);
-		conf->nr_waiting--;
-	}
-	conf->nr_pending++;
+	atomic_inc(&conf->nr_pending);
+	atomic_dec(&conf->nr_waiting);
 	spin_unlock_irq(&conf->resync_lock);
 }
 
 static void allow_barrier(conf_t *conf)
 {
-	unsigned long flags;
-	spin_lock_irqsave(&conf->resync_lock, flags);
-	conf->nr_pending--;
-	spin_unlock_irqrestore(&conf->resync_lock, flags);
+	atomic_dec(&conf->nr_pending);
 	wake_up(&conf->wait_barrier);
 }
 
@@ -708,10 +746,10 @@ static void freeze_array(conf_t *conf)
 	 * we continue.
 	 */
 	spin_lock_irq(&conf->resync_lock);
-	conf->barrier++;
-	conf->nr_waiting++;
+	atomic_inc(&conf->barrier);
+	atomic_inc(&conf->nr_waiting);
 	wait_event_lock_irq(conf->wait_barrier,
-			    conf->nr_pending == conf->nr_queued+1,
+			    atomic_read(&conf->nr_pending) == conf->nr_queued+1,
 			    conf->resync_lock,
 			    flush_pending_writes(conf));
 	spin_unlock_irq(&conf->resync_lock);
@@ -720,8 +758,8 @@ static void unfreeze_array(conf_t *conf)
 {
 	/* reverse the effect of the freeze */
 	spin_lock_irq(&conf->resync_lock);
-	conf->barrier--;
-	conf->nr_waiting--;
+	atomic_dec(&conf->barrier);
+	atomic_dec(&conf->nr_waiting);
 	wake_up(&conf->wait_barrier);
 	spin_unlock_irq(&conf->resync_lock);
 }
@@ -1839,7 +1877,7 @@ static sector_t sync_request(mddev_t *md
 	 * and resync is going fast enough,
 	 * then let it though before starting on this new sync request.
 	 */
-	if (!go_faster && conf->nr_waiting)
+	if (!go_faster && atomic_read(&conf->nr_waiting))
 		msleep_interruptible(1000);
 
 	bitmap_cond_end_sync(mddev->bitmap, sector_nr);
@@ -2401,7 +2439,7 @@ static void *raid1_takeover(mddev_t *mdd
 		mddev->new_chunk_sectors = 0;
 		conf = setup_conf(mddev);
 		if (!IS_ERR(conf))
-			conf->barrier = 1;
+			atomic_set(&conf->barrier, 1);
 		return conf;
 	}
 	return ERR_PTR(-EINVAL);
--- a/drivers/md/raid1.h
+++ b/drivers/md/raid1.h
@@ -40,10 +40,10 @@ struct r1_private_data_s {
 	/* for use when syncing mirrors: */
 
 	spinlock_t		resync_lock;
-	int			nr_pending;
-	int			nr_waiting;
+	atomic_t		nr_pending;
+	atomic_t		nr_waiting;
 	int			nr_queued;
-	int			barrier;
+	atomic_t		barrier;
 	sector_t		next_resync;
 	int			fullsync;  /* set to 1 if a full sync is needed,
 					    * (fresh device added).
