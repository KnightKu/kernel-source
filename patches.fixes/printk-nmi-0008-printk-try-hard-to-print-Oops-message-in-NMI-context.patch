From: Petr Mladek <pmladek@suse.cz>
Date: Wed, 7 May 2014 11:40:33 +0200
Subject: [RFC PATCH 08/11] printk: try hard to print Oops message in NMI context
Patch-mainline: Submitted, https://lkml.org/lkml/2014/5/9/127
References: bnc#831949

pmladek@suse.cz:
This patch needed some modifications for SLED12 to avoid KABI changes.
They were done using:

    sed -i \
	-e "s/struct printk_log \*/enum printk_log_type /g" \
	-e "s/struct printk_msg/struct printk_log/g" \
	-e "s/main_logbuf_lock/logbuf_lock/g" \
	-e "s/nmi_log\.buf/nmi_log_buf/g" \
	-e "s/&main_log/MAIN_LOG/g" \
	-e "s/&nmi_log/NMI_LOG/g" \
	-e "s/NMI_LOGbuf_lock/\&nmi_logbuf_lock/g" \
	-e "s/main_log\.cont/\&main_cont/g" \
	-e "s/msg_from_idx/log_from_idx/g" \
	-e "s/get_dict/log_dict/g" \
	-e "s/get_text/get_dict/g" \
	-e "s/nmi_log\.nmi\.first_id/nmi_log_first_id/g" \
	-e "s/nmi_log\.nmi\.next_id/nmi_log_next_id/g" \
	orig.patch


Oops messages are important for debugging. We should try harder to get them into
the main ring buffer and print them to the console. This is problematic in NMI
context because the needed locks might already be taken.

What we can do, though, is to zap all printk locks. We already do this
when a printk recursion is detected. This should be safe because the system
is crashing and there shouldn't be any printk caller by now. In case somebody
manages to grab the logbuf_lock after zap_locks then we just fallback to the
NMI ring buffer and hope that someone else will merge the messages strings and
flush the buffer.

Signed-off-by: Petr Mladek <pmladek@suse.cz>
---
 kernel/printk/printk.c | 22 ++++++++++++++++++----
 1 file changed, 18 insertions(+), 4 deletions(-)

--- a/kernel/printk/printk.c
+++ b/kernel/printk/printk.c
@@ -2125,16 +2125,26 @@ asmlinkage int vprintk_emit(int facility
 	 * when we managed to preempt the currently running printk from NMI
 	 * context. When we are not sure, rather copy the current message
 	 * into NMI ring buffer and merge it later.
+	 *
+	 * Special case are Oops messages from NMI context. We try hard to print
+	 * them. So we forcefully drop existing locks, try to pass them via the
+	 * main log buffer, and even later push them to the console.
 	 */
 	if (likely(!in_nmi())) {
 		raw_spin_lock(&logbuf_lock);
 	} else {
 		/*
 		 * Always use NMI ring buffer if something is already
-		 * in the cont buffer.
+		 * in the cont buffer, except for Oops.
 		 */
-		if ((nmi_cont.len && nmi_cont.owner == current) ||
-		    !raw_spin_trylock(&logbuf_lock)) {
+		bool force_nmi_logbuf = nmi_cont.len &&
+			nmi_cont.owner == current &&
+			!oops_in_progress;
+
+		if (oops_in_progress)
+			zap_locks();
+
+		if (force_nmi_logbuf || !raw_spin_trylock(&logbuf_lock)) {
 			if (!nmi_log_buf) {
 				lockdep_on();
 				local_irq_restore(flags);
@@ -2264,8 +2274,12 @@ asmlinkage int vprintk_emit(int facility
 	/*
 	 * If called from the scheduler or NMI context, we can not get console
 	 * without a possible deadlock.
+	 *
+	 * The only exception are Oops messages from NMI context where all
+	 * relevant locks have been forcefully dropped above. We have to try
+	 * to get the console, otherwise the last messages would get lost.
 	 */
-	if (in_sched || in_nmi())
+	if (in_sched || (in_nmi() && !oops_in_progress))
 		return printed_len;
 
       	lockdep_off();
