From: Hannes Reinecke <hare@suse.de>
Date: Wed, 27 Jul 2016 08:43:15 +0200
Subject: [PATCH] md: lockless I/O submission for RAID1
Patch-mainline: Not yet, this patch is specific for SLE11-SP4 
Reference: bsc#982783

We should avoid taking a lock in the hotpath during I/O submission;
this hurts performance very bad when running on fast storage.

(Rebased by Coly Li for SLE11-SP4)

Signed-off-by: Hannes Reinecke <hare@suse.com>
Signed-off-by: Coly Li <colyli@suse.de>
---
 drivers/md/raid1.c |   65 ++++++++++++++++++++++++++---------------------------
 drivers/md/raid1.h |   16 +++++++------
 2 files changed, 41 insertions(+), 40 deletions(-)

--- a/drivers/md/raid1.c
+++ b/drivers/md/raid1.c
@@ -633,15 +633,17 @@ static void raise_barrier(conf_t *conf)
 	spin_lock_irq(&conf->resync_lock);
 
 	/* Wait until no block IO is waiting */
-	wait_event_lock_irq(conf->wait_barrier, !conf->nr_waiting,
+	wait_event_lock_irq(conf->wait_barrier,
+			    !atomic_read(&conf->nr_waiting),
 			    conf->resync_lock, );
 
 	/* block any new IO from starting */
-	conf->barrier++;
+	atomic_inc(&conf->barrier);
 
 	/* Now wait for all pending IO to complete */
 	wait_event_lock_irq(conf->wait_barrier,
-			    !conf->nr_pending && conf->barrier < RESYNC_DEPTH,
+			    !atomic_read(&conf->nr_pending) &&
+			    atomic_read(&conf->barrier) < RESYNC_DEPTH,
 			    conf->resync_lock, );
 
 	spin_unlock_irq(&conf->resync_lock);
@@ -649,19 +651,18 @@ static void raise_barrier(conf_t *conf)
 
 static void lower_barrier(conf_t *conf)
 {
-	unsigned long flags;
-	BUG_ON(conf->barrier <= 0);
-	spin_lock_irqsave(&conf->resync_lock, flags);
-	conf->barrier--;
-	spin_unlock_irqrestore(&conf->resync_lock, flags);
+	BUG_ON(atomic_read(&conf->barrier) <= 0);
+	atomic_dec(&conf->barrier);
 	wake_up(&conf->wait_barrier);
 }
 
 static void wait_barrier(conf_t *conf)
 {
-	spin_lock_irq(&conf->resync_lock);
-	if (conf->barrier) {
-		conf->nr_waiting++;
+	atomic_inc(&conf->nr_pending);
+	if (atomic_read(&conf->barrier)) {
+		spin_lock_irq(&conf->resync_lock);
+		atomic_inc(&conf->nr_waiting);
+		atomic_dec(&conf->nr_pending);
 		/* Wait for the barrier to drop.
 		 * However if there are already pending
 		 * requests (preventing the barrier from
@@ -672,24 +673,21 @@ static void wait_barrier(conf_t *conf)
 		 * count down.
 		 */
 		wait_event_lock_irq(conf->wait_barrier,
-				    !conf->barrier ||
-				    (conf->nr_pending &&
+				    !atomic_read(&conf->barrier) ||
+				    (atomic_read(&conf->nr_pending) &&
 				     current->bio_list &&
 				     !bio_list_empty(current->bio_list)),
 				    conf->resync_lock,
 			);
-		conf->nr_waiting--;
+		atomic_inc(&conf->nr_pending);
+		atomic_dec(&conf->nr_waiting);
+		spin_unlock_irq(&conf->resync_lock);
 	}
-	conf->nr_pending++;
-	spin_unlock_irq(&conf->resync_lock);
 }
 
 static void allow_barrier(conf_t *conf)
 {
-	unsigned long flags;
-	spin_lock_irqsave(&conf->resync_lock, flags);
-	conf->nr_pending--;
-	spin_unlock_irqrestore(&conf->resync_lock, flags);
+	atomic_dec(&conf->nr_pending);
 	wake_up(&conf->wait_barrier);
 }
 
@@ -708,10 +706,10 @@ static void freeze_array(conf_t *conf)
 	 * we continue.
 	 */
 	spin_lock_irq(&conf->resync_lock);
-	conf->barrier++;
-	conf->nr_waiting++;
+	atomic_inc(&conf->barrier);
+	atomic_inc(&conf->nr_waiting);
 	wait_event_lock_irq(conf->wait_barrier,
-			    conf->nr_pending == conf->nr_queued+1,
+			    atomic_read(&conf->nr_pending) == conf->nr_queued+1,
 			    conf->resync_lock,
 			    flush_pending_writes(conf));
 	spin_unlock_irq(&conf->resync_lock);
@@ -720,8 +718,8 @@ static void unfreeze_array(conf_t *conf)
 {
 	/* reverse the effect of the freeze */
 	spin_lock_irq(&conf->resync_lock);
-	conf->barrier--;
-	conf->nr_waiting--;
+	atomic_dec(&conf->barrier);
+	atomic_dec(&conf->nr_waiting);
 	wake_up(&conf->wait_barrier);
 	spin_unlock_irq(&conf->resync_lock);
 }
@@ -1149,7 +1147,7 @@ static int raid1_add_disk(mddev_t *mddev
 			 * if this was recently any drive of the array
 			 */
 			if (rdev->saved_raid_disk < 0)
-				conf->fullsync = 1;
+				set_bit(R1FLAG_FULLSYNC, &conf->flags);
 			rcu_assign_pointer(p->rdev, rdev);
 			break;
 		}
@@ -1811,7 +1809,7 @@ static sector_t sync_request(mddev_t *md
 			bitmap_end_sync(mddev->bitmap, mddev->curr_resync,
 						&sync_blocks, 1);
 		else /* completed sync */
-			conf->fullsync = 0;
+			clear_bit(R1FLAG_FULLSYNC, &conf->flags);
 
 		bitmap_close_sync(mddev->bitmap);
 		close_sync(conf);
@@ -1821,7 +1819,7 @@ static sector_t sync_request(mddev_t *md
 	if (mddev->bitmap == NULL &&
 	    mddev->recovery_cp == MaxSector &&
 	    !test_bit(MD_RECOVERY_REQUESTED, &mddev->recovery) &&
-	    conf->fullsync == 0) {
+	    !test_bit(R1FLAG_FULLSYNC, &conf->flags)) {
 		*skipped = 1;
 		return max_sector - sector_nr;
 	}
@@ -1829,7 +1827,8 @@ static sector_t sync_request(mddev_t *md
 	 * This call the bitmap_start_sync doesn't actually record anything
 	 */
 	if (!bitmap_start_sync(mddev->bitmap, sector_nr, &sync_blocks, 1) &&
-	    !conf->fullsync && !test_bit(MD_RECOVERY_REQUESTED, &mddev->recovery)) {
+	    !test_bit(R1FLAG_FULLSYNC, &conf->flags) &&
+	    !test_bit(MD_RECOVERY_REQUESTED, &mddev->recovery)) {
 		/* We can skip this block, and probably several more */
 		*skipped = 1;
 		return sync_blocks;
@@ -1839,7 +1838,7 @@ static sector_t sync_request(mddev_t *md
 	 * and resync is going fast enough,
 	 * then let it though before starting on this new sync request.
 	 */
-	if (!go_faster && conf->nr_waiting)
+	if (!go_faster && atomic_read(&conf->nr_waiting))
 		msleep_interruptible(1000);
 
 	bitmap_cond_end_sync(mddev->bitmap, sector_nr);
@@ -1942,7 +1941,7 @@ static sector_t sync_request(mddev_t *md
 		if (sync_blocks == 0) {
 			if (!bitmap_start_sync(mddev->bitmap, sector_nr,
 					       &sync_blocks, still_degraded) &&
-			    !conf->fullsync &&
+			    !test_bit(R1FLAG_FULLSYNC, &conf->flags) &&
 			    !test_bit(MD_RECOVERY_REQUESTED, &mddev->recovery))
 				break;
 			BUG_ON(sync_blocks < (PAGE_SIZE>>9));
@@ -2076,7 +2075,7 @@ static conf_t *setup_conf(mddev_t *mddev
 		    !test_bit(In_sync, &disk->rdev->flags)) {
 			disk->head_position = 0;
 			if (disk->rdev)
-				conf->fullsync = 1;
+				set_bit(R1FLAG_FULLSYNC, &conf->flags);
 		} else if (conf->last_used < 0)
 			/*
 			 * The first working device is used as a
@@ -2401,7 +2400,7 @@ static void *raid1_takeover(mddev_t *mdd
 		mddev->new_chunk_sectors = 0;
 		conf = setup_conf(mddev);
 		if (!IS_ERR(conf))
-			conf->barrier = 1;
+			atomic_set(&conf->barrier, 1);
 		return conf;
 	}
 	return ERR_PTR(-EINVAL);
--- a/drivers/md/raid1.h
+++ b/drivers/md/raid1.h
@@ -40,15 +40,17 @@ struct r1_private_data_s {
 	/* for use when syncing mirrors: */
 
 	spinlock_t		resync_lock;
-	int			nr_pending;
-	int			nr_waiting;
+	atomic_t		nr_pending;
+	atomic_t		nr_waiting;
 	int			nr_queued;
-	int			barrier;
+	atomic_t		barrier;
 	sector_t		next_resync;
-	int			fullsync;  /* set to 1 if a full sync is needed,
-					    * (fresh device added).
-					    * Cleared when a sync completes.
-					    */
+	unsigned long		flags;
+#define R1FLAG_ARRAY_FROZEN 0
+	/* Set if a full sync is needed, (fresh device added).
+	* Cleared when a sync completes.
+	*/
+#define R1FLAG_FULLSYNC 1
 
 	wait_queue_head_t	wait_barrier;
 
